{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from urllib.request import urlretrieve as download\n",
    "from glob import glob as g\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from mypdb import PDB_file as mypdb\n",
    "from Bio.Blast.Applications import NcbipsiblastCommandline\n",
    "from time import time as t\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "from time import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to handle and download the hits from a blast search\n",
    "\n",
    "\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    for k2, v2 in dict2.items():\n",
    "        if k2 in dict1:\n",
    "            if isinstance(dict1[k2], list):\n",
    "                dict1[k2].append(v2)\n",
    "            else:\n",
    "                dict1[k2] = [dict1[k2], v2]\n",
    "        else:\n",
    "            print(f\"New Key : {k2}\")\n",
    "            dict1[k2] = v2\n",
    "    return dict1\n",
    "\n",
    "class hit:\n",
    "    def __init__(self, xml_hit):\n",
    "        self.hit_num = xml_hit[0].text\n",
    "        pdb_info = xml_hit[1].text.split(\"|\")[1:]\n",
    "        self.pdb_id = pdb_info[0]\n",
    "        self.pdb_chain = pdb_info[1]\n",
    "        self.description = xml_hit[2].text\n",
    "        data = [[y.tag.replace(\"Hsp_\", \"\"), y.text] for y in xml_hit[5][0]]\n",
    "        self.data = dict(data)\n",
    "        self.pdb = None\n",
    "        self._not_exists = False\n",
    "\n",
    "    @staticmethod\n",
    "    def download2(code, pdir=None):\n",
    "        base_url = \"https://files.rcsb.org/download\"\n",
    "        pdb_url = f\"{base_url}/{code}.pdb\"\n",
    "        f_p = os.path.join(pdir, f\"{code}.pdb\")\n",
    "        try:\n",
    "            download(pdb_url, f_p)\n",
    "            return f_p  # Return the file path if succeeded\n",
    "        except Exception:\n",
    "            print(f\"File {code} not found.\")\n",
    "            return None  # Return None if failed\n",
    "\n",
    "    def _assign(self, f_p):\n",
    "        self.pdb_path = f_p\n",
    "        self.pdb = mypdb(f_p)\n",
    "        \n",
    "    def check_for_pdb(self, pdir=None):\n",
    "        if pdir is None:\n",
    "            pdir = \"./pdbs/\"\n",
    "        elif pdir[-1] != \"/\":\n",
    "            pdir += \"/\"\n",
    "        if not os.path.isdir(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "        matches = g(pdir + f\"{self.pdb_id}*\")\n",
    "        if matches:\n",
    "            if not self.pdb:\n",
    "                self._assign(matches[0])\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2139fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to handle the blast search\n",
    "class blast:\n",
    "    def __init__(self, sequence, name, database, path, program=\"blastp\", hitlen=10000):\n",
    "        self.sequence = sequence\n",
    "        self.program = program\n",
    "        self.database = database\n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.hitlen = hitlen\n",
    "\n",
    "        if program == \"blastp\" and (self.path and os.path.isfile(self.path)):\n",
    "            self.parse_search()\n",
    "        elif program == \"psiblast\":\n",
    "            self.psiblast_search()\n",
    "        else:\n",
    "            self.bsearch()\n",
    "\n",
    "    def bsearch(self):\n",
    "        # Perform the initial BLASTP search\n",
    "        if self.program == \"blastp\":\n",
    "            print(\"Searching BLASTP...\")\n",
    "            t1 = t()\n",
    "            self.results = NCBIWWW.qblast(self.program, self.database, self.sequence, hitlist_size=self.hitlen)\n",
    "            t2 = t()\n",
    "            print(f\"BLASTP took {round(t2-t1,4)} seconds\")\n",
    "            if not self.path:\n",
    "                self.path = f\"{self.name}-blast.xml\"\n",
    "            with open(self.path, \"w\") as output_xml:\n",
    "                output_xml.write(self.results.read())\n",
    "            self.parse_search()\n",
    "\n",
    "    def psiblast_search(self):\n",
    "        # Run local PSI-BLAST using NcbipsiblastCommandline\n",
    "        print(f\"Running PSI-BLAST on {self.name}...\")\n",
    "        input_fasta = f\"{self.name}.fasta\"\n",
    "        with open(input_fasta, \"w\") as f:\n",
    "            f.write(f\">query\\n{self.sequence}\\n\")\n",
    "\n",
    "        psiblast_cline = NcbipsiblastCommandline(\n",
    "            query=input_fasta,\n",
    "            db=\"/home/marmatt/ncbi-blast-2.16.0+/bin/pdbaa\",\n",
    "            evalue=10,\n",
    "            num_iterations=3,\n",
    "            out_ascii_pssm=f\"{self.name}.pssm\",\n",
    "            out=f\"{self.name}-psiblast.xml\",\n",
    "            outfmt=5\n",
    "        )\n",
    "        stdout, stderr = psiblast_cline()\n",
    "        if stderr:\n",
    "            print(f\"PSI-BLAST ERROR: {stderr}\")\n",
    "        else:\n",
    "            print(\"PSI-BLAST search completed.\")\n",
    "            self.parse_search(xml_file=f\"{self.name}-psiblast.xml\")\n",
    "\n",
    "    def parse_search(self, xml_file=None):\n",
    "        xml_file = xml_file or self.path\n",
    "        if not xml_file:\n",
    "            raise Exception(\"No XML file path provided.\")\n",
    "        t1 = t()\n",
    "        tree = ET.parse(xml_file)\n",
    "        iteration = tree.findall(\"./BlastOutput_iterations/Iteration/\")\n",
    "        self.query_length = iteration[3].text\n",
    "        hits = [hit(x) for x in iteration[-2]] #returns hit objects\n",
    "        self.hits = hits\n",
    "        mega_dict = hits[0].data\n",
    "        for x in hits[1:]:\n",
    "            mega_dict = merge_dicts(mega_dict, x.data)\n",
    "        mega_dict[\"PDB ID\"] = [x.pdb_id for x in hits]\n",
    "        mega_dict[\"Chain\"] = [x.pdb_chain for x in hits]\n",
    "        mega_dict[\"Description\"] = [x.description for x in hits]\n",
    "        self.df = pd.DataFrame.from_dict(mega_dict)\n",
    "        print(self.df)\n",
    "        t2 = t()\n",
    "        print(f\"Time taken to parse {t2-t1}\")\n",
    "    \n",
    "    def download_pdbs(self, pdir=None):\n",
    "        default_dir = \"./PDBs\"\n",
    "        pdir = os.path.abspath(pdir if pdir else default_dir)\n",
    "        if not os.path.isdir(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "        \n",
    "        files = [os.path.splitext(f)[0] for f in os.listdir(pdir)]\n",
    "        hit_bar = tqdm(self.hits, desc=\"Processing Hits\")\n",
    "        \n",
    "        for x in hit_bar:\n",
    "            if x.pdb_id not in files:\n",
    "                hit_bar.set_description(f\"Downloading {x}\")\n",
    "                \n",
    "                try:\n",
    "                    file_path = x.download2(x.pdb_id, pdir=pdir)\n",
    "\n",
    "                    if file_path:\n",
    "                        x._assign(file_path)\n",
    "                    else:\n",
    "                        # In case download2 does not return a valid path\n",
    "                        raise Exception(\"Download failed\")\n",
    "                except Exception as e:\n",
    "                    # Print a message if download fails or file path is invalid\n",
    "                    print(f\"Structure {x.pdb_id} was not found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f2b43",
   "metadata": {},
   "source": [
    "## Actually running the blast search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Here we perform a blast search on:\n",
    " 1.   The BRAF monomer\n",
    "\"\"\"\n",
    "with open(\"./fastas.txt\") as f:\n",
    "    fastas = f.readlines()\n",
    "\n",
    "braf_fasta = fastas[1]\n",
    "name = \"braf\"\n",
    "\n",
    "# Directory for storing blast search results\n",
    "output_dir = \"./blast_search\"\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "res_path = os.path.join(output_dir, f\"{name}-blast.xml\")  # Path for the results file\n",
    "\n",
    "# Check for saved results in the specified directory\n",
    "if os.path.exists(res_path):\n",
    "    bs = blast(braf_fasta, name, database=\"pdb\", path=res_path)\n",
    "else:\n",
    "    bs = blast(braf_fasta, name, database=\"pdb\", path=None)\n",
    "\n",
    "bs.download_pdbs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84c34d",
   "metadata": {},
   "source": [
    "## Counting the number of pdb files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'PDBs'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58070b",
   "metadata": {},
   "source": [
    "## Counting the number of pdb hits in the xml file and checking which ones have not been downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0522d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_pdb_files(directory):\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Extract the base filenames (without extension) to compare with PDB IDs\n",
    "    pdb_file_ids = {os.path.splitext(os.path.basename(f))[0] for f in pdb_files}\n",
    "\n",
    "    return pdb_file_ids\n",
    "\n",
    "def find_unique_and_duplicate_pdb_hit_ids(xml_file):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Dictionary to count occurrences of each PDB hit ID\n",
    "    hit_id_counts = defaultdict(int)\n",
    "\n",
    "    # Iterate over all Hit elements in the XML\n",
    "    for hit in root.findall('.//Hit'):\n",
    "        # Extract the Hit_id text\n",
    "        hit_id = hit.find('Hit_id').text\n",
    "\n",
    "        # Assuming the Hit_id format is 'pdb|PDB_ID|Chain', extract the PDB_ID\n",
    "        pdb_id = hit_id.split('|')[1]\n",
    "\n",
    "        # Increment the count for this PDB_ID\n",
    "        hit_id_counts[pdb_id] += 1\n",
    "\n",
    "    # Find all PDB IDs (unique and duplicates)\n",
    "    all_hit_ids = set(hit_id_counts.keys())\n",
    "\n",
    "    return all_hit_ids\n",
    "\n",
    "# Specify the directory and XML file\n",
    "pdb_directory = 'PDBs'\n",
    "xml_file = 'braf-blast.xml'\n",
    "\n",
    "# Get the PDB file IDs from the directory\n",
    "pdb_file_ids = count_pdb_files(pdb_directory)\n",
    "\n",
    "# Get all PDB hit IDs from the XML\n",
    "all_pdb_ids = find_unique_and_duplicate_pdb_hit_ids(xml_file)\n",
    "\n",
    "# Calculate the number of total PDB hits\n",
    "total_pdb_hits = len(all_pdb_ids)\n",
    "\n",
    "# Find PDB IDs in XML that are not in the directory\n",
    "missing_pdb_ids = all_pdb_ids - pdb_file_ids\n",
    "\n",
    "print(f\"There are {total_pdb_hits} total PDB hits in the file '{xml_file}'.\")\n",
    "print(f\"There are {len(missing_pdb_ids)} PDB IDs in the XML not found in the directory '{pdb_directory}':\")\n",
    "print(missing_pdb_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b9660",
   "metadata": {},
   "source": [
    "## Here we are stripping the downloaded pdb files to only contain the chain of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc269f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "from time import time as t\n",
    "!ls\n",
    "def sglob(fp, absolute=True):\n",
    "    fps = sorted(glob(fp))\n",
    "    if absolute:\n",
    "        fps = [os.path.abspath(f) for f in fps]\n",
    "    return fps\n",
    "\n",
    "def strip_to_chain(pdb_file, chain_ID):\n",
    "    u = mda.Universe(pdb_file)\n",
    "    print(f\"Loaded trajectory from {pdb_file} with {len(u.atoms)} atoms.\")\n",
    "\n",
    "    chain = u.select_atoms(f\"protein and chainID {chain_ID}\")\n",
    "    if len(chain) == 0:\n",
    "        print(f\"Chain {chain_ID} not found in {pdb_file}.\")\n",
    "        return None\n",
    "    return chain\n",
    "\n",
    "def post_process(fname):\n",
    "    with open(fname, \"r\") as f_o:\n",
    "        initial_lines = f_o.readlines()\n",
    "\n",
    "    print(f\"File {fname} before post_process, first few lines:\")\n",
    "    print(\"\".join(initial_lines[:20]))\n",
    "\n",
    "    final_lines = initial_lines[-2:].copy()\n",
    "    no_ter = [line for line in initial_lines if line[:3] != \"TER\" or line in final_lines]\n",
    "\n",
    "    if len(no_ter) != len(initial_lines):\n",
    "        with open(fname, \"w\") as f_o:\n",
    "            print(f\"Rewriting {fname}, lines reduced from {len(initial_lines)} to {len(no_ter)}\")\n",
    "            f_o.write(\"\".join(no_ter))\n",
    "\n",
    "    print(f\"File {fname} after post_process, first few lines:\")\n",
    "    with open(fname, \"r\") as f_r:\n",
    "        print(\"\".join(f_r.readlines()[:20]))\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    hit_id = re.compile(r\"<Hit_id>(.*?)<.Hit_id>\")\n",
    "    with open(xml_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "        results = [h.split(\"|\")[1:] for h in hit_id.findall(text)]\n",
    "        pdb_chain_dict = {}\n",
    "        for r in results:\n",
    "            pdb_chain_dict[r[0]+f\"_{r[1]}\"] = r[1]\n",
    "    return pdb_chain_dict\n",
    "\n",
    "def get_pdb_id(fp):\n",
    "    fp = fp.rsplit(\".\", 1)[0]\n",
    "    if \"/\" in fp:\n",
    "        fp = fp.rsplit(\"/\", 1)[1]\n",
    "    return fp\n",
    "\n",
    "def target_name(fp, target_dir, chain):\n",
    "    orig_path, file_name = fp.rsplit(\"/\", 1)\n",
    "    fp = fp.replace(orig_path, target_dir)\n",
    "    fp = fp.split(\".\")[0] + f\"_{chain}.pdb\"\n",
    "    return fp\n",
    "\n",
    "def find_pdb_file(PDB_chain_id, files):\n",
    "    print(files)\n",
    "    if \"_\" in PDB_chain_id:\n",
    "        PDB_id = PDB_chain_id.split(\"_\")[0]\n",
    "    assert len(PDB_id) == 4\n",
    "    for f in files:\n",
    "        filename = os.path.basename(f).split('.')[0]\n",
    "        if filename.startswith(PDB_id):\n",
    "            print(f\"Found file: {f} for PDB ID: {PDB_id}\")\n",
    "            return f\n",
    "\n",
    "print(\"BEGIN\")\n",
    "t1 = t()\n",
    "xml = \"braf-blast.xml\"\n",
    "pdb_dir = \"PDBs\"\n",
    "target_dir = \"Results/activation_segments/unaligned\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "xml_chain_dict = parse_xml(xml)\n",
    "print(xml_chain_dict)\n",
    "\n",
    "pdb_files = sorted(sglob(\"PDBs/*.pdb\"))\n",
    "print(pdb_files)\n",
    "keys = sorted([*xml_chain_dict.keys()], key=get_pdb_id)\n",
    "\n",
    "print(keys)\n",
    "files = [find_pdb_file(k, pdb_files) for k in keys]\n",
    "print(files)\n",
    "chain_IDs = [xml_chain_dict[k] for k in keys]\n",
    "print(len(chain_IDs))\n",
    "\n",
    "valid_file_chain_pairs = [(f, c) for f, c in zip(files, chain_IDs) if f is not None]\n",
    "print(len(valid_file_chain_pairs))\n",
    "new_file_paths = [target_name(f, target_dir, c) for f, c in valid_file_chain_pairs]\n",
    "print(new_file_paths)\n",
    "file_paths = [f for f, c in valid_file_chain_pairs]\n",
    "print(file_paths)\n",
    "print(len(file_paths))\n",
    "\n",
    "for (fp, chain_ID, tp) in zip(file_paths, chain_IDs, new_file_paths):\n",
    "    if fp is not None:\n",
    "        try:\n",
    "            print(fp, chain_ID, tp)\n",
    "            chain = strip_to_chain(fp, chain_ID)\n",
    "            if chain is not None:\n",
    "                with mda.Writer(tp) as w:\n",
    "                    w.write(chain)\n",
    "                post_process(tp)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {fp} with chain {chain_ID}: {e}\")\n",
    "        \n",
    "t2 = t()\n",
    "t_t = round(t2 - t1, 3) // 60\n",
    "t_t = str((t_t // 60)) + \":\" + str(t_t % 60)\n",
    "print(f\"Time taken {t_t} for sequential processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616153e7",
   "metadata": {},
   "source": [
    "## Counting again how many pdb files are in the directory after stripping the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'Results/activation_segments/unaligned'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e014c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the full sequence from the pdb files, checking if there are any non-natural amino acids and substituting them and selecting only sequences with a maximum gap length of 4 amino acids to be reconstructed\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from Bio.SeqUtils import seq1\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def substitute_non_natural_amino_acid(residue, aligned_atom, index):\n",
    "    \"\"\"Substitute non-natural amino acids with their natural counterparts.\"\"\"\n",
    "    substitutions = {\n",
    "        'X': 'G',  # Glycine\n",
    "        'B': 'N',  # Asparagine\n",
    "        'Z': 'Q',  # Glutamine\n",
    "        'J': 'L'   # Leucine\n",
    "    }\n",
    "    \n",
    "    if residue == 'X':\n",
    "        # Check if 'X' is surrounded by missing amino acids in aligned_atom\n",
    "        if index > 0 and aligned_atom[index - 1] == '-':\n",
    "            return residue\n",
    "        if index < len(aligned_atom) - 1 and aligned_atom[index + 1] == '-':\n",
    "            return residue\n",
    "\n",
    "    return substitutions.get(residue, residue)\n",
    "\n",
    "def extract_seqres_sequence(pdb_file):\n",
    "    \"\"\"Extract SEQRES sequences for each chain from a PDB file.\"\"\"\n",
    "    seq_dict = {}\n",
    "    with open(pdb_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    current_chain = None\n",
    "    current_seq = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"SEQRES\"):\n",
    "            parts = line.split()\n",
    "            chain_id = parts[2]\n",
    "            if chain_id != current_chain:\n",
    "                if current_chain is not None:\n",
    "                    seq_dict[current_chain] = ''.join(seq1(residue) for residue in current_seq)\n",
    "                current_chain = chain_id\n",
    "                current_seq = []\n",
    "            current_seq.extend(parts[4:])\n",
    "\n",
    "    if current_chain is not None:\n",
    "        seq_dict[current_chain] = ''.join(seq1(residue) for residue in current_seq)\n",
    "\n",
    "    return seq_dict\n",
    "\n",
    "def extract_atom_sequence(pdb_file, chain_id):\n",
    "    \"\"\"Extract sequence from atomic coordinates for a specific chain.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        chain = model[chain_id]\n",
    "        \n",
    "        ppb = PPBuilder()\n",
    "        sequence = ''\n",
    "        for pp in ppb.build_peptides(chain):\n",
    "            sequence += pp.get_sequence()\n",
    "        return sequence\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    index = sequence.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "def align_and_highlight_gaps(seqres_segment, atom_segment):\n",
    "    \"\"\"Align SEQRES and ATOM segments and highlight gaps in ATOM.\"\"\"\n",
    "    aligned_seqres = ''\n",
    "    aligned_atom = ''\n",
    "    atom_index = 0\n",
    "    max_gap_length = 0\n",
    "    current_gap_length = 0\n",
    "\n",
    "    for res_seqres in seqres_segment:\n",
    "        if atom_index < len(atom_segment) and res_seqres == atom_segment[atom_index]:\n",
    "            aligned_seqres += res_seqres\n",
    "            aligned_atom += atom_segment[atom_index]\n",
    "            atom_index += 1\n",
    "            current_gap_length = 0\n",
    "        else:\n",
    "            aligned_seqres += res_seqres\n",
    "            aligned_atom += '-'\n",
    "            current_gap_length += 1\n",
    "            max_gap_length = max(max_gap_length, current_gap_length)\n",
    "\n",
    "    return aligned_seqres, aligned_atom, max_gap_length\n",
    "\n",
    "def main():\n",
    "    target_dir = \"Results/activation_segments/unaligned\"\n",
    "    pdb_dir = \"PDBs\"\n",
    "    fasta_output_file = \"seqres_sequences.fasta\"  # File to store full sequences\n",
    "    text_output_file = \"seqres_info.txt\"\n",
    "    aligned_sequences = {}\n",
    "    satisfying_structures_count = 0\n",
    "\n",
    "    pdb_files = glob(os.path.join(target_dir, \"*.pdb\"))\n",
    "\n",
    "    with open(text_output_file, \"w\") as text_output, open(fasta_output_file, \"w\") as fasta_output:\n",
    "        for pdb_file in tqdm(pdb_files, desc=\"Processing PDB files\"):\n",
    "            pdb_name = os.path.basename(pdb_file)\n",
    "            pdb_id, chain_id_file = os.path.splitext(pdb_name)[0].split('_')\n",
    "            chain_id = chain_id_file\n",
    "\n",
    "            full_pdb_path = os.path.join(pdb_dir, pdb_id + '.pdb')\n",
    "            if not os.path.isfile(full_pdb_path):\n",
    "                print(f\"Corresponding full PDB for {pdb_id} not found.\")\n",
    "                continue\n",
    "\n",
    "            seqres_seqs = extract_seqres_sequence(full_pdb_path)\n",
    "            atom_seq = extract_atom_sequence(full_pdb_path, chain_id)\n",
    "\n",
    "            if chain_id in seqres_seqs and atom_seq:\n",
    "                seqres_sequence = seqres_seqs[chain_id]\n",
    "                seqres_dfg_index = find_motif_indices(seqres_sequence, 'DFG')\n",
    "                seqres_ape_index = find_motif_indices(seqres_sequence, 'APE')\n",
    "                atom_dfg_index = find_motif_indices(atom_seq, 'DFG')\n",
    "                atom_ape_index = find_motif_indices(atom_seq, 'APE')\n",
    "\n",
    "                # Determine the start and end indices for the segments\n",
    "                if None not in [seqres_dfg_index, seqres_ape_index, atom_dfg_index, atom_ape_index]:\n",
    "                    seqres_start = min(seqres_dfg_index, seqres_ape_index)\n",
    "                    seqres_end = max(seqres_dfg_index + 3, seqres_ape_index + 3)\n",
    "                    atom_start = min(atom_dfg_index, atom_ape_index)\n",
    "                    atom_end = max(atom_dfg_index + 3, atom_ape_index + 3)\n",
    "\n",
    "                    seqres_segment = seqres_sequence[seqres_start:seqres_end]\n",
    "                    atom_segment = atom_seq[atom_start:atom_end]\n",
    "\n",
    "                    aligned_seqres, aligned_atom, max_gap_length = align_and_highlight_gaps(seqres_segment, atom_segment)\n",
    "                    \n",
    "                    # Check for differences and substitute non-natural amino acids\n",
    "                    exclude_due_to_non_natural_diff = False\n",
    "                    corrected_seqres = ''\n",
    "                    for index, (res_seqres, res_atom) in enumerate(zip(aligned_seqres, aligned_atom)):\n",
    "                        if res_seqres != res_atom:\n",
    "                            corrected_residue = substitute_non_natural_amino_acid(res_seqres, aligned_atom, index)\n",
    "                            corrected_seqres += corrected_residue\n",
    "                            if corrected_residue != res_seqres:\n",
    "                                print(f\"Substituting non-natural amino acid '{res_seqres}' with '{corrected_residue}' in SEQRES for {pdb_id}_{chain_id}.\")\n",
    "                        else:\n",
    "                            corrected_seqres += res_seqres\n",
    "\n",
    "                    if not exclude_due_to_non_natural_diff and max_gap_length <= 4:\n",
    "                        satisfying_structures_count += 1\n",
    "                        info = (f\"Aligned Sequences for {pdb_id}_{chain_id}: (Max gap length: {max_gap_length})\\n\"\n",
    "                                f\"SEQRES Segment: {corrected_seqres}\\n\"\n",
    "                                f\"ATOM Segment:   {aligned_atom}\\n\\n\")\n",
    "                        print(info)\n",
    "                        text_output.write(info)\n",
    "                        \n",
    "                        aligned_sequences[f\"{pdb_id}_{chain_id}_SEQRES\"] = corrected_seqres\n",
    "                        aligned_sequences[f\"{pdb_id}_{chain_id}_ATOM\"] = aligned_atom\n",
    "\n",
    "                        # Write full SEQRES and ATOM sequences to the FASTA file\n",
    "                        fasta_output.write(f\">{pdb_id}_{chain_id}_SEQRES\\n{seqres_sequence}\\n\")\n",
    "                        fasta_output.write(f\">{pdb_id}_{chain_id}_ATOM\\n{atom_seq}\\n\")\n",
    "                    else:\n",
    "                        exclusion_msg = f\"Excluding {pdb_id}_{chain_id} due to gap length: {max_gap_length} or non-natural amino acid difference.\\n\"\n",
    "                        print(exclusion_msg)\n",
    "                        text_output.write(exclusion_msg)\n",
    "                else:\n",
    "                    motif_msg = f\"Motifs not found in {pdb_id}_{chain_id}.\\n\"\n",
    "                    print(motif_msg)\n",
    "                    text_output.write(motif_msg)\n",
    "            else:\n",
    "                chain_msg = f\"Chain {chain_id} not found in SEQRES of {pdb_id} or no atomic sequence available.\\n\"\n",
    "                print(chain_msg)\n",
    "                text_output.write(chain_msg)\n",
    "                \n",
    "        count_msg = f\"Total structures satisfying the condition: {satisfying_structures_count}\"\n",
    "        text_output.write(count_msg)\n",
    "        print(count_msg)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COunting how many seqences are in the fasta file\n",
    "def count_total_pdb_ids(file_path):\n",
    "    total_pdb_ids = 0\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                total_pdb_ids += 1\n",
    "\n",
    "    print(f\"Total number of PDB IDs: {int(total_pdb_ids/2)}\") #here we divide by 2 because we have two lines per PDB ID\n",
    "\n",
    "# Provide the path to your seqres_sequence.fasta file\n",
    "file_path = \"seqres_sequences.fasta\"\n",
    "count_total_pdb_ids(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to use MODELLER to reconstruct the sequences that have a gap length of 4 or less, if there are no differences between the SEQRES and ATOM sequences, the original PDB file is copied to the target directory\n",
    "# I have also added renumbering of the residues starting from id of the first residue in original pdb (with missing residues)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    \"\"\"Read sequences from a FASTA file into a dictionary.\"\"\"\n",
    "    sequences = {}\n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        header = None\n",
    "        sequence = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header:\n",
    "                    sequences[header] = ''.join(sequence)\n",
    "                header = line[1:]\n",
    "                sequence = []\n",
    "            else:\n",
    "                sequence.append(line)\n",
    "        if header:\n",
    "            sequences[header] = ''.join(sequence)\n",
    "    return sequences\n",
    "\n",
    "def extract_atom_sequence(pdb_file):\n",
    "    \"\"\"Extract sequence from atomic coordinates for the first chain found in the PDB file.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            ppb = PPBuilder()\n",
    "            sequence = ''\n",
    "            for pp in ppb.build_peptides(chain):\n",
    "                sequence += pp.get_sequence()\n",
    "            return str(sequence)\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    index = sequence.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "def reconstruct_with_modeller(pdb_chain_id, pdb_path, target_path, full_sequence, atom_sequence):\n",
    "    print(f\"Processing {pdb_chain_id}\")\n",
    "\n",
    "    # Find indices of the DFG and APE motifs\n",
    "    seqres_dfg_index = find_motif_indices(full_sequence, 'DFG')\n",
    "    seqres_ape_index = find_motif_indices(full_sequence, 'APE')\n",
    "    atom_dfg_index = find_motif_indices(atom_sequence, 'DFG')\n",
    "    atom_ape_index = find_motif_indices(atom_sequence, 'APE')\n",
    "\n",
    "    # Determine if reconstruction is needed\n",
    "    if None not in [seqres_dfg_index, seqres_ape_index, atom_dfg_index, atom_ape_index]:\n",
    "        seqres_start = min(seqres_dfg_index, seqres_ape_index)\n",
    "        seqres_end = max(seqres_dfg_index + 3, seqres_ape_index + 3)\n",
    "        atom_start = min(atom_dfg_index, atom_ape_index)\n",
    "        atom_end = max(atom_dfg_index + 3, atom_ape_index + 3)\n",
    "\n",
    "        seqres_segment = full_sequence[seqres_start:seqres_end]\n",
    "        atom_segment = atom_sequence[atom_start:atom_end]\n",
    "\n",
    "        # Check for differences in the segment\n",
    "        if seqres_segment != atom_segment:\n",
    "            print(f\"Reconstructing full sequence for {pdb_chain_id} using MODELLER\")\n",
    "\n",
    "            # Setting up MODELLER\n",
    "            env = environ()\n",
    "            aln = alignment(env)\n",
    "            \n",
    "            # Read the structure to work on\n",
    "            mdl = model(env, file=pdb_path)\n",
    "            aln.append_model(mdl, align_codes='template', atom_files=pdb_path)\n",
    "\n",
    "            # Append the full target sequence\n",
    "            aln.append_sequence(full_sequence)\n",
    "            aln[-1].code = 'target'\n",
    "            \n",
    "            # Perform the alignment\n",
    "            aln.align2d(max_gap_length=50)\n",
    "\n",
    "            # Create AutoModel object and build models\n",
    "            a = automodel(env, alnfile=aln, knowns='template', sequence='target')\n",
    "            a.starting_model = 1\n",
    "            a.ending_model = 1\n",
    "            \n",
    "            # Build the model\n",
    "            a.make()\n",
    "            \n",
    "            # Save the best model to the target directory\n",
    "            model_path = os.path.join(target_path, f\"{pdb_chain_id}_filled.pdb\")\n",
    "            os.rename(a.outputs[0]['name'], model_path)\n",
    "            print(f\"Reconstruction completed for {pdb_chain_id}. File saved at {model_path}\")\n",
    "\n",
    "            # Adjust residue numbering and overwrite the filled file\n",
    "            adjust_residue_numbering(pdb_path, model_path, model_path)\n",
    "            print(f\"Residue numbering adjusted. File saved at {model_path}\")\n",
    "        else:\n",
    "            # No reconstruction needed, copy original PDB\n",
    "            shutil.copy(pdb_path, os.path.join(target_path, f\"{pdb_chain_id}.pdb\"))\n",
    "            print(f\"No differences found for {pdb_chain_id}. Original PDB copied to target directory.\")\n",
    "    else:\n",
    "        print(f\"Motifs not found in {pdb_chain_id}.\")\n",
    "\n",
    "def adjust_residue_numbering(original_pdb, reconstructed_pdb, output_pdb):\n",
    "    \"\"\"\n",
    "    Adjusts the residue numbering in the reconstructed PDB file to match the original PDB file.\n",
    "    \n",
    "    Parameters:\n",
    "    original_pdb (str): Path to the original PDB file.\n",
    "    reconstructed_pdb (str): Path to the reconstructed PDB file.\n",
    "    output_pdb (str): Path where the adjusted PDB file will be saved.\n",
    "    \"\"\"\n",
    "    # Read original PDB file to get the original residue numbering\n",
    "    original_residues = []\n",
    "    with open(original_pdb, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"ATOM\") or line.startswith(\"HETATM\"):\n",
    "                res_num = int(line[22:26].strip())\n",
    "                if res_num not in original_residues:\n",
    "                    original_residues.append(res_num)\n",
    "\n",
    "    # Read the reconstructed PDB file and adjust the residue numbering\n",
    "    with open(reconstructed_pdb, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    adjusted_lines = []\n",
    "    current_res_index = 0\n",
    "    last_res_id = None\n",
    "    for line in lines:\n",
    "        if line.startswith(\"ATOM\") or line.startswith(\"HETATM\"):\n",
    "            # Get the current residue identifier (residue number and chain)\n",
    "            res_id = line[17:26]  # Includes residue name, chain ID, and residue number\n",
    "\n",
    "            # Check if we are still on the same residue\n",
    "            if res_id != last_res_id:\n",
    "                # Increment residue index only when moving to a new residue\n",
    "                if current_res_index < len(original_residues):\n",
    "                    current_res_index += 1\n",
    "                last_res_id = res_id\n",
    "\n",
    "            # Map the reconstructed residue number to the original one\n",
    "            new_res_num = original_residues[current_res_index - 1]  # -1 because we incremented before\n",
    "\n",
    "            # Create a new line with the adjusted residue number\n",
    "            new_line = line[:22] + f\"{new_res_num:4}\" + line[26:]\n",
    "            adjusted_lines.append(new_line)\n",
    "        else:\n",
    "            adjusted_lines.append(line)\n",
    "\n",
    "    # Write the adjusted PDB to the output file\n",
    "    with open(output_pdb, 'w') as file:\n",
    "        file.writelines(adjusted_lines)\n",
    "\n",
    "    print(f\"Residue numbering adjusted and saved to {output_pdb}\")\n",
    "\n",
    "def main():\n",
    "    seqres_fasta = \"seqres_sequences.fasta\"\n",
    "    pdb_dir = \"Results/activation_segments/unaligned\"\n",
    "    target_dir = \"Results/activation_segments/reconstructedModeller\"\n",
    "\n",
    "    # Read the sequences from the FASTA file\n",
    "    seqres_sequences = read_fasta_sequences(seqres_fasta)\n",
    "\n",
    "    for header, full_sequence in seqres_sequences.items():\n",
    "        if \"_SEQRES\" in header:  # Only consider SEQRES entries\n",
    "            pdb_chain_id = header.replace(\"_SEQRES\", \"\")\n",
    "            pdb_file_path = os.path.join(pdb_dir, f\"{pdb_chain_id}.pdb\")\n",
    "\n",
    "            # Extract the atomic sequence\n",
    "            atom_sequence = extract_atom_sequence(pdb_file_path)\n",
    "\n",
    "            if atom_sequence is None:\n",
    "                print(f\"Could not extract sequence for {pdb_chain_id}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(target_dir):\n",
    "                os.makedirs(target_dir)\n",
    "            \n",
    "            reconstruct_with_modeller(pdb_chain_id, pdb_file_path, target_dir, full_sequence, atom_sequence)\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of pdb files in the directory after reconstruction\n",
    "import glob\n",
    "import os\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'Results/activation_segments/reconstructedModeller'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119237e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast checking if reconstruction was successful, need to just change the pdb id and chain id\n",
    "import MDAnalysis as mda\n",
    "import nglview as nv\n",
    "from Bio.PDB import PDBParser\n",
    "from MDAnalysis.analysis import align\n",
    "\n",
    "# Dictionary to convert three-letter amino acid codes to one-letter codes\n",
    "three_to_one = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D',\n",
    "    'CYS': 'C', 'GLU': 'E', 'GLN': 'Q', 'GLY': 'G',\n",
    "    'HIS': 'H', 'ILE': 'I', 'LEU': 'L', 'LYS': 'K',\n",
    "    'MET': 'M', 'PHE': 'F', 'PRO': 'P', 'SER': 'S',\n",
    "    'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "}\n",
    "\n",
    "def extract_sequence_and_mapping(pdb_file):\n",
    "    \"\"\"Extract sequence and create a mapping from sequence index to PDB residue ID.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    sequence = []\n",
    "    index_to_resid = {}\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if 'CA' in residue:  # Check if it's an amino acid\n",
    "                    resname = residue.get_resname()\n",
    "                    if resname in three_to_one:\n",
    "                        sequence.append(three_to_one[resname])\n",
    "                        index_to_resid[len(sequence) - 1] = residue.get_id()[1]  # Map sequence index to PDB resid\n",
    "            break\n",
    "        break\n",
    "\n",
    "    return sequence, index_to_resid\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    sequence_str = ''.join(sequence)\n",
    "    index = sequence_str.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "# Extract the sequence and mapping for the single chain\n",
    "atom_sequence, index_to_resid = extract_sequence_and_mapping(\"Results/activation_segments/unaligned/7OPO_A.pdb\")\n",
    "\n",
    "# Find indices of DFG and APE motifs\n",
    "dfg_index = find_motif_indices(atom_sequence, 'DFG')\n",
    "ape_index = find_motif_indices(atom_sequence, 'APE')\n",
    "\n",
    "# Ensure indices are found and select the residues between them\n",
    "if dfg_index is not None and ape_index is not None:\n",
    "    # Use the mapping to get the correct residue IDs\n",
    "    dfg_resid = index_to_resid[dfg_index]\n",
    "    ape_resid = index_to_resid[ape_index + 2]  # +2 to include the entire 'APE' motif\n",
    "\n",
    "    u_missing = mda.Universe(\"Results/activation_segments/unaligned/7OPO_A.pdb\")\n",
    "    selected_atoms = u_missing.select_atoms(f\"resid {dfg_resid}:{ape_resid}\")\n",
    "\n",
    "    print(\"Number of Atoms Selected:\", selected_atoms.n_atoms)\n",
    "\n",
    "    u_reconstructed = mda.Universe(\"Results/activation_segments/reconstructedModeller/7OPO_A_filled.pdb\")\n",
    "    print(\"Number of Atoms Reconstructed:\", u_reconstructed.select_atoms(f\"all\").n_atoms)\n",
    "\n",
    "    # Merge the aligned atoms for visualization\n",
    "    merged = mda.Merge(selected_atoms, u_reconstructed.atoms)\n",
    "    print(merged.residues)\n",
    "\n",
    "    # Create NGLView widget\n",
    "    w = nv.show_mdanalysis(merged)\n",
    "\n",
    "    # Add a representation for each residue name with the corresponding color\n",
    "    w.clear()\n",
    "    w.add_cartoon(color=\"resname\")\n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"Motifs not found in the sequence.\")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions and function to run MUSTANG on the reconstructed pdb files\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from glob import glob as g\n",
    "import mdtraj as md\n",
    "from mpi4py import MPI\n",
    "from time import time as t\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sg(f_p):\n",
    "    return sorted(g(f_p))\n",
    "\n",
    "\n",
    "def find_pdbs(directory):\n",
    "    \"\"\"\n",
    "    Find topologies in a directory.\n",
    "    Currently excludes cif files.\n",
    "    \"\"\"\n",
    "    return sg(directory+\"/*.pdb\")\n",
    "\n",
    "\n",
    "def fname(file):\n",
    "    return file.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1]\n",
    "\n",
    "\n",
    "def ifnotmake(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "\n",
    "\n",
    "def run_mustang(f1, f2, name=None):\n",
    "    \"\"\"\n",
    "    Writes a MUSTANG input file which aligns\n",
    "    file1 to file 2.\n",
    "    If no name defaults to the second file.\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = fname(f2)\n",
    "    if not os.path.isdir(f\"./{name}\"):\n",
    "        os.makedirs(f\"./{name}\")\n",
    "    new_fp = f\"./{name}/{name}\"\n",
    "    structs = f\"{f1} {f2} \"\n",
    "    command = f\"/home/marmatt/Downloads/MUSTANG_v3.2.4/bin/mustang-3.2.4 -i {structs} -o {new_fp} -F fasta -s ON\" #here you should change the path to where you install MUSTANG\n",
    "    command = command.split()\n",
    "    new_fp = f\"{new_fp}.pdb\"\n",
    "    try:\n",
    "        command = subprocess.run(command, capture_output=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(command)\n",
    "        return None\n",
    "    return new_fp\n",
    "\n",
    "\n",
    "def postprocess(file_path):\n",
    "    \"\"\"\n",
    "    file_path is the name of a pdb file.\n",
    "    It deletes the first chain which is always the alignment structures\n",
    "    \"\"\"\n",
    "    structure = md.load(file_path)\n",
    "    aligned_chain_idx = [[atom.index for atom in res.atoms] for res in\n",
    "                         structure.top._chains[1]._residues]\n",
    "    aligned_chain_idx = sum(aligned_chain_idx, [])\n",
    "    structure = structure.atom_slice(aligned_chain_idx)\n",
    "    structure.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993598b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform MUSTANG alignment on the sequences that have not been reconstructed, I need to fix this bug\n",
    "\n",
    "#from mustang import *\n",
    "import subprocess\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Constants (most to be made variable)\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "# Define the paths explicitly\n",
    "pdb_path = \"Results/activation_segments/reconstructedModeller\"\n",
    "#pdb_path = \"/home/marmatt/Documents/projects/BRAF/myWork/reproduceBRAFWork/Results/activation_segments/unaligned\"\n",
    "target_dir = \"Results/activation_segments/mustangs\"\n",
    "template_pdb = \"6UAN_chainD.pdb\"\n",
    "# Ensure the target directory exists\n",
    "ifnotmake(target_dir)\n",
    "pdb_path = os.path.abspath(pdb_path)\n",
    "target_dir = os.path.abspath(target_dir)\n",
    "template_pdb = os.path.abspath(template_pdb)\n",
    "print(pdb_path, target_dir, template_pdb)\n",
    "os.chdir(target_dir)\n",
    "os.system(\"pwd\")\n",
    "if rank == 0:\n",
    "    pdbs = find_pdbs(pdb_path)\n",
    "    n_files = len(pdbs)\n",
    "    n_slices = (n_files // size)\n",
    "    step = int(n_files / n_slices)\n",
    "    if n_files % n_slices != 0:\n",
    "        n_slices += 1\n",
    "    slices = [slice(i*n_slices, (i+1)*n_slices) for i in range(step)]\n",
    "    pdbs = [pdbs[s] for s in slices]\n",
    "else:\n",
    "    pdbs = None\n",
    "\n",
    "pdbs = comm.scatter(pdbs, root=0)\n",
    "print(\"RANK:\\t\", rank, \"DATA SIZE:\\t\", len(pdbs))\n",
    "t1 = t()\n",
    "failures = []\n",
    "for pdb in tqdm(pdbs):\n",
    "    name = fname(pdb)\n",
    "    # Check if the name contains \"_filled\" and skip if it does\n",
    "    if \"_filled\" in name:\n",
    "        continue\n",
    "    new_fp = run_mustang(template_pdb, pdb, name=name)\n",
    "    if new_fp:\n",
    "        if os.path.isfile(new_fp):\n",
    "            postprocess(new_fp)\n",
    "        else:\n",
    "            failures.append(pdb)\n",
    "    else:\n",
    "        failures.append(pdb)\n",
    "t2 = t()\n",
    "print(\"FINISHED RANK:\\t\", rank, \"DATA SIZE:\\t\", len(pdbs),\n",
    "      \"TIME:\\t\", round(t2-t1, 4))\n",
    "failures = comm.gather(failures, root=0)\n",
    "if rank == 0:\n",
    "    failures = sum(failures, [])\n",
    "    with open(\"./failures.txt\", \"w\") as f_o:\n",
    "        f_o.write(\"\\n\".join(f for f in failures))\n",
    "    t2 = t()\n",
    "    print(round(t2-t1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14388f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of directories representing the number of pdb files that have been aligned\n",
    "import os\n",
    "def count_directories(directory):\n",
    "    # List all entries in the given directory\n",
    "    entries = os.listdir(directory)\n",
    "\n",
    "    # Use os.path.join to get the full path and os.path.isdir to check if it's a directory\n",
    "    directories = [entry for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "\n",
    "    # Return the count of directories\n",
    "    return len(directories)\n",
    "\n",
    "# Specify the directory\n",
    "directory_path = 'Results/activation_segments/mustangs'\n",
    "\n",
    "# Get the count of directories\n",
    "directory_count = count_directories(directory_path)\n",
    "\n",
    "print(f\"There are {directory_count} directories in the directory '{directory_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of files representing the number of pdb files that did not need reconstruction\n",
    "import os\n",
    "from glob import glob as g\n",
    "\n",
    "def count_non_filled_pdbs(directory):\n",
    "    # Find all PDB files in the directory\n",
    "    pdb_files = g(os.path.join(directory, \"*.pdb\"))\n",
    "\n",
    "    # Filter out files with '_filled' in their names\n",
    "    non_filled_pdbs = [pdb for pdb in pdb_files if \"_filled\" not in os.path.basename(pdb)]\n",
    "\n",
    "    # Return the count of non '_filled' PDB files\n",
    "    return len(non_filled_pdbs)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory_path = 'Results/activation_segments/reconstructedModeller'\n",
    "\n",
    "# Get the count of non '_filled' PDB files\n",
    "non_filled_pdb_count = count_non_filled_pdbs(pdb_directory_path)\n",
    "\n",
    "print(f\"There are {non_filled_pdb_count} PDB files without '_filled' in the directory '{pdb_directory_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pprint import pprint as pp\n",
    "alignment_dir = \"/home/marmatt/Documents/projects/BRAF/myWork/reproduceBRAFWork/Results/activation_segments/mustangs\"\n",
    "\n",
    "class alignment:\n",
    "    \"\"\"\n",
    "    Class to hold alignments.\n",
    "    Currently only supports braf_monomers!\n",
    "    \"\"\"\n",
    "    def __init__(self,name,seq1,seq2):\n",
    "        self.name = name\n",
    "        self.seq1 = seq1\n",
    "        self.seq2 = seq2\n",
    "        self.aligned = self.find_aligned()\n",
    "\n",
    "    def find_pdb(self):\n",
    "        pdb_files = []\n",
    "        for root, dirs, files in os.walk(alignment_dir):\n",
    "            pdb_files += [os.path.join(root, file) for file in files if file.endswith('.pdb')]\n",
    "        \n",
    "        pdb = [f for f in pdb_files if self.name in f]\n",
    "        print(pdb)\n",
    "        if len(pdb) == 1:\n",
    "            return pdb[0]\n",
    "        \n",
    "\n",
    "    def find_aligned(self):\n",
    "        aligned = []\n",
    "        for char1, char2 in zip(self.seq1,self.seq2):\n",
    "            if char1 != \"-\":\n",
    "                aligned.append((char1,char2))\n",
    "        return aligned\n",
    "\n",
    "    def aligned_res(self):\n",
    "        seq1, seq2 = self.seq1, self.seq2\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        seq_length = len(aligned)\n",
    "        pdb2_top = self.load_pdb()\n",
    "        full_seq2 = \"\".join(char for char in seq2 if char != \"-\")\n",
    "        residues = pdb2_top.top._residues\n",
    "        n_res = len(residues)\n",
    "        res_counter = 0\n",
    "        for i in range(seq_length):\n",
    "            if res_counter >= n_res:\n",
    "                break\n",
    "            if aligned[i][1] != \"-\":\n",
    "                aligned[i][1] = residues[res_counter]\n",
    "                res_counter += 1\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def aligned_xyz(self):\n",
    "        \"\"\"\n",
    "        Return xyz of aligned residues\n",
    "        \"\"\"\n",
    "        xyz = self.load_pdb()._xyz[0] # Only one frame\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        for k,(_,res) in enumerate(self.residues):\n",
    "            if not isinstance(res,str):\n",
    "                idxs = []\n",
    "                for atom in res._atoms:\n",
    "                    idxs.append(atom.index)\n",
    "                res_xyz = xyz[idxs]\n",
    "                aligned[k][1] = res_xyz\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def aligned_ca_xyz(self):\n",
    "        \"\"\"\n",
    "        Return xyz of aligned residues\n",
    "        \"\"\"\n",
    "        xyz = self.load_pdb()._xyz[0] # Only one frame\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        for k,(_,res) in enumerate(self.residues):\n",
    "            if not isinstance(res,str):\n",
    "                for atom in res._atoms:\n",
    "                    if atom.name == \"CA\":\n",
    "                        idxs = atom.index\n",
    "                        break\n",
    "                try:\n",
    "                    res_xyz = xyz[idxs]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\"ERROR FOR:\")\n",
    "                    print(self.name)\n",
    "                    return None\n",
    "                aligned[k][1] = res_xyz\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def load_pdb(self):\n",
    "        return md.load(self.pdb_file)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.seq1[idx],self.seq2[idx])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "    def find_match_id(self):\n",
    "        seq1, seq2 = self.seq1, self.seq2\n",
    "        full_seq2 = \"\".join(char for char in seq2 if char != \"-\")\n",
    "        pp(full_seq2)\n",
    "        aligned = self.aligned\n",
    "        actv_low = 155\n",
    "        actv_hgh = 181\n",
    "        match_residues = aligned[actv_low:actv_hgh] # These are what we need\n",
    "        seq2_Seq = [a[1] for a in match_residues if a[1] != \"-\"]\n",
    "        begin_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19861f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afasta_parse(file):\n",
    "    \"\"\"\n",
    "    Parse mustang afasta format output file.\n",
    "    Returns two lists of equal length\n",
    "    \"\"\"\n",
    "    with open(file,\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    names = [l.split(\".\")[0][1:] for l in lines if l[0] == \">\"]\n",
    "    for i in range(1,len(lines)):\n",
    "        if lines[i].isspace():\n",
    "            lines[i] = \"BREAK\"\n",
    "            break\n",
    "        elif lines[i][0] == \">\":\n",
    "            lines[i] = \"BREAK\" + lines[i]\n",
    "            break\n",
    "    lines = [l.strip() for l in lines if l[0] != \">\"]\n",
    "    lines = \"\".join(lines)\n",
    "    fastas = lines.split(\"BREAK\")\n",
    "    fastas = [*filter(None,fastas)]\n",
    "    return fastas[0], fastas[1]\n",
    "\n",
    "def load_alignments(kind=\"mustang\"):\n",
    "    if kind==\"mustang\":\n",
    "        ppath = \"/home/marmatt/Documents/projects/BRAF/myWork/reproduceBRAFWork/Results/activation_segments/mustangs/mustang_alignments.fasta\"\n",
    "        #print(\"Loading pickled alignments...\")\n",
    "    elif kind==\"blast\":\n",
    "        ppath = \"blast_alignments.fasta\"\n",
    "    if os.path.isfile(ppath):\n",
    "        with open(ppath,\"rb\") as pickled:\n",
    "            #print(\"Loading pickled alignments...\")\n",
    "            return p.load(pickled)\n",
    "    else:\n",
    "        make_align_pickle()\n",
    "        #print(\"No pickled alignments found. Creating...\")\n",
    "        return load_alignments()\n",
    "    \n",
    "def make_align_pickle(kind=\"mustang\"):\n",
    "    if kind == \"mustang\":\n",
    "\n",
    "        alignments = []\n",
    "\n",
    "        # Iterate over directories in the alignment directory\n",
    "        for directory_name in os.listdir(alignment_dir):\n",
    "            directory_path = os.path.join(alignment_dir, directory_name)\n",
    "\n",
    "            # Ensure we are working with directories\n",
    "            if os.path.isdir(directory_path):\n",
    "                #print(f\"Processing directory: {directory_name}\")\n",
    "                fasta_files = tqdm(glob(os.path.join(directory_path, \"*.afasta\")), desc=f\"Processing {directory_name} .afasta files\")\n",
    "                \n",
    "                for fasta_file in fasta_files:\n",
    "                    name = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "                    fasta_files.set_description(f\"Working on {name}\")\n",
    "\n",
    "                    # Simulate the alignment logic\n",
    "                    aligned = alignment(name, *afasta_parse(fasta_file))  # Assuming `alignment` and `afasta_parse` are predefined\n",
    "                    alignments.append(aligned)\n",
    "\n",
    "        # Define a path for the output pickle file\n",
    "        ppath = os.path.join(alignment_dir, \"mustang_alignments.fasta\")\n",
    "        with open(ppath, \"wb\") as pickled:\n",
    "            p.dump(alignments, pickled)\n",
    "    \n",
    "    elif kind == \"blast\":\n",
    "        b = BLAST_results()\n",
    "        alignments = []\n",
    "        for k, dicti in tqdm(b.alignments.items(),total=len(b.alignments)):\n",
    "            seq1 = dicti[\"Query\"]\n",
    "            seq2 = dicti[\"Subject\"]\n",
    "            alignments.append(alignment(k,seq1,seq2))\n",
    "        ppath = \"blast_alignments.fasta\"\n",
    "        with open(ppath, \"wb\") as pickled:\n",
    "            p.dump(alignments, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a51087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from glob import glob as g\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams.update({'font.size': 8})\n",
    "kind = \"mustang\"\n",
    "make_align_pickle(kind) #create MSA file --> important, substituted .p with .html in alignment class\n",
    "aligned = load_alignments(kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a71ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some helper functions to extract the sequence from the pdb files and to extract the sequence from the alignment files\n",
    "def braf_res():\n",
    "    fp = \"./6UAN_chainD.pdb\"\n",
    "    top = md.load(fp).top\n",
    "    return [res_namer(res) for res in top.residues]\n",
    "\n",
    "\n",
    "def res_namer(res):\n",
    "    return f\"{res.name}-{res.resSeq}\"\n",
    "\n",
    "def fname(fp):\n",
    "    return fp.rsplit(\".\",1)[0].rsplit(\"/\",1)[-1]\n",
    "\n",
    "def make_seg(a):\n",
    "    seq = [t for t in a.aligned if t[0] != \"-\"]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting the number of aligned residues over the BRAF BLAST search results to show what are the most conserved residues throughout the alignment\n",
    "\"\"\"\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "seq1mag = len(aligned[0].seq1.replace(\"-\",\"\"))\n",
    "counts = np.zeros(seq1mag)\n",
    "for a in aligned:\n",
    "    segment = make_seg(a)\n",
    "    for i,(b,c) in enumerate(segment):\n",
    "        if c != \"-\":\n",
    "            counts[i] += 1\n",
    "counts =  counts / max(counts)\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "fig,ax = plt.subplots(1,figsize=(10,5))\n",
    "x = [*range(len(counts))]\n",
    "ax.set_xticks(x[::5])\n",
    "x_lbl = braf_res()\n",
    "ax.set_xticklabels(x_lbl[::5],rotation=90,fontsize=7)\n",
    "# subtract 11\n",
    "ax.axvspan(36,48, facecolor='g', alpha=0.5)\n",
    "ax.axvspan(92,100, facecolor='c', alpha=0.5)\n",
    "ax.axvspan(144,168, facecolor='r', alpha=0.5)\n",
    "ax.axvspan(177,186, facecolor='y', alpha=0.5)\n",
    "ax.axvspan(204,215, facecolor='pink', alpha=0.8)\n",
    "ax.axvspan(222,240, facecolor='dodgerblue', alpha=0.8)\n",
    "ax.bar(x,counts,linewidth=0.05,width=1)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n",
    "title = ax.set_title(\"Number of MUSTANG aligned residues over the BRAF BLAST search results\")\n",
    "ax1 = plt.xlabel(\"Resiude Name-Number\")\n",
    "ax1 = plt.ylabel(\"Percent matching in structural alignments\")\n",
    "ax.tick_params(length=2,color=\"black\",direction=\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell finds PDBs with DFG and APE motifs aligned\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "count = 0\n",
    "new_aligned = []\n",
    "bad = []\n",
    "for k,a in enumerate(aligned):\n",
    "    # If doesn't have anything matched totally to DFG and APE discard it\n",
    "    if a.seq1.find(\"DFG\") == -1 or a.seq1.find(\"APE\") == -1:\n",
    "        # print(a.name)\n",
    "        bad.append(aligned)\n",
    "        count += 1\n",
    "    else:\n",
    "        new_aligned.append(a)\n",
    "print(count,\"/\",len(aligned), \" don't match.\")\n",
    "print(f\"Continuing with {len(new_aligned)} structures\")\n",
    "\n",
    "counts  = {}\n",
    "counter = 0\n",
    "new_aligned2 = []\n",
    "aligning_segs = {}\n",
    "lengths = []\n",
    "for a in new_aligned:\n",
    "    DFG_index = a.seq1.find(\"DFG\")\n",
    "    APE_index = a.seq1.find(\"APE\")\n",
    "    dfg = a.seq2[DFG_index]\n",
    "    ape = a.seq2[APE_index]\n",
    "    if dfg != \"-\" and ape != \"-\":\n",
    "        counter+=1\n",
    "        length = APE_index - DFG_index\n",
    "        counts.setdefault(length,[]).append((a.name,DFG_index,APE_index))\n",
    "        new_aligned2.append(a)\n",
    "        aligning_segs.setdefault(a.name,\n",
    "                                 (a.seq2[DFG_index:DFG_index+3],\n",
    "                                  a.seq2[APE_index:APE_index+3],\n",
    "                                  a.seq2[DFG_index:APE_index]))\n",
    "        lengths.append(len(a.seq2[DFG_index:APE_index]))\n",
    "    else:\n",
    "        pass\n",
    "        # print(a.name,dfg,ape)\n",
    "print(f\"{counter} structures with an alignment to the D and A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d518b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import mdtraj as md\n",
    "\n",
    "\"\"\"\n",
    "Strips to CAs\n",
    "\"\"\"\n",
    "def align_to_actv(pdb,\n",
    "                  alignment,\n",
    "                  align_distance=50):\n",
    "    pdb = md.load(pdb)\n",
    "    DFG_index = alignment.seq1.find(\"DFG\")\n",
    "    APE_index = alignment.seq1.find(\"APE\") + 2 \n",
    "    \n",
    "    '''The following is to make sure that gaps are not counted in the index'''\n",
    "    DFG_index = DFG_index - sum([1 for a in alignment.seq2[:DFG_index] if a == \"-\"])\n",
    "    APE_index = APE_index - sum([1 for a in alignment.seq2[:APE_index] if a == \"-\"])\n",
    "    \n",
    "    actv_range = range(DFG_index, APE_index)\n",
    "    \n",
    "    \n",
    "    atoms_indices = [[atom.index for atom in res.atoms if atom.name == \"CA\"] for res in pdb.top._residues if res.index in actv_range]\n",
    "    atoms_indices = sum(atoms_indices, [])\n",
    "    return pdb.atom_slice(atoms_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65927bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following block of code is the same as in the case of align_to_actv version with top_seq_matcher'''\n",
    "names = []\n",
    "rmsds = []\n",
    "target_dir = f\"Results/activation_segments/CA_segments/{kind}/\"\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Get PDB names\n",
    "pdb_dir = \"Results/activation_segments/mustangs/\"\n",
    "fps = glob(pdb_dir+\"/*/*.pdb\")\n",
    "\n",
    "# Process alignments\n",
    "for align_obj in tqdm(new_aligned2):\n",
    "    match_found = False\n",
    "    for fp in fps:\n",
    "        if align_obj.name in fp and \"pdb\" in fp:\n",
    "            match_found = True\n",
    "            break\n",
    "    \n",
    "    if not match_found:\n",
    "        continue\n",
    "    \n",
    "    stripped = align_to_actv(fp, align_obj)\n",
    "    \n",
    "    if stripped.n_residues > 35 or stripped.n_residues < 10:\n",
    "        continue\n",
    "    \n",
    "    # Construct the file name and save the stripped pdb\n",
    "    new_name = os.path.join(target_dir, align_obj.name + \".pdb\")\n",
    "    stripped.save(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The following block of code is the same as in the case of align_to_actv version with top_seq_matcher'''\n",
    "names = []\n",
    "rmsds = []\n",
    "target_dir = f\"Results/activation_segments/CA_segments/{kind}/\"\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Get PDB names\n",
    "pdb_dir = \"Results/activation_segments/mustangs/\"\n",
    "fps = glob(pdb_dir+\"/*/*.pdb\")\n",
    "\n",
    "# Process alignments\n",
    "for align_obj in tqdm(new_aligned2):\n",
    "    match_found = False\n",
    "    for fp in fps:\n",
    "        if align_obj.name in fp and \"pdb\" in fp:\n",
    "            match_found = True\n",
    "            break\n",
    "    \n",
    "    if not match_found:\n",
    "        continue\n",
    "    print(fp)\n",
    "    stripped = align_to_actv(fp, align_obj)\n",
    "    \n",
    "    if stripped.n_residues > 35 or stripped.n_residues < 10:\n",
    "        continue\n",
    "    \n",
    "    # Construct the file name and save the stripped pdb\n",
    "    new_name = os.path.join(target_dir, align_obj.name + \".pdb\")\n",
    "    stripped.save(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d358bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is right and it works, but renumbers residues meaning that the ones that are missing are ignored\n",
    "\n",
    "from Bio import PDB\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "import tempfile\n",
    "import mdtraj as md\n",
    "from scipy.interpolate import make_interp_spline\n",
    "def fitting_code(fp_or_traj, save_path):\n",
    "    # Function to read PDB file or trajectory object and get model\n",
    "    def read_structure(input_data):\n",
    "        if isinstance(input_data, str):\n",
    "            # If input is a string, treat it as a file path\n",
    "            parser = PDB.PDBParser(QUIET=True)\n",
    "            structure = parser.get_structure('structure', input_data)\n",
    "        elif isinstance(input_data, md.Trajectory):\n",
    "            # If input is a trajectory, save to temp PDB and read\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdb\", delete=False) as tmpfile:\n",
    "                input_data.save(tmpfile.name)\n",
    "                tmpfile.close()\n",
    "                parser = PDB.PDBParser(QUIET=True)\n",
    "                structure = parser.get_structure('structure', tmpfile.name)\n",
    "            os.unlink(tmpfile.name)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input type. Provide a file path or md.Trajectory.\")\n",
    "        return structure[0]\n",
    "\n",
    "    # Read the template for CA atoms configuration\n",
    "    template_model = read_structure('template.pdb')\n",
    "    Nnew = len([atom for atom in template_model.get_atoms() if atom.get_id() == 'CA'])\n",
    "\n",
    "    # Read input PDB file or trajectory\n",
    "    my_model = read_structure(fp_or_traj)\n",
    "    atom_list = [atom for atom in my_model.get_atoms() if atom.get_id() == 'CA']\n",
    "    #print(f'shape atom list in input{np.shape(atom_list)}')\n",
    "\n",
    "    n = len(atom_list)\n",
    "    avg = np.array([[atom.coord[0], atom.coord[1], atom.coord[2]] for atom in atom_list])\n",
    "    #print(f'shape atom coordinates in input{np.shape(avg)}')\n",
    "\n",
    "    \n",
    "    dims = ['x', 'y', 'z']\n",
    "    fits = {}\n",
    "    #splines = {}\n",
    "    # Fit cubic interpolation for each axis\n",
    "    for j, dim in enumerate(dims):\n",
    "        #splines[dim] = make_interp_spline(np.arange(n), avg[:, j], k=3)  # 'k=3' for cubic spline \n",
    "        fits[dim] = interp1d(np.arange(n), avg[:, j], kind='cubic', fill_value='extrapolate')\n",
    "    print(avg)\n",
    "    #print(f'fits{fits}')\n",
    "    # Interpolation scales\n",
    "    X = np.arange(0, n-1, 0.1)\n",
    "    #X = np.arange(0, n - 0.2, 0.1)\n",
    "\n",
    "    # Get derivative\n",
    "    dYdX = {dim: np.gradient(fits[dim](X)) for dim in dims}\n",
    "    #evaluated_splines = {dim: np.gradient(splines[dim](X)) for dim in dims}\n",
    "    #print(f'derivative{dYdX}')\n",
    "    # Calculate path length\n",
    "    Y = np.sqrt(sum(np.square(dYdX[dim]) for dim in dims))\n",
    "    #Y = np.sqrt(sum(np.square(evaluated_splines[dim]) for dim in dims))\n",
    "    L = np.trapz(Y, X)\n",
    "\n",
    "    # Create an interpolated arc length\n",
    "    Li = np.linspace(0, L, Nnew)\n",
    "    \n",
    "    # Calculate the arc length for each sampled point\n",
    "    flen = np.array([np.trapz(Y[:ibig], X[:ibig]) for ibig in range(1, len(X))])\n",
    "\n",
    "    pt = np.zeros(Nnew, dtype=int)\n",
    "    for i in range(Nnew):\n",
    "        pt[i] = np.argmin(np.abs(flen - Li[i]))\n",
    "\n",
    "    new_coords = np.array([[fits[dim](X[pt[i]]) for dim in dims] for i in range(Nnew)])\n",
    "    #new_coords = np.array([[splines[dim](X[pt[i]]) for dim in dims] for i in range(Nnew)])\n",
    "    ca_index = 0\n",
    "    for atom in template_model.get_atoms():\n",
    "            if atom.get_id() == 'CA':\n",
    "                atom.set_coord(new_coords[ca_index])\n",
    "                ca_index += 1\n",
    "        \n",
    "        # Now let's write the updated PDB to the file\n",
    "    try:\n",
    "        with open(save_path, \"w\") as file:\n",
    "            io = PDB.PDBIO()\n",
    "            io.set_structure(template_model)\n",
    "            io.save(file)\n",
    "        print(f'Successfully saved the structure to {save_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file save: {e}\")\n",
    "    pt = np.zeros(Nnew, dtype=int)\n",
    "    for i in range(Nnew):\n",
    "        pt[i] = np.argmin(np.abs(flen - Li[i]))\n",
    "\n",
    "    new_coords = np.array([[fits[dim](X[pt[i]]) for dim in dims] for i in range(Nnew)])\n",
    "    #new_coords = np.array([[splines[dim](X[pt[i]]) for dim in dims] for i in range(Nnew)])\n",
    "    ca_index = 0\n",
    "    for atom in template_model.get_atoms():\n",
    "            if atom.get_id() == 'CA':\n",
    "                atom.set_coord(new_coords[ca_index])\n",
    "                ca_index += 1\n",
    "        \n",
    "        # Now let's write the updated PDB to the file\n",
    "    try:\n",
    "        with open(save_path, \"w\") as file:\n",
    "            io = PDB.PDBIO()\n",
    "            io.set_structure(template_model)\n",
    "            io.save(file)\n",
    "        print(f'Successfully saved the structure to {save_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file save: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fc504-c470-43da-9076-5d83bac25afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "# Start MATLAB\n",
    "#import matlab.engine\n",
    "from tqdm.notebook import tqdm\n",
    "TARGET_DIR = \"Results/fitted_matlab_segments/mustang\"\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "def pdb_id(fp):\n",
    "    return fp.rsplit(\".\",1)[0].rsplit(\"/\",1)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa60c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_PATH = \"Results/activation_segments/CA_segments/mustang/\"\n",
    "#xyz = md.load(\"/home/marmatt/Documents/projects/BRAF/results/activation_segments/CA_segments/mustang/1A9U_A.pdb\")\n",
    "files = glob(PDB_PATH+\"/*.pdb\")\n",
    "ids = [pdb_id(f) for f in files]\n",
    "print(ids)\n",
    "out = [TARGET_DIR + f\"/{i}.pdb\" for i in ids]\n",
    "#print(files[0],ids[0],out[0])\n",
    "\n",
    "for f, n in tqdm(zip(files,out),total=len(files)):\n",
    "    fitting_code(f,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mdtraj as md\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "xyz = md.load(\"Results/activation_segments/CA_segments/mustang/1AD5_A.pdb\")\n",
    "\n",
    "DFG = xyz.top.to_fasta()[0].find(\"DFG\")\n",
    "APE = xyz.top.to_fasta()[0].find(\"APE\")+2\n",
    "atoms = sum([[atom.index for atom in res.atoms if atom.name == \"CA\"] for res in xyz.top._residues[:]],[])\n",
    "\n",
    "coords = xyz.xyz[0,atoms].T\n",
    "x = coords[0]\n",
    "y = coords[1]\n",
    "z = coords[2]\n",
    "new_coords = md.load(\"Results/fitted_matlab_segments/mustang/1AD5_A.pdb\")\n",
    "\n",
    "atoms = sum([[atom.index for atom in res.atoms if atom.name == \"CA\"] for res in new_coords.top._residues[:]],[])\n",
    "new_coords = new_coords.xyz[0,atoms].T\n",
    "xp = new_coords[0]\n",
    "yp = new_coords[1]\n",
    "zp = new_coords[2]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x,y,z, 'blue',marker=\"o\")\n",
    "ax.plot3D(xp, yp, zp, 'red')\n",
    "plt.tick_params(bottom=False, top=False, labelbottom=False)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
