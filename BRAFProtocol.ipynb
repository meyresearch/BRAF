{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from urllib.request import urlretrieve as download\n",
    "from glob import glob as g\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from mypdb import PDB_file as mypdb\n",
    "from Bio.Blast.Applications import NcbipsiblastCommandline\n",
    "from time import time as t\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "from time import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to handle and download the hits from a blast search\n",
    "\n",
    "\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    for k2, v2 in dict2.items():\n",
    "        if k2 in dict1:\n",
    "            if isinstance(dict1[k2], list):\n",
    "                dict1[k2].append(v2)\n",
    "            else:\n",
    "                dict1[k2] = [dict1[k2], v2]\n",
    "        else:\n",
    "            print(f\"New Key : {k2}\")\n",
    "            dict1[k2] = v2\n",
    "    return dict1\n",
    "\n",
    "class hit:\n",
    "    def __init__(self, xml_hit):\n",
    "        self.hit_num = xml_hit[0].text\n",
    "        pdb_info = xml_hit[1].text.split(\"|\")[1:]\n",
    "        self.pdb_id = pdb_info[0]\n",
    "        self.pdb_chain = pdb_info[1]\n",
    "        self.description = xml_hit[2].text\n",
    "        data = [[y.tag.replace(\"Hsp_\", \"\"), y.text] for y in xml_hit[5][0]]\n",
    "        self.data = dict(data)\n",
    "        self.pdb = None\n",
    "        self._not_exists = False\n",
    "\n",
    "    @staticmethod\n",
    "    def download2(code, pdir=None):\n",
    "        base_url = \"https://files.rcsb.org/download\"\n",
    "        pdb_url = f\"{base_url}/{code}.pdb\"\n",
    "        f_p = os.path.join(pdir, f\"{code}.pdb\")\n",
    "        try:\n",
    "            download(pdb_url, f_p)\n",
    "            return f_p  # Return the file path if succeeded\n",
    "        except Exception:\n",
    "            print(f\"File {code} not found.\")\n",
    "            return None  # Return None if failed\n",
    "\n",
    "    def _assign(self, f_p):\n",
    "        self.pdb_path = f_p\n",
    "        self.pdb = mypdb(f_p)\n",
    "        \n",
    "    def check_for_pdb(self, pdir=None):\n",
    "        if pdir is None:\n",
    "            pdir = \"./pdbs/\"\n",
    "        elif pdir[-1] != \"/\":\n",
    "            pdir += \"/\"\n",
    "        if not os.path.isdir(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "        matches = g(pdir + f\"{self.pdb_id}*\")\n",
    "        if matches:\n",
    "            if not self.pdb:\n",
    "                self._assign(matches[0])\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2139fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to handle the blast search\n",
    "class blast:\n",
    "    def __init__(self, sequence, name, database, path, program=\"blastp\", hitlen=10000):\n",
    "        self.sequence = sequence\n",
    "        self.program = program\n",
    "        self.database = database\n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.hitlen = hitlen\n",
    "\n",
    "        if program == \"blastp\" and (self.path and os.path.isfile(self.path)):\n",
    "            self.parse_search()\n",
    "        elif program == \"psiblast\":\n",
    "            self.psiblast_search()\n",
    "        else:\n",
    "            self.bsearch()\n",
    "\n",
    "    def bsearch(self):\n",
    "        # Perform the initial BLASTP search\n",
    "        if self.program == \"blastp\":\n",
    "            print(\"Searching BLASTP...\")\n",
    "            t1 = t()\n",
    "            self.results = NCBIWWW.qblast(self.program, self.database, self.sequence, hitlist_size=self.hitlen)\n",
    "            t2 = t()\n",
    "            print(f\"BLASTP took {round(t2-t1,4)} seconds\")\n",
    "            if not self.path:\n",
    "                self.path = f\"{self.name}-blast.xml\"\n",
    "            with open(self.path, \"w\") as output_xml:\n",
    "                output_xml.write(self.results.read())\n",
    "            self.parse_search()\n",
    "\n",
    "    def psiblast_search(self):\n",
    "        # Run local PSI-BLAST using NcbipsiblastCommandline\n",
    "        print(f\"Running PSI-BLAST on {self.name}...\")\n",
    "        input_fasta = f\"{self.name}.fasta\"\n",
    "        with open(input_fasta, \"w\") as f:\n",
    "            f.write(f\">query\\n{self.sequence}\\n\")\n",
    "\n",
    "        psiblast_cline = NcbipsiblastCommandline(\n",
    "            query=input_fasta,\n",
    "            db=\"/home/marmatt/ncbi-blast-2.16.0+/bin/pdbaa\",\n",
    "            evalue=10,\n",
    "            num_iterations=3,\n",
    "            out_ascii_pssm=f\"{self.name}.pssm\",\n",
    "            out=f\"{self.name}-psiblast.xml\",\n",
    "            outfmt=5\n",
    "        )\n",
    "        stdout, stderr = psiblast_cline()\n",
    "        if stderr:\n",
    "            print(f\"PSI-BLAST ERROR: {stderr}\")\n",
    "        else:\n",
    "            print(\"PSI-BLAST search completed.\")\n",
    "            self.parse_search(xml_file=f\"{self.name}-psiblast.xml\")\n",
    "\n",
    "    def parse_search(self, xml_file=None):\n",
    "        xml_file = xml_file or self.path\n",
    "        if not xml_file:\n",
    "            raise Exception(\"No XML file path provided.\")\n",
    "        t1 = t()\n",
    "        tree = ET.parse(xml_file)\n",
    "        iteration = tree.findall(\"./BlastOutput_iterations/Iteration/\")\n",
    "        self.query_length = iteration[3].text\n",
    "        hits = [hit(x) for x in iteration[-2]] #returns hit objects\n",
    "        self.hits = hits\n",
    "        mega_dict = hits[0].data\n",
    "        for x in hits[1:]:\n",
    "            mega_dict = merge_dicts(mega_dict, x.data)\n",
    "        mega_dict[\"PDB ID\"] = [x.pdb_id for x in hits]\n",
    "        mega_dict[\"Chain\"] = [x.pdb_chain for x in hits]\n",
    "        mega_dict[\"Description\"] = [x.description for x in hits]\n",
    "        self.df = pd.DataFrame.from_dict(mega_dict)\n",
    "        print(self.df)\n",
    "        t2 = t()\n",
    "        print(f\"Time taken to parse {t2-t1}\")\n",
    "    \n",
    "    def download_pdbs(self, pdir=None):\n",
    "        default_dir = \"./PDBs\"\n",
    "        pdir = os.path.abspath(pdir if pdir else default_dir)\n",
    "        if not os.path.isdir(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "        \n",
    "        files = [os.path.splitext(f)[0] for f in os.listdir(pdir)]\n",
    "        hit_bar = tqdm(self.hits, desc=\"Processing Hits\")\n",
    "        \n",
    "        for x in hit_bar:\n",
    "            if x.pdb_id not in files:\n",
    "                hit_bar.set_description(f\"Downloading {x}\")\n",
    "                \n",
    "                try:\n",
    "                    file_path = x.download2(x.pdb_id, pdir=pdir)\n",
    "\n",
    "                    if file_path:\n",
    "                        x._assign(file_path)\n",
    "                    else:\n",
    "                        # In case download2 does not return a valid path\n",
    "                        raise Exception(\"Download failed\")\n",
    "                except Exception as e:\n",
    "                    # Print a message if download fails or file path is invalid\n",
    "                    print(f\"Structure {x.pdb_id} was not found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f2b43",
   "metadata": {},
   "source": [
    "## Actually running the blast search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Here we perform a blast search on:\n",
    " 1.   The BRAF monomer\n",
    "\"\"\"\n",
    "with open(\"./fastas.txt\") as f:\n",
    "    fastas = f.readlines()\n",
    "\n",
    "braf_fasta = fastas[1]\n",
    "name = \"braf\"\n",
    "\n",
    "# Directory for storing blast search results\n",
    "output_dir = \"./blast_search\"\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "res_path = os.path.join(output_dir, f\"{name}-blast.xml\")  # Path for the results file\n",
    "\n",
    "# Check for saved results in the specified directory\n",
    "if os.path.exists(res_path):\n",
    "    bs = blast(braf_fasta, name, database=\"pdb\", path=res_path)\n",
    "else:\n",
    "    bs = blast(braf_fasta, name, database=\"pdb\", path=None)\n",
    "\n",
    "bs.download_pdbs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84c34d",
   "metadata": {},
   "source": [
    "## Counting the number of pdb files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'PDBs'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58070b",
   "metadata": {},
   "source": [
    "## Counting the number of pdb hits in the xml file and checking which ones have not been downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0522d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_pdb_files(directory):\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Extract the base filenames (without extension) to compare with PDB IDs\n",
    "    pdb_file_ids = {os.path.splitext(os.path.basename(f))[0] for f in pdb_files}\n",
    "\n",
    "    return pdb_file_ids\n",
    "\n",
    "def find_unique_and_duplicate_pdb_hit_ids(xml_file):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Dictionary to count occurrences of each PDB hit ID\n",
    "    hit_id_counts = defaultdict(int)\n",
    "\n",
    "    # Iterate over all Hit elements in the XML\n",
    "    for hit in root.findall('.//Hit'):\n",
    "        # Extract the Hit_id text\n",
    "        hit_id = hit.find('Hit_id').text\n",
    "\n",
    "        # Assuming the Hit_id format is 'pdb|PDB_ID|Chain', extract the PDB_ID\n",
    "        pdb_id = hit_id.split('|')[1]\n",
    "\n",
    "        # Increment the count for this PDB_ID\n",
    "        hit_id_counts[pdb_id] += 1\n",
    "\n",
    "    # Find all PDB IDs (unique and duplicates)\n",
    "    all_hit_ids = set(hit_id_counts.keys())\n",
    "\n",
    "    return all_hit_ids\n",
    "\n",
    "# Specify the directory and XML file\n",
    "pdb_directory = 'PDBs'\n",
    "xml_file = 'braf-blast.xml'\n",
    "\n",
    "# Get the PDB file IDs from the directory\n",
    "pdb_file_ids = count_pdb_files(pdb_directory)\n",
    "\n",
    "# Get all PDB hit IDs from the XML\n",
    "all_pdb_ids = find_unique_and_duplicate_pdb_hit_ids(xml_file)\n",
    "\n",
    "# Calculate the number of total PDB hits\n",
    "total_pdb_hits = len(all_pdb_ids)\n",
    "\n",
    "# Find PDB IDs in XML that are not in the directory\n",
    "missing_pdb_ids = all_pdb_ids - pdb_file_ids\n",
    "\n",
    "print(f\"There are {total_pdb_hits} total PDB hits in the file '{xml_file}'.\")\n",
    "print(f\"There are {len(missing_pdb_ids)} PDB IDs in the XML not found in the directory '{pdb_directory}':\")\n",
    "print(missing_pdb_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b9660",
   "metadata": {},
   "source": [
    "## Here we are stripping the downloaded pdb files to only contain the chain of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc269f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "from time import time as t\n",
    "!ls\n",
    "def sglob(fp, absolute=True):\n",
    "    fps = sorted(glob(fp))\n",
    "    if absolute:\n",
    "        fps = [os.path.abspath(f) for f in fps]\n",
    "    return fps\n",
    "\n",
    "def strip_to_chain(pdb_file, chain_ID):\n",
    "    u = mda.Universe(pdb_file)\n",
    "    print(f\"Loaded trajectory from {pdb_file} with {len(u.atoms)} atoms.\")\n",
    "\n",
    "    chain = u.select_atoms(f\"protein and chainID {chain_ID}\")\n",
    "    if len(chain) == 0:\n",
    "        print(f\"Chain {chain_ID} not found in {pdb_file}.\")\n",
    "        return None\n",
    "    return chain\n",
    "\n",
    "def post_process(fname):\n",
    "    with open(fname, \"r\") as f_o:\n",
    "        initial_lines = f_o.readlines()\n",
    "\n",
    "    print(f\"File {fname} before post_process, first few lines:\")\n",
    "    print(\"\".join(initial_lines[:20]))\n",
    "\n",
    "    final_lines = initial_lines[-2:].copy()\n",
    "    no_ter = [line for line in initial_lines if line[:3] != \"TER\" or line in final_lines]\n",
    "\n",
    "    if len(no_ter) != len(initial_lines):\n",
    "        with open(fname, \"w\") as f_o:\n",
    "            print(f\"Rewriting {fname}, lines reduced from {len(initial_lines)} to {len(no_ter)}\")\n",
    "            f_o.write(\"\".join(no_ter))\n",
    "\n",
    "    print(f\"File {fname} after post_process, first few lines:\")\n",
    "    with open(fname, \"r\") as f_r:\n",
    "        print(\"\".join(f_r.readlines()[:20]))\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    hit_id = re.compile(r\"<Hit_id>(.*?)<.Hit_id>\")\n",
    "    with open(xml_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "        results = [h.split(\"|\")[1:] for h in hit_id.findall(text)]\n",
    "        pdb_chain_dict = {}\n",
    "        for r in results:\n",
    "            pdb_chain_dict[r[0]+f\"_{r[1]}\"] = r[1]\n",
    "    return pdb_chain_dict\n",
    "\n",
    "def get_pdb_id(fp):\n",
    "    fp = fp.rsplit(\".\", 1)[0]\n",
    "    if \"/\" in fp:\n",
    "        fp = fp.rsplit(\"/\", 1)[1]\n",
    "    return fp\n",
    "\n",
    "def target_name(fp, target_dir, chain):\n",
    "    orig_path, file_name = fp.rsplit(\"/\", 1)\n",
    "    fp = fp.replace(orig_path, target_dir)\n",
    "    fp = fp.split(\".\")[0] + f\"_{chain}.pdb\"\n",
    "    return fp\n",
    "\n",
    "def find_pdb_file(PDB_chain_id, files):\n",
    "    print(files)\n",
    "    if \"_\" in PDB_chain_id:\n",
    "        PDB_id = PDB_chain_id.split(\"_\")[0]\n",
    "    assert len(PDB_id) == 4\n",
    "    for f in files:\n",
    "        filename = os.path.basename(f).split('.')[0]\n",
    "        if filename.startswith(PDB_id):\n",
    "            print(f\"Found file: {f} for PDB ID: {PDB_id}\")\n",
    "            return f\n",
    "\n",
    "print(\"BEGIN\")\n",
    "t1 = t()\n",
    "xml = \"braf-blast.xml\"\n",
    "pdb_dir = \"PDBs\"\n",
    "target_dir = \"Results/activation_segments/unaligned\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "xml_chain_dict = parse_xml(xml)\n",
    "print(xml_chain_dict)\n",
    "\n",
    "pdb_files = sorted(sglob(\"PDBs/*.pdb\"))\n",
    "print(pdb_files)\n",
    "keys = sorted([*xml_chain_dict.keys()], key=get_pdb_id)\n",
    "\n",
    "print(keys)\n",
    "files = [find_pdb_file(k, pdb_files) for k in keys]\n",
    "print(files)\n",
    "chain_IDs = [xml_chain_dict[k] for k in keys]\n",
    "print(len(chain_IDs))\n",
    "\n",
    "valid_file_chain_pairs = [(f, c) for f, c in zip(files, chain_IDs) if f is not None]\n",
    "print(len(valid_file_chain_pairs))\n",
    "new_file_paths = [target_name(f, target_dir, c) for f, c in valid_file_chain_pairs]\n",
    "print(new_file_paths)\n",
    "file_paths = [f for f, c in valid_file_chain_pairs]\n",
    "print(file_paths)\n",
    "print(len(file_paths))\n",
    "\n",
    "for (fp, chain_ID, tp) in zip(file_paths, chain_IDs, new_file_paths):\n",
    "    if fp is not None:\n",
    "        try:\n",
    "            print(fp, chain_ID, tp)\n",
    "            chain = strip_to_chain(fp, chain_ID)\n",
    "            if chain is not None:\n",
    "                with mda.Writer(tp) as w:\n",
    "                    w.write(chain)\n",
    "                post_process(tp)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {fp} with chain {chain_ID}: {e}\")\n",
    "        \n",
    "t2 = t()\n",
    "t_t = round(t2 - t1, 3) // 60\n",
    "t_t = str((t_t // 60)) + \":\" + str(t_t % 60)\n",
    "print(f\"Time taken {t_t} for sequential processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616153e7",
   "metadata": {},
   "source": [
    "## Counting again how many pdb files are in the directory after stripping the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'Results/activation_segments/unaligned'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e014c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the full sequence from the pdb files, checking if there are any non-natural amino acids and substituting them and selecting only sequences with a maximum gap length of 4 amino acids to be reconstructed\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from Bio.SeqUtils import seq1\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def substitute_non_natural_amino_acid(residue, aligned_atom, index):\n",
    "    \"\"\"Substitute non-natural amino acids with their natural counterparts.\"\"\"\n",
    "    substitutions = {\n",
    "        'X': 'G',  # Glycine\n",
    "        'B': 'N',  # Asparagine\n",
    "        'Z': 'Q',  # Glutamine\n",
    "        'J': 'L'   # Leucine\n",
    "    }\n",
    "    \n",
    "    if residue == 'X':\n",
    "        # Check if 'X' is surrounded by missing amino acids in aligned_atom\n",
    "        if index > 0 and aligned_atom[index - 1] == '-':\n",
    "            return residue\n",
    "        if index < len(aligned_atom) - 1 and aligned_atom[index + 1] == '-':\n",
    "            return residue\n",
    "\n",
    "    return substitutions.get(residue, residue)\n",
    "\n",
    "def extract_seqres_sequence(pdb_file):\n",
    "    \"\"\"Extract SEQRES sequences for each chain from a PDB file.\"\"\"\n",
    "    seq_dict = {}\n",
    "    with open(pdb_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    current_chain = None\n",
    "    current_seq = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"SEQRES\"):\n",
    "            parts = line.split()\n",
    "            chain_id = parts[2]\n",
    "            if chain_id != current_chain:\n",
    "                if current_chain is not None:\n",
    "                    seq_dict[current_chain] = ''.join(seq1(residue) for residue in current_seq)\n",
    "                current_chain = chain_id\n",
    "                current_seq = []\n",
    "            current_seq.extend(parts[4:])\n",
    "\n",
    "    if current_chain is not None:\n",
    "        seq_dict[current_chain] = ''.join(seq1(residue) for residue in current_seq)\n",
    "\n",
    "    return seq_dict\n",
    "\n",
    "def extract_atom_sequence(pdb_file, chain_id):\n",
    "    \"\"\"Extract sequence from atomic coordinates for a specific chain.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        chain = model[chain_id]\n",
    "        \n",
    "        ppb = PPBuilder()\n",
    "        sequence = ''\n",
    "        for pp in ppb.build_peptides(chain):\n",
    "            sequence += pp.get_sequence()\n",
    "        return sequence\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    index = sequence.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "def align_and_highlight_gaps(seqres_segment, atom_segment):\n",
    "    \"\"\"Align SEQRES and ATOM segments and highlight gaps in ATOM.\"\"\"\n",
    "    aligned_seqres = ''\n",
    "    aligned_atom = ''\n",
    "    atom_index = 0\n",
    "    max_gap_length = 0\n",
    "    current_gap_length = 0\n",
    "\n",
    "    for res_seqres in seqres_segment:\n",
    "        if atom_index < len(atom_segment) and res_seqres == atom_segment[atom_index]:\n",
    "            aligned_seqres += res_seqres\n",
    "            aligned_atom += atom_segment[atom_index]\n",
    "            atom_index += 1\n",
    "            current_gap_length = 0\n",
    "        else:\n",
    "            aligned_seqres += res_seqres\n",
    "            aligned_atom += '-'\n",
    "            current_gap_length += 1\n",
    "            max_gap_length = max(max_gap_length, current_gap_length)\n",
    "\n",
    "    return aligned_seqres, aligned_atom, max_gap_length\n",
    "\n",
    "def main():\n",
    "    target_dir = \"Results/activation_segments/unaligned\"\n",
    "    pdb_dir = \"PDBs\"\n",
    "    fasta_output_file = \"seqres_sequences.fasta\"  # File to store full sequences\n",
    "    text_output_file = \"seqres_info.txt\"\n",
    "    aligned_sequences = {}\n",
    "    satisfying_structures_count = 0\n",
    "\n",
    "    pdb_files = glob(os.path.join(target_dir, \"*.pdb\"))\n",
    "\n",
    "    with open(text_output_file, \"w\") as text_output, open(fasta_output_file, \"w\") as fasta_output:\n",
    "        for pdb_file in tqdm(pdb_files, desc=\"Processing PDB files\"):\n",
    "            pdb_name = os.path.basename(pdb_file)\n",
    "            pdb_id, chain_id_file = os.path.splitext(pdb_name)[0].split('_')\n",
    "            chain_id = chain_id_file\n",
    "\n",
    "            full_pdb_path = os.path.join(pdb_dir, pdb_id + '.pdb')\n",
    "            if not os.path.isfile(full_pdb_path):\n",
    "                print(f\"Corresponding full PDB for {pdb_id} not found.\")\n",
    "                continue\n",
    "\n",
    "            seqres_seqs = extract_seqres_sequence(full_pdb_path)\n",
    "            atom_seq = extract_atom_sequence(full_pdb_path, chain_id)\n",
    "\n",
    "            if chain_id in seqres_seqs and atom_seq:\n",
    "                seqres_sequence = seqres_seqs[chain_id]\n",
    "                seqres_dfg_index = find_motif_indices(seqres_sequence, 'DFG')\n",
    "                seqres_ape_index = find_motif_indices(seqres_sequence, 'APE')\n",
    "                atom_dfg_index = find_motif_indices(atom_seq, 'DFG')\n",
    "                atom_ape_index = find_motif_indices(atom_seq, 'APE')\n",
    "\n",
    "                # Determine the start and end indices for the segments\n",
    "                if None not in [seqres_dfg_index, seqres_ape_index, atom_dfg_index, atom_ape_index]:\n",
    "                    seqres_start = min(seqres_dfg_index, seqres_ape_index)\n",
    "                    seqres_end = max(seqres_dfg_index + 3, seqres_ape_index + 3)\n",
    "                    atom_start = min(atom_dfg_index, atom_ape_index)\n",
    "                    atom_end = max(atom_dfg_index + 3, atom_ape_index + 3)\n",
    "\n",
    "                    seqres_segment = seqres_sequence[seqres_start:seqres_end]\n",
    "                    atom_segment = atom_seq[atom_start:atom_end]\n",
    "\n",
    "                    aligned_seqres, aligned_atom, max_gap_length = align_and_highlight_gaps(seqres_segment, atom_segment)\n",
    "                    \n",
    "                    # Check for differences and substitute non-natural amino acids\n",
    "                    exclude_due_to_non_natural_diff = False\n",
    "                    corrected_seqres = ''\n",
    "                    for index, (res_seqres, res_atom) in enumerate(zip(aligned_seqres, aligned_atom)):\n",
    "                        if res_seqres != res_atom:\n",
    "                            corrected_residue = substitute_non_natural_amino_acid(res_seqres, aligned_atom, index)\n",
    "                            corrected_seqres += corrected_residue\n",
    "                            if corrected_residue != res_seqres:\n",
    "                                print(f\"Substituting non-natural amino acid '{res_seqres}' with '{corrected_residue}' in SEQRES for {pdb_id}_{chain_id}.\")\n",
    "                        else:\n",
    "                            corrected_seqres += res_seqres\n",
    "\n",
    "                    if not exclude_due_to_non_natural_diff and max_gap_length <= 4:\n",
    "                        satisfying_structures_count += 1\n",
    "                        info = (f\"Aligned Sequences for {pdb_id}_{chain_id}: (Max gap length: {max_gap_length})\\n\"\n",
    "                                f\"SEQRES Segment: {corrected_seqres}\\n\"\n",
    "                                f\"ATOM Segment:   {aligned_atom}\\n\\n\")\n",
    "                        print(info)\n",
    "                        text_output.write(info)\n",
    "                        \n",
    "                        aligned_sequences[f\"{pdb_id}_{chain_id}_SEQRES\"] = corrected_seqres\n",
    "                        aligned_sequences[f\"{pdb_id}_{chain_id}_ATOM\"] = aligned_atom\n",
    "\n",
    "                        # Write full SEQRES and ATOM sequences to the FASTA file\n",
    "                        fasta_output.write(f\">{pdb_id}_{chain_id}_SEQRES\\n{seqres_sequence}\\n\")\n",
    "                        fasta_output.write(f\">{pdb_id}_{chain_id}_ATOM\\n{atom_seq}\\n\")\n",
    "                    else:\n",
    "                        exclusion_msg = f\"Excluding {pdb_id}_{chain_id} due to gap length: {max_gap_length} or non-natural amino acid difference.\\n\"\n",
    "                        print(exclusion_msg)\n",
    "                        text_output.write(exclusion_msg)\n",
    "                else:\n",
    "                    motif_msg = f\"Motifs not found in {pdb_id}_{chain_id}.\\n\"\n",
    "                    print(motif_msg)\n",
    "                    text_output.write(motif_msg)\n",
    "            else:\n",
    "                chain_msg = f\"Chain {chain_id} not found in SEQRES of {pdb_id} or no atomic sequence available.\\n\"\n",
    "                print(chain_msg)\n",
    "                text_output.write(chain_msg)\n",
    "                \n",
    "        count_msg = f\"Total structures satisfying the condition: {satisfying_structures_count}\"\n",
    "        text_output.write(count_msg)\n",
    "        print(count_msg)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COunting how many seqences are in the fasta file\n",
    "def count_total_pdb_ids(file_path):\n",
    "    total_pdb_ids = 0\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                total_pdb_ids += 1\n",
    "\n",
    "    print(f\"Total number of PDB IDs: {int(total_pdb_ids/2)}\") #here we divide by 2 because we have two lines per PDB ID\n",
    "\n",
    "# Provide the path to your seqres_sequence.fasta file\n",
    "file_path = \"seqres_sequences.fasta\"\n",
    "count_total_pdb_ids(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to use MODELLER to reconstruct the sequences that have a gap length of 4 or less, if there are no differences between the SEQRES and ATOM sequences, the original PDB file is copied to the target directory\n",
    "# FIX was to eliminate remarks from the pdb file\n",
    "import os\n",
    "import shutil\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    \"\"\"Read sequences from a FASTA file into a dictionary.\"\"\"\n",
    "    sequences = {}\n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        header = None\n",
    "        sequence = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header:\n",
    "                    sequences[header] = ''.join(sequence)\n",
    "                header = line[1:]\n",
    "                sequence = []\n",
    "            else:\n",
    "                sequence.append(line)\n",
    "        if header:\n",
    "            sequences[header] = ''.join(sequence)\n",
    "    return sequences\n",
    "\n",
    "def extract_atom_sequence(pdb_file):\n",
    "    \"\"\"Extract sequence from atomic coordinates for the first chain found in the PDB file.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            ppb = PPBuilder()\n",
    "            sequence = ''\n",
    "            for pp in ppb.build_peptides(chain):\n",
    "                sequence += pp.get_sequence()\n",
    "            return str(sequence)\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    index = sequence.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "def reconstruct_with_modeller(pdb_chain_id, pdb_path, target_path, full_sequence, atom_sequence):\n",
    "    print(f\"Processing {pdb_chain_id}\")\n",
    "\n",
    "    # Find indices of the DFG and APE motifs\n",
    "    seqres_dfg_index = find_motif_indices(full_sequence, 'DFG')\n",
    "    seqres_ape_index = find_motif_indices(full_sequence, 'APE')\n",
    "    atom_dfg_index = find_motif_indices(atom_sequence, 'DFG')\n",
    "    atom_ape_index = find_motif_indices(atom_sequence, 'APE')\n",
    "\n",
    "    # Determine if reconstruction is needed\n",
    "    if None not in [seqres_dfg_index, seqres_ape_index, atom_dfg_index, atom_ape_index]:\n",
    "        seqres_start = min(seqres_dfg_index, seqres_ape_index)\n",
    "        seqres_end = max(seqres_dfg_index + 3, seqres_ape_index + 3)\n",
    "        atom_start = min(atom_dfg_index, atom_ape_index)\n",
    "        atom_end = max(atom_dfg_index + 3, atom_ape_index + 3)\n",
    "\n",
    "        seqres_segment = full_sequence[seqres_start:seqres_end]\n",
    "        atom_segment = atom_sequence[atom_start:atom_end]\n",
    "\n",
    "        # Check for differences in the segment\n",
    "        if seqres_segment != atom_segment:\n",
    "            print(f\"Reconstructing full sequence for {pdb_chain_id} using MODELLER\")\n",
    "\n",
    "            # Setting up MODELLER\n",
    "            env = environ()\n",
    "            aln = alignment(env)\n",
    "            \n",
    "            # Read the structure to work on\n",
    "            mdl = model(env, file=pdb_path)\n",
    "            aln.append_model(mdl, align_codes='template', atom_files=pdb_path)\n",
    "\n",
    "            # Append the full target sequence\n",
    "            aln.append_sequence(full_sequence)\n",
    "            aln[-1].code = 'target'\n",
    "            \n",
    "            # Perform the alignment\n",
    "            aln.align2d(max_gap_length=50)\n",
    "\n",
    "            # Create AutoModel object and build models\n",
    "            a = automodel(env, alnfile=aln, knowns='template', sequence='target')\n",
    "            a.starting_model = 1\n",
    "            a.ending_model = 1\n",
    "            \n",
    "            # Build the model\n",
    "            a.make()\n",
    "            \n",
    "            # Save the best model to the target directory\n",
    "            model_path = os.path.join(target_path, f\"{pdb_chain_id}_filled.pdb\")\n",
    "            os.rename(a.outputs[0]['name'], model_path)\n",
    "            print(f\"Reconstruction completed for {pdb_chain_id}. File saved at {model_path}\")\n",
    "\n",
    "            # Post-process to remove REMARK lines\n",
    "            remove_remark_lines(model_path)\n",
    "        else:\n",
    "            # No reconstruction needed, copy original PDB\n",
    "            shutil.copy(pdb_path, os.path.join(target_path, f\"{pdb_chain_id}.pdb\"))\n",
    "            print(f\"No differences found for {pdb_chain_id}. Original PDB copied to target directory.\")\n",
    "    else:\n",
    "        print(f\"Motifs not found in {pdb_chain_id}.\")\n",
    "\n",
    "def remove_remark_lines(pdb_file):\n",
    "    \"\"\"Remove lines starting with 'REMARK' from the PDB file.\"\"\"\n",
    "    with open(pdb_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    with open(pdb_file, 'w') as file:\n",
    "        for line in lines:\n",
    "            if not line.startswith(\"REMARK\"):\n",
    "                file.write(line)\n",
    "    \n",
    "    print(f\"Removed REMARK lines from {pdb_file}\")\n",
    "\n",
    "def main():\n",
    "    seqres_fasta = \"seqres_sequences.fasta\"\n",
    "    pdb_dir = \"Results/activation_segments/unaligned\"\n",
    "    target_dir = \"Results/activation_segments/reconstructedModeller\"\n",
    "\n",
    "    # Read the sequences from the FASTA file\n",
    "    seqres_sequences = read_fasta_sequences(seqres_fasta)\n",
    "\n",
    "    for header, full_sequence in seqres_sequences.items():\n",
    "        if \"_SEQRES\" in header:  # Only consider SEQRES entries\n",
    "            pdb_chain_id = header.replace(\"_SEQRES\", \"\")\n",
    "            pdb_file_path = os.path.join(pdb_dir, f\"{pdb_chain_id}.pdb\")\n",
    "\n",
    "            # Extract the atomic sequence\n",
    "            atom_sequence = extract_atom_sequence(pdb_file_path)\n",
    "\n",
    "            if atom_sequence is None:\n",
    "                print(f\"Could not extract sequence for {pdb_chain_id}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(target_dir):\n",
    "                os.makedirs(target_dir)\n",
    "            \n",
    "            reconstruct_with_modeller(pdb_chain_id, pdb_file_path, target_dir, full_sequence, atom_sequence)\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of pdb files in the directory after reconstruction\n",
    "import glob\n",
    "import os\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'Results/activation_segments/reconstructedModeller'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119237e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast checking if reconstruction was successful, need to just change the pdb id and chain id\n",
    "import MDAnalysis as mda\n",
    "import nglview as nv\n",
    "from Bio.PDB import PDBParser\n",
    "from MDAnalysis.analysis import align\n",
    "\n",
    "# Dictionary to convert three-letter amino acid codes to one-letter codes\n",
    "three_to_one = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D',\n",
    "    'CYS': 'C', 'GLU': 'E', 'GLN': 'Q', 'GLY': 'G',\n",
    "    'HIS': 'H', 'ILE': 'I', 'LEU': 'L', 'LYS': 'K',\n",
    "    'MET': 'M', 'PHE': 'F', 'PRO': 'P', 'SER': 'S',\n",
    "    'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "}\n",
    "\n",
    "def extract_sequence_and_mapping(pdb_file):\n",
    "    \"\"\"Extract sequence and create a mapping from sequence index to PDB residue ID.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    sequence = []\n",
    "    index_to_resid = {}\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if 'CA' in residue:  # Check if it's an amino acid\n",
    "                    resname = residue.get_resname()\n",
    "                    if resname in three_to_one:\n",
    "                        sequence.append(three_to_one[resname])\n",
    "                        index_to_resid[len(sequence) - 1] = residue.get_id()[1]  # Map sequence index to PDB resid\n",
    "            break\n",
    "        break\n",
    "\n",
    "    return sequence, index_to_resid\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    sequence_str = ''.join(sequence)\n",
    "    index = sequence_str.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "# Extract the sequence and mapping for the single chain\n",
    "atom_sequence, index_to_resid = extract_sequence_and_mapping(\"Results/activation_segments/unaligned/7OPO_A.pdb\")\n",
    "\n",
    "# Find indices of DFG and APE motifs\n",
    "dfg_index = find_motif_indices(atom_sequence, 'DFG')\n",
    "ape_index = find_motif_indices(atom_sequence, 'APE')\n",
    "\n",
    "# Ensure indices are found and select the residues between them\n",
    "if dfg_index is not None and ape_index is not None:\n",
    "    # Use the mapping to get the correct residue IDs\n",
    "    dfg_resid = index_to_resid[dfg_index]\n",
    "    ape_resid = index_to_resid[ape_index + 2]  # +2 to include the entire 'APE' motif\n",
    "\n",
    "    u_missing = mda.Universe(\"Results/activation_segments/unaligned/7OPO_A.pdb\")\n",
    "    selected_atoms = u_missing.select_atoms(f\"resid {dfg_resid}:{ape_resid}\")\n",
    "\n",
    "    print(\"Number of Atoms Selected:\", selected_atoms.n_atoms)\n",
    "\n",
    "    u_reconstructed = mda.Universe(\"Results/activation_segments/reconstructedModeller/7OPO_A_filled.pdb\")\n",
    "    print(\"Number of Atoms Reconstructed:\", u_reconstructed.select_atoms(f\"all\").n_atoms)\n",
    "\n",
    "    # Merge the aligned atoms for visualization\n",
    "    merged = mda.Merge(selected_atoms, u_reconstructed.atoms)\n",
    "    print(merged.residues)\n",
    "\n",
    "    # Create NGLView widget\n",
    "    w = nv.show_mdanalysis(merged)\n",
    "\n",
    "    # Add a representation for each residue name with the corresponding color\n",
    "    w.clear()\n",
    "    w.add_cartoon(color=\"resname\")\n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"Motifs not found in the sequence.\")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions and function to run MUSTANG on the reconstructed pdb files\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from glob import glob as g\n",
    "import mdtraj as md\n",
    "from mpi4py import MPI\n",
    "from time import time as t\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sg(f_p):\n",
    "    return sorted(g(f_p))\n",
    "\n",
    "\n",
    "def find_pdbs(directory):\n",
    "    \"\"\"\n",
    "    Find topologies in a directory.\n",
    "    Currently excludes cif files.\n",
    "    \"\"\"\n",
    "    return sg(directory+\"/*.pdb\")\n",
    "\n",
    "\n",
    "def fname(file):\n",
    "    return file.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1]\n",
    "\n",
    "\n",
    "def ifnotmake(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "\n",
    "\n",
    "def run_mustang(f1, f2, name=None):\n",
    "    \"\"\"\n",
    "    Writes a MUSTANG input file which aligns\n",
    "    file1 to file 2.\n",
    "    If no name defaults to the second file.\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = fname(f2)\n",
    "    if not os.path.isdir(f\"./{name}\"):\n",
    "        os.makedirs(f\"./{name}\")\n",
    "    new_fp = f\"./{name}/{name}\"\n",
    "    structs = f\"{f1} {f2} \"\n",
    "    command = f\"/home/marmatt/Downloads/MUSTANG_v3.2.4/bin/mustang-3.2.4 -i {structs} -o {new_fp} -F fasta -s ON\" #here you should change the path to where you install MUSTANG\n",
    "    command = command.split()\n",
    "    new_fp = f\"{new_fp}.pdb\"\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        print(f\"Running command: {' '.join(command)}\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error in MUSTANG execution\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "    return new_fp\n",
    "\n",
    "\n",
    "def postprocess(file_path):\n",
    "    \"\"\"\n",
    "    file_path is the name of a pdb file.\n",
    "    It deletes the first chain which is always the alignment structures\n",
    "    \"\"\"\n",
    "    structure = md.load(file_path)\n",
    "    aligned_chain_idx = [[atom.index for atom in res.atoms] for res in\n",
    "                         structure.top._chains[1]._residues]\n",
    "    aligned_chain_idx = sum(aligned_chain_idx, [])\n",
    "    structure = structure.atom_slice(aligned_chain_idx)\n",
    "    structure.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993598b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform MUSTANG alignment on the sequences that have not been reconstructed, I need to fix this bug\n",
    "\n",
    "#from mustang import *\n",
    "import subprocess\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Constants (most to be made variable)\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "# Define the paths explicitly\n",
    "pdb_path = \"Results/activation_segments/reconstructedModeller\"\n",
    "target_dir = \"Results/activation_segments/mustangs\"\n",
    "template_pdb = \"6UAN_chainD.pdb\"\n",
    "# Ensure the target directory exists\n",
    "ifnotmake(target_dir)\n",
    "pdb_path = os.path.abspath(pdb_path)\n",
    "target_dir = os.path.abspath(target_dir)\n",
    "template_pdb = os.path.abspath(template_pdb)\n",
    "print(pdb_path, target_dir, template_pdb)\n",
    "os.chdir(target_dir)\n",
    "os.system(\"pwd\")\n",
    "if rank == 0:\n",
    "    pdbs = find_pdbs(pdb_path)\n",
    "    n_files = len(pdbs)\n",
    "    n_slices = (n_files // size)\n",
    "    step = int(n_files / n_slices)\n",
    "    if n_files % n_slices != 0:\n",
    "        n_slices += 1\n",
    "    slices = [slice(i*n_slices, (i+1)*n_slices) for i in range(step)]\n",
    "    pdbs = [pdbs[s] for s in slices]\n",
    "else:\n",
    "    pdbs = None\n",
    "\n",
    "pdbs = comm.scatter(pdbs, root=0)\n",
    "print(\"RANK:\\t\", rank, \"DATA SIZE:\\t\", len(pdbs))\n",
    "t1 = t()\n",
    "failures = []\n",
    "for pdb in tqdm(pdbs):\n",
    "    name = fname(pdb)\n",
    "    new_fp = run_mustang(template_pdb, pdb, name=name)\n",
    "    if new_fp:\n",
    "        if os.path.isfile(new_fp):\n",
    "            postprocess(new_fp)\n",
    "        else:\n",
    "            failures.append(pdb)\n",
    "    else:\n",
    "        failures.append(pdb)\n",
    "t2 = t()\n",
    "print(\"FINISHED RANK:\\t\", rank, \"DATA SIZE:\\t\", len(pdbs),\n",
    "      \"TIME:\\t\", round(t2-t1, 4))\n",
    "failures = comm.gather(failures, root=0)\n",
    "if rank == 0:\n",
    "    failures = sum(failures, [])\n",
    "    with open(\"./failures.txt\", \"w\") as f_o:\n",
    "        f_o.write(\"\\n\".join(f for f in failures))\n",
    "    t2 = t()\n",
    "    print(round(t2-t1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14388f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of directories representing the number of pdb files that have been aligned\n",
    "import os\n",
    "def count_directories(directory):\n",
    "    # List all entries in the given directory\n",
    "    entries = os.listdir(directory)\n",
    "\n",
    "    # Use os.path.join to get the full path and os.path.isdir to check if it's a directory\n",
    "    directories = [entry for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "\n",
    "    # Return the count of directories\n",
    "    return len(directories)\n",
    "\n",
    "# Specify the directory\n",
    "directory_path = 'Results/activation_segments/mustangs'\n",
    "\n",
    "# Get the count of directories\n",
    "directory_count = count_directories(directory_path)\n",
    "\n",
    "print(f\"There are {directory_count} directories in the directory '{directory_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of files representing the number of pdb files that did not need reconstruction\n",
    "import os\n",
    "from glob import glob as g\n",
    "\n",
    "def count_non_filled_pdbs(directory):\n",
    "    # Find all PDB files in the directory\n",
    "    pdb_files = g(os.path.join(directory, \"*.pdb\"))\n",
    "\n",
    "    # Filter out files with '_filled' in their names\n",
    "    non_filled_pdbs = [pdb for pdb in pdb_files if \"_filled\" not in os.path.basename(pdb)]\n",
    "\n",
    "    # Return the count of non '_filled' PDB files\n",
    "    return len(non_filled_pdbs)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory_path = 'Results/activation_segments/reconstructedModeller'\n",
    "\n",
    "# Get the count of non '_filled' PDB files\n",
    "non_filled_pdb_count = count_non_filled_pdbs(pdb_directory_path)\n",
    "\n",
    "print(f\"There are {non_filled_pdb_count} PDB files without '_filled' in the directory '{pdb_directory_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pprint import pprint as pp\n",
    "alignment_dir = \"Results/activation_segments/mustangs\"\n",
    "\n",
    "class alignment:\n",
    "    \"\"\"\n",
    "    Class to hold alignments.\n",
    "    Currently only supports braf_monomers!\n",
    "    \"\"\"\n",
    "    def __init__(self,name,seq1,seq2):\n",
    "        self.name = name\n",
    "        self.seq1 = seq1\n",
    "        self.seq2 = seq2\n",
    "        self.aligned = self.find_aligned()\n",
    "\n",
    "    def find_pdb(self):\n",
    "        pdb_files = []\n",
    "        for root, dirs, files in os.walk(alignment_dir):\n",
    "            pdb_files += [os.path.join(root, file) for file in files if file.endswith('.pdb')]\n",
    "        \n",
    "        pdb = [f for f in pdb_files if self.name in f]\n",
    "        print(pdb)\n",
    "        if len(pdb) == 1:\n",
    "            return pdb[0]\n",
    "        \n",
    "\n",
    "    def find_aligned(self):\n",
    "        aligned = []\n",
    "        for char1, char2 in zip(self.seq1,self.seq2):\n",
    "            if char1 != \"-\":\n",
    "                aligned.append((char1,char2))\n",
    "        return aligned\n",
    "\n",
    "    def aligned_res(self):\n",
    "        seq1, seq2 = self.seq1, self.seq2\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        seq_length = len(aligned)\n",
    "        pdb2_top = self.load_pdb()\n",
    "        full_seq2 = \"\".join(char for char in seq2 if char != \"-\")\n",
    "        residues = pdb2_top.top._residues\n",
    "        n_res = len(residues)\n",
    "        res_counter = 0\n",
    "        for i in range(seq_length):\n",
    "            if res_counter >= n_res:\n",
    "                break\n",
    "            if aligned[i][1] != \"-\":\n",
    "                aligned[i][1] = residues[res_counter]\n",
    "                res_counter += 1\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def aligned_xyz(self):\n",
    "        \"\"\"\n",
    "        Return xyz of aligned residues\n",
    "        \"\"\"\n",
    "        xyz = self.load_pdb()._xyz[0] # Only one frame\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        for k,(_,res) in enumerate(self.residues):\n",
    "            if not isinstance(res,str):\n",
    "                idxs = []\n",
    "                for atom in res._atoms:\n",
    "                    idxs.append(atom.index)\n",
    "                res_xyz = xyz[idxs]\n",
    "                aligned[k][1] = res_xyz\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def aligned_ca_xyz(self):\n",
    "        \"\"\"\n",
    "        Return xyz of aligned residues\n",
    "        \"\"\"\n",
    "        xyz = self.load_pdb()._xyz[0] # Only one frame\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        for k,(_,res) in enumerate(self.residues):\n",
    "            if not isinstance(res,str):\n",
    "                for atom in res._atoms:\n",
    "                    if atom.name == \"CA\":\n",
    "                        idxs = atom.index\n",
    "                        break\n",
    "                try:\n",
    "                    res_xyz = xyz[idxs]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\"ERROR FOR:\")\n",
    "                    print(self.name)\n",
    "                    return None\n",
    "                aligned[k][1] = res_xyz\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def load_pdb(self):\n",
    "        return md.load(self.pdb_file)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.seq1[idx],self.seq2[idx])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "    def find_match_id(self):\n",
    "        seq1, seq2 = self.seq1, self.seq2\n",
    "        full_seq2 = \"\".join(char for char in seq2 if char != \"-\")\n",
    "        pp(full_seq2)\n",
    "        aligned = self.aligned\n",
    "        actv_low = 155\n",
    "        actv_hgh = 181\n",
    "        match_residues = aligned[actv_low:actv_hgh] # These are what we need\n",
    "        seq2_Seq = [a[1] for a in match_residues if a[1] != \"-\"]\n",
    "        begin_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19861f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afasta_parse(file):\n",
    "    \"\"\"\n",
    "    Parse mustang afasta format output file.\n",
    "    Returns two lists of equal length\n",
    "    \"\"\"\n",
    "    with open(file,\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    names = [l.split(\".\")[0][1:] for l in lines if l[0] == \">\"]\n",
    "    for i in range(1,len(lines)):\n",
    "        if lines[i].isspace():\n",
    "            lines[i] = \"BREAK\"\n",
    "            break\n",
    "        elif lines[i][0] == \">\":\n",
    "            lines[i] = \"BREAK\" + lines[i]\n",
    "            break\n",
    "    lines = [l.strip() for l in lines if l[0] != \">\"]\n",
    "    lines = \"\".join(lines)\n",
    "    fastas = lines.split(\"BREAK\")\n",
    "    fastas = [*filter(None,fastas)]\n",
    "    return fastas[0], fastas[1]\n",
    "\n",
    "def load_alignments(kind=\"mustang\"):\n",
    "    if kind==\"mustang\":\n",
    "        #ppath = \"/home/marmatt/Documents/projects/BRAF/myWork/reproduceBRAFWork/Results/activation_segments/mustangs/mustang_alignments.fasta\"\n",
    "        ppath = \"Results/activation_segments/mustangs/mustang_alignments.fasta\"\n",
    "        \n",
    "        #print(\"Loading pickled alignments...\")\n",
    "    elif kind==\"blast\":\n",
    "        ppath = \"blast_alignments.fasta\"\n",
    "    if os.path.isfile(ppath):\n",
    "        with open(ppath,\"rb\") as pickled:\n",
    "            #print(\"Loading pickled alignments...\")\n",
    "            return p.load(pickled)\n",
    "    else:\n",
    "        make_align_pickle()\n",
    "        #print(\"No pickled alignments found. Creating...\")\n",
    "        return load_alignments()\n",
    "    \n",
    "def make_align_pickle(kind=\"mustang\"):\n",
    "    if kind == \"mustang\":\n",
    "\n",
    "        alignments = []\n",
    "\n",
    "        # Iterate over directories in the alignment directory\n",
    "        for directory_name in os.listdir(alignment_dir):\n",
    "            directory_path = os.path.join(alignment_dir, directory_name)\n",
    "\n",
    "            # Ensure we are working with directories\n",
    "            if os.path.isdir(directory_path):\n",
    "                #print(f\"Processing directory: {directory_name}\")\n",
    "                fasta_files = tqdm(glob(os.path.join(directory_path, \"*.afasta\")), desc=f\"Processing {directory_name} .afasta files\")\n",
    "                \n",
    "                for fasta_file in fasta_files:\n",
    "                    name = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "                    fasta_files.set_description(f\"Working on {name}\")\n",
    "\n",
    "                    # Simulate the alignment logic\n",
    "                    aligned = alignment(name, *afasta_parse(fasta_file))  # Assuming `alignment` and `afasta_parse` are predefined\n",
    "                    alignments.append(aligned)\n",
    "\n",
    "        # Define a path for the output pickle file\n",
    "        ppath = os.path.join(alignment_dir, \"mustang_alignments.fasta\")\n",
    "        with open(ppath, \"wb\") as pickled:\n",
    "            p.dump(alignments, pickled)\n",
    "    \n",
    "    elif kind == \"blast\":\n",
    "        b = BLAST_results()\n",
    "        alignments = []\n",
    "        for k, dicti in tqdm(b.alignments.items(),total=len(b.alignments)):\n",
    "            seq1 = dicti[\"Query\"]\n",
    "            seq2 = dicti[\"Subject\"]\n",
    "            alignments.append(alignment(k,seq1,seq2))\n",
    "        ppath = \"blast_alignments.fasta\"\n",
    "        with open(ppath, \"wb\") as pickled:\n",
    "            p.dump(alignments, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a51087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from glob import glob as g\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams.update({'font.size': 8})\n",
    "kind = \"mustang\"\n",
    "make_align_pickle(kind) #create MSA file --> important, substituted .p with .html in alignment class\n",
    "aligned = load_alignments(kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a71ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some helper functions to extract the sequence from the pdb files and to extract the sequence from the alignment files\n",
    "def braf_res():\n",
    "    fp = \"./6UAN_chainD.pdb\"\n",
    "    top = md.load(fp).top\n",
    "    return [res_namer(res) for res in top.residues]\n",
    "\n",
    "\n",
    "def res_namer(res):\n",
    "    return f\"{res.name}-{res.resSeq}\"\n",
    "\n",
    "def fname(fp):\n",
    "    return fp.rsplit(\".\",1)[0].rsplit(\"/\",1)[-1]\n",
    "\n",
    "def make_seg(a):\n",
    "    seq = [t for t in a.aligned if t[0] != \"-\"]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting the number of aligned residues over the BRAF BLAST search results to show what are the most conserved residues throughout the alignment\n",
    "\"\"\"\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "print(aligned)\n",
    "seq1mag = len(aligned[0].seq1.replace(\"-\",\"\"))\n",
    "counts = np.zeros(seq1mag)\n",
    "for a in aligned:\n",
    "    segment = make_seg(a)\n",
    "    for i,(b,c) in enumerate(segment):\n",
    "        if c != \"-\":\n",
    "            counts[i] += 1\n",
    "counts =  counts / max(counts)\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "fig,ax = plt.subplots(1,figsize=(10,5))\n",
    "x = [*range(len(counts))]\n",
    "ax.set_xticks(x[::5])\n",
    "x_lbl = braf_res()\n",
    "ax.set_xticklabels(x_lbl[::5],rotation=90,fontsize=7)\n",
    "# subtract 11\n",
    "ax.axvspan(36,48, facecolor='g', alpha=0.5)\n",
    "ax.axvspan(92,100, facecolor='c', alpha=0.5)\n",
    "ax.axvspan(144,168, facecolor='r', alpha=0.5)\n",
    "ax.axvspan(177,186, facecolor='y', alpha=0.5)\n",
    "ax.axvspan(204,215, facecolor='pink', alpha=0.8)\n",
    "ax.axvspan(222,240, facecolor='dodgerblue', alpha=0.8)\n",
    "ax.bar(x,counts,linewidth=0.05,width=1)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n",
    "title = ax.set_title(\"Number of MUSTANG aligned residues over the BRAF BLAST search results\")\n",
    "ax1 = plt.xlabel(\"Resiude Name-Number\")\n",
    "ax1 = plt.ylabel(\"Percent matching in structural alignments\")\n",
    "ax.tick_params(length=2,color=\"black\",direction=\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell finds PDBs with DFG and APE motifs aligned.\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Initialize counters and lists\n",
    "count = 0\n",
    "new_aligned = []\n",
    "bad = []\n",
    "\n",
    "# First filtering loop\n",
    "for k, a in enumerate(aligned):\n",
    "    if a.seq1.find(\"DFG\") == -1 or a.seq1.find(\"APE\") == -1:\n",
    "        bad.append(a)\n",
    "        count += 1\n",
    "    else:\n",
    "        new_aligned.append(a)\n",
    "\n",
    "print(count, \"/\", len(aligned), \" don't match.\")\n",
    "print(f\"Continuing with {len(new_aligned)} structures\")\n",
    "\n",
    "# Second filtering and analysis loop\n",
    "counts = {}\n",
    "counter = 0\n",
    "new_aligned2 = []\n",
    "aligning_segs = {}\n",
    "lengths = []\n",
    "\n",
    "for a in new_aligned:\n",
    "    DFG_index = a.seq1.find(\"DFG\")\n",
    "    APE_index = a.seq1.find(\"APE\")\n",
    "    dfg = a.seq2[DFG_index]\n",
    "    ape = a.seq2[APE_index]\n",
    "    \n",
    "    if dfg != \"-\" and ape != \"-\":\n",
    "        counter += 1\n",
    "        length = APE_index - DFG_index\n",
    "        counts.setdefault(length, []).append((a.name, DFG_index, APE_index))\n",
    "        new_aligned2.append(a)\n",
    "        aligning_segs.setdefault(a.name,\n",
    "                                 (a.seq2[DFG_index:DFG_index+3],\n",
    "                                  a.seq2[APE_index:APE_index+3],\n",
    "                                  a.seq2[DFG_index:APE_index+3]))\n",
    "        lengths.append(len(a.seq2[DFG_index:APE_index+3]))\n",
    "\n",
    "print(f\"{counter} structures with an alignment to the D and A\")\n",
    "\n",
    "# Write to CSV\n",
    "with open('alignment_results.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Seq1 Segment', 'Seq2 Segment', 'Status'])\n",
    "\n",
    "    # Write new_aligned2\n",
    "    for a in new_aligned2:\n",
    "        DFG_index = a.seq1.find(\"DFG\")\n",
    "        APE_index = a.seq1.find(\"APE\")\n",
    "        seq1_segment = a.seq1[DFG_index:APE_index+3]\n",
    "        seq2_segment = a.seq2[DFG_index:APE_index+3]\n",
    "        writer.writerow([a.name, seq1_segment, seq2_segment, 'Aligned'])\n",
    "\n",
    "    # Write bad\n",
    "    for a in bad:\n",
    "        writer.writerow([a.name, '', '', 'Not Aligned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymol import cmd\n",
    "from glob import glob\n",
    "import os\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "\n",
    "# Paths\n",
    "pdb_dir = \"Results/activation_segments/mustangs/\"\n",
    "reference_pdb = \"6UAN_chainD.pdb\"\n",
    "output_dir = \"Results/activation_segments/mustangs_realigned/\"\n",
    "image_output_path = \"Results/aligned_loops.png\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the reference structure\n",
    "cmd.load(reference_pdb, \"6UAN_chainD\")\n",
    "\n",
    "# Create selections for the reference structure\n",
    "cmd.select(\"6UAN_chainD_dfg_selection\", \"6UAN_chainD and resi 594-596 and name CA\")\n",
    "cmd.select(\"6UAN_chainD_ape_selection\", \"6UAN_chainD and resi 621-623 and name CA\")\n",
    "cmd.select(\"6UAN_chainD_ends_selection\", \"6UAN_chainD and (resi 594-596 or resi 621-623) and name CA\")\n",
    "\n",
    "def extract_atom_sequence(pdb_file):\n",
    "    \"\"\"Extract sequence from atomic coordinates for the first chain found in the PDB file.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            ppb = PPBuilder()\n",
    "            sequence = ''\n",
    "            for pp in ppb.build_peptides(chain):\n",
    "                sequence += pp.get_sequence()\n",
    "            return str(sequence)\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(seq, motif):\n",
    "    index = seq.find(motif)\n",
    "    if index == -1:\n",
    "        return None\n",
    "    return index, index + len(motif)\n",
    "\n",
    "def print_residues_in_selection(selection_name):\n",
    "    model = cmd.get_model(selection_name)\n",
    "    residues = set((atom.resi, atom.resn) for atom in model.atom)\n",
    "    print(f\"Residues in {selection_name}: {sorted(residues)}\")\n",
    "\n",
    "def process_structure(pdb_file, ref_name=\"6UAN_chainD\"):\n",
    "    pdb_code = os.path.basename(pdb_file).split('.')[0]\n",
    "    cmd.load(pdb_file, pdb_code)\n",
    "\n",
    "    # Extract sequence from atomic coordinates\n",
    "    seq = extract_atom_sequence(pdb_file)\n",
    "    if seq is None:\n",
    "        print(f\"Skipping {pdb_code} due to inability to extract sequence.\")\n",
    "        return\n",
    "    print(f\"Sequence for {pdb_code}: {seq}\")\n",
    "\n",
    "    # Find indices for DFG and APE\n",
    "    dfg_indices = find_motif_indices(seq, \"DFG\")\n",
    "    ape_indices = find_motif_indices(seq, \"APE\")\n",
    "\n",
    "    if not dfg_indices or not ape_indices:\n",
    "        print(f\"Skipping {pdb_code} due to missing motifs.\")\n",
    "        return\n",
    "\n",
    "    # Create selections using sequence indices\n",
    "    dfg_residues = list(range(dfg_indices[0] + 1, dfg_indices[1] + 1))\n",
    "    ape_residues = list(range(ape_indices[0] + 1, ape_indices[1] + 1))\n",
    "\n",
    "    # Create selections\n",
    "    cmd.select(f\"{pdb_code}_dfg_selection\", f\"{pdb_code} and resi {dfg_residues[0]}-{dfg_residues[-1]} and name CA\")\n",
    "    cmd.select(f\"{pdb_code}_ape_selection\", f\"{pdb_code} and resi {ape_residues[0]}-{ape_residues[-1]} and name CA\")\n",
    "    cmd.select(f\"{pdb_code}_ends_selection\", f\"{pdb_code} and (resi {dfg_residues[0]}-{dfg_residues[-1]} or resi {ape_residues[0]}-{ape_residues[-1]}) and name CA\")\n",
    "\n",
    "    # Print residues in selections\n",
    "    print_residues_in_selection(f\"{pdb_code}_dfg_selection\")\n",
    "    print_residues_in_selection(f\"{pdb_code}_ape_selection\")\n",
    "    print_residues_in_selection(f\"{pdb_code}_ends_selection\")\n",
    "\n",
    "    # RMSD before alignment\n",
    "    rms_dfg_before = cmd.rms_cur(f\"{pdb_code}_dfg_selection\", f\"{ref_name}_dfg_selection\", matchmaker=-1)\n",
    "    rms_ape_before = cmd.rms_cur(f\"{pdb_code}_ape_selection\", f\"{ref_name}_ape_selection\", matchmaker=-1)\n",
    "    rms_ends_before = cmd.rms_cur(f\"{pdb_code}_ends_selection\", f\"{ref_name}_ends_selection\", matchmaker=-1)\n",
    "\n",
    "    print(f\"Before alignment RMSD for {pdb_code}: DFG={rms_dfg_before}, APE={rms_ape_before}, ENDS={rms_ends_before}\")\n",
    "\n",
    "    # Aligning\n",
    "    cmd.align(f\"{pdb_code}_ends_selection\", f\"{ref_name}_ends_selection\", cycles=0, transform=1)\n",
    "\n",
    "    # Save aligned structure\n",
    "    aligned_pdb_path = os.path.join(output_dir, f\"{pdb_code}_aligned.pdb\")\n",
    "    cmd.save(aligned_pdb_path, pdb_code)\n",
    "    print(f\"Saved aligned structure to {aligned_pdb_path}\")\n",
    "\n",
    "    # RMSD after alignment\n",
    "    rms_dfg_after = cmd.rms_cur(f\"{pdb_code}_dfg_selection\", f\"{ref_name}_dfg_selection\", matchmaker=-1)\n",
    "    rms_ape_after = cmd.rms_cur(f\"{pdb_code}_ape_selection\", f\"{ref_name}_ape_selection\", matchmaker=-1)\n",
    "    rms_ends_after = cmd.rms_cur(f\"{pdb_code}_ends_selection\", f\"{ref_name}_ends_selection\", matchmaker=-1)\n",
    "\n",
    "    print(f\"After alignment RMSD for {pdb_code}: DFG={rms_dfg_after}, APE={rms_ape_after}, ENDS={rms_ends_after}\")\n",
    "\n",
    "# Load all PDB files and process them\n",
    "fps = glob(pdb_dir + \"/*/*.pdb\")\n",
    "for fp in fps:\n",
    "    process_structure(fp)\n",
    "\n",
    "# Visualize and color\n",
    "cmd.select(\"all_loops\", \"byres 6UAN_chainD_ends_selection\")\n",
    "cmd.color(\"red\", \"6UAN_chainD and 6UAN_chainD_ends_selection\")\n",
    "cmd.color(\"white\", \"not 6UAN_chainD and all_loops\")\n",
    "\n",
    "# Set visualization style\n",
    "cmd.show(\"cartoon\")\n",
    "cmd.hide(\"lines\")\n",
    "\n",
    "# Save the image\n",
    "cmd.png(image_output_path, width=1200, height=800, dpi=300, ray=1)\n",
    "print(f\"Image saved to {image_output_path}\")\n",
    "\n",
    "cmd.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import biobox as bb\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "################################################################################\n",
    "# Code to save only the first and last CA atoms in activation segment PDB files \n",
    "# both before and after Mustang realignment, for quick verification.\n",
    "################################################################################\n",
    "# --- BEFORE ENDS REALIGNMENT ---\n",
    "files_before = f\"Results/activation_segments/mustangs/*/*.pdb\"\n",
    "\n",
    "start_before = []\n",
    "end_before = []\n",
    "\n",
    "for f in glob.glob(files_before):\n",
    "    M = bb.Molecule(f)\n",
    "    pts = M.atomselect(\"*\", \"*\", \"CA\")\n",
    "    start_before.append(pts[0])\n",
    "    end_before.append(pts[-1])\n",
    "\n",
    "M_start_before = bb.Structure(p=np.array(start_before))\n",
    "M_end_before = bb.Structure(p=np.array(end_before))\n",
    "\n",
    "M_start_before.write_pdb(\"start_CA_beforeReAlign.pdb\")\n",
    "M_end_before.write_pdb(\"end_CA_beforeReAlign.pdb\")\n",
    "\n",
    "# --- AFTER ENDS REALIGNMENT ---\n",
    "files_after = f\"Results/activation_segments/mustangs_realigned/*.pdb\"\n",
    "start_after = []\n",
    "end_after = []\n",
    "\n",
    "for f in glob.glob(files_after):\n",
    "    M = bb.Molecule(f)\n",
    "    pts = M.atomselect(\"*\", \"*\", \"CA\")\n",
    "    start_after.append(pts[0])\n",
    "    end_after.append(pts[-1])\n",
    "\n",
    "M_start_after = bb.Structure(p=np.array(start_after))\n",
    "M_end_after = bb.Structure(p=np.array(end_after))\n",
    "\n",
    "M_start_after.write_pdb(\"start_CA_afterReAlign.pdb\")\n",
    "M_end_after.write_pdb(\"end_CA_afterReAlign.pdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f99703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "# Function to load a PDB file and strip it down to CA atoms from a specified\n",
    "# start_residue to an end_residue (indices are 0-based).\n",
    "###############################################################################\n",
    "def strip_to_ca(pdb_path, start_residue, end_residue):\n",
    "    \"\"\"\n",
    "    Loads the PDB, extracts CA atoms for residues in the specified slice,\n",
    "    and returns an mdtraj.Trajectory object with just those atoms.\n",
    "    \"\"\"\n",
    "    pdb = md.load(pdb_path)\n",
    "    print(f\"Loaded PDB: {pdb_path}\")\n",
    "\n",
    "    # Extract CA atoms within the specified range of residues\n",
    "    atom_indices = [\n",
    "        atom.index \n",
    "        for res in pdb.top._residues[start_residue:end_residue] \n",
    "        for atom in res.atoms \n",
    "        if atom.name == \"CA\"\n",
    "    ]\n",
    "    print(f\"Atom indices for CA: {atom_indices}\")\n",
    "\n",
    "    return pdb.atom_slice(atom_indices)\n",
    "\n",
    "###############################################################################\n",
    "# Function to process alignments by looking for the DFG and APE motifs in seq1,\n",
    "# verifying alignment in seq2, and saving out the PDB stripped to CA atoms only.\n",
    "###############################################################################\n",
    "def process_alignments(pdb_dir, target_dir, alignments):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdb_dir : str\n",
    "        Directory where the PDB files are stored (e.g., mustangs or mustangs_realigned).\n",
    "    target_dir : str\n",
    "        Directory where stripped PDBs will be saved.\n",
    "    alignments : list\n",
    "        A list of alignment objects, each with attributes like:\n",
    "          - name\n",
    "          - seq1 (aligned sequence 1)\n",
    "          - seq2 (aligned sequence 2)\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"\\nProcessing:\\n  PDB directory = {pdb_dir}\\n  Target directory = {target_dir}\")\n",
    "\n",
    "    # Find all PDB files in pdb_dir\n",
    "    pdb_files = glob(os.path.join(pdb_dir, \"*.pdb\"))\n",
    "    print(f\"Found PDB files: {pdb_files}\")\n",
    "\n",
    "    # Process each alignment in alignments\n",
    "    for align_obj in tqdm(alignments, desc=\"Processing alignments\"):\n",
    "        print(f\"\\nChecking alignment: {align_obj.name}\")\n",
    "        match_found = False\n",
    "        matching_fp = \"\"\n",
    "\n",
    "        # Look for a matching PDB file\n",
    "        for fp in pdb_files:\n",
    "            if align_obj.name in fp and \"pdb\" in fp:\n",
    "                match_found = True\n",
    "                matching_fp = fp\n",
    "                print(f\"Match found for {align_obj.name} in file: {fp}\")\n",
    "                break\n",
    "        \n",
    "        if not match_found:\n",
    "            print(f\"No match found for {align_obj.name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Indices of DFG and APE in seq1\n",
    "        DFG_index = align_obj.seq1.find(\"DFG\")\n",
    "        APE_index = align_obj.seq1.find(\"APE\")\n",
    "        print(f\"DFG index (in seq1): {DFG_index} | APE index (in seq1): {APE_index}\")\n",
    "\n",
    "        # Skip if DFG/APE motifs are not found\n",
    "        if DFG_index == -1 or APE_index == -1:\n",
    "            print(f\"Skipping file {matching_fp} as it does not contain the DFG or APE motifs in seq1.\")\n",
    "            continue\n",
    "\n",
    "        # Check if the same segment (DFG) is also aligned in seq2\n",
    "        # (i.e., we want seq2 to also have the substring \"DFG\" in the same place):\n",
    "        if align_obj.seq2[DFG_index:DFG_index + 3] != \"DFG\":\n",
    "            print(f\"Skipping file {matching_fp} as the DFG motif is not aligned properly in seq2.\")\n",
    "            continue\n",
    "\n",
    "        # Adjust DFG and APE indices to account for gaps in seq2\n",
    "        DFG_index_adjusted = DFG_index - sum([1 for a in align_obj.seq2[:DFG_index] if a == \"-\"])\n",
    "        APE_index_adjusted = APE_index - sum([1 for a in align_obj.seq2[:APE_index] if a == \"-\"])\n",
    "        print(f\"Adjusted DFG index: {DFG_index_adjusted} | Adjusted APE index: {APE_index_adjusted}\")\n",
    "\n",
    "        # Strip to CA atoms between the adjusted DFG and APE (including the 3 residues of \"APE\")\n",
    "        stripped = strip_to_ca(matching_fp, DFG_index_adjusted, APE_index_adjusted + 3)\n",
    "\n",
    "        # Construct the file name and save the stripped pdb\n",
    "        new_name = os.path.join(target_dir, os.path.basename(matching_fp))\n",
    "        print(f\"Saving stripped PDB to: {new_name}\")\n",
    "        stripped.save(new_name)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Example usage\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    # 1) Process for the Mustang realigned directory\n",
    "    pdb_dir_realigned = \"Results/activation_segments/mustangs_realigned/\"\n",
    "    target_dir_realigned = f\"Results/activation_segments/CA_segments/'mustangs_endsRealignment'/\"\n",
    "    process_alignments(pdb_dir_realigned, target_dir_realigned, new_aligned2)\n",
    "\n",
    "    # 2) Process for the non-realigned Mustang directory\n",
    "    pdb_dir_no_realign = \"Results/activation_segments/mustangs/*/\"\n",
    "    target_dir_no_realign = \"Results/activation_segments/CA_segments/mustangs_noRealignment\"\n",
    "    process_alignments(pdb_dir_no_realign, target_dir_no_realign, new_aligned2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d358bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import PDB\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import tempfile\n",
    "import mdtraj as md\n",
    "\n",
    "def fitting_code(fp_or_traj, save_path):\n",
    "    # Function to read PDB file or trajectory object and get model\n",
    "    def read_structure(input_data):\n",
    "        if isinstance(input_data, str):\n",
    "            # If input is a string, treat it as a file path\n",
    "            parser = PDB.PDBParser(QUIET=True)\n",
    "            structure = parser.get_structure('structure', input_data)\n",
    "        elif isinstance(input_data, md.Trajectory):\n",
    "            # If input is a trajectory, save to temp PDB and read\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdb\", delete=False) as tmpfile:\n",
    "                input_data.save(tmpfile.name)\n",
    "                tmpfile.close()\n",
    "                parser = PDB.PDBParser(QUIET=True)\n",
    "                structure = parser.get_structure('structure', tmpfile.name)\n",
    "            os.unlink(tmpfile.name)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input type. Provide a file path or md.Trajectory.\")\n",
    "        return structure[0]\n",
    "\n",
    "    # Read the template for CA atoms configuration\n",
    "    template_model = read_structure('template.pdb')\n",
    "    Nnew = len([atom for atom in template_model.get_atoms() if atom.get_id() == 'CA'])\n",
    "\n",
    "    # Read input PDB file or trajectory\n",
    "    my_model = read_structure(fp_or_traj)\n",
    "    atom_list = [atom for atom in my_model.get_atoms() if atom.get_id() == 'CA']\n",
    "\n",
    "    # Coordinates for each CA atom\n",
    "    avg = np.array([atom.coord for atom in atom_list])\n",
    "    n = len(atom_list)\n",
    "\n",
    "    # Fit cubic interpolation for each axis (x, y, z)\n",
    "    dims = ['x', 'y', 'z']\n",
    "    fits = {}\n",
    "    for j, dim in enumerate(dims):\n",
    "        fits[dim] = interp1d(np.arange(n), avg[:, j], kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "    # Create a finer grid of points (X) for interpolation\n",
    "    X = np.arange(0, n - 1, 0.1) \n",
    "    # Gradient in each dimension\n",
    "    dYdX = {dim: np.gradient(fits[dim](X)) for dim in dims}\n",
    "    # Speed along path (magnitude of the gradient)\n",
    "    Y = np.sqrt(sum(np.square(dYdX[dim]) for dim in dims))\n",
    "    # Total arc length (area under the speed curve)\n",
    "    L = np.trapz(Y, X)\n",
    "\n",
    "    # Create an evenly spaced set of arc lengths (Li)\n",
    "    Li = np.linspace(0, L, Nnew)\n",
    "\n",
    "    # Precompute partial arc length at each step in X\n",
    "    flen = np.array([np.trapz(Y[:ibig], X[:ibig]) for ibig in range(1, len(X))])\n",
    "\n",
    "    # For each required point (Nnew), find the corresponding index in X\n",
    "    pt = np.zeros(Nnew, dtype=int)\n",
    "    for i in range(Nnew):\n",
    "        pt[i] = np.argmin(np.abs(flen - Li[i]))\n",
    "\n",
    "    # Interpolated 3D coordinates for each new point\n",
    "    new_coords = np.array([[fits[dim](X[pt[i]]) for dim in dims] for i in range(Nnew)])\n",
    "\n",
    "    # Update the template CA positions with interpolated coordinates\n",
    "    ca_index = 0\n",
    "    for atom in template_model.get_atoms():\n",
    "        if atom.get_id() == 'CA':\n",
    "            atom.set_coord(new_coords[ca_index])\n",
    "            ca_index += 1\n",
    "\n",
    "    # Save the updated structure\n",
    "    try:\n",
    "        with open(save_path, \"w\") as file:\n",
    "            io = PDB.PDBIO()\n",
    "            io.set_structure(template_model)\n",
    "            io.save(file)\n",
    "        print(f'Successfully saved the structure to {save_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file save: {e}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Process two different input/output directory pairs:\n",
    "#   1) mustangs_endsRealignment\n",
    "#   2) mustangs_noRealignment\n",
    "###############################################################################\n",
    "\n",
    "cases = [\n",
    "    (\n",
    "       \"Results/activation_segments/CA_segments/mustangs_endsRealignment/\",\n",
    "       \"Results/fitted_matlab_segments/mustangs_endsRealignment/\"\n",
    "    ),\n",
    "    (\n",
    "       \"Results/activation_segments/CA_segments/mustangs_noRealignment/\",\n",
    "       \"Results/fitted_matlab_segments/mustangs_noRealignment/\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for input_dir, output_dir in cases:\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\nProcessing directory:\\n  Input:  {input_dir}\\n  Output: {output_dir}\")\n",
    "\n",
    "    # Process each PDB file in the input directory\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.pdb'):\n",
    "            input_file_path = os.path.join(input_dir, file_name)\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "            fitting_code(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99814eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymol import cmd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###############################################################################\n",
    "# Function to print details of a selection\n",
    "###############################################################################\n",
    "def print_selection_details(selection_name):\n",
    "    print(f\"Details for selection: {selection_name}\")\n",
    "    cmd.iterate(selection_name, \"print(resi, resn, name)\")\n",
    "    count = cmd.count_atoms(selection_name)\n",
    "    print(f\"Number of atoms in {selection_name}: {count}\")\n",
    "\n",
    "###############################################################################\n",
    "# Function to process a single directory of fitted PDBs\n",
    "###############################################################################\n",
    "def process_directory(fitted_pdb_dir, directory_label):\n",
    "    \"\"\"\n",
    "    Processes all PDB files in 'fitted_pdb_dir', measuring:\n",
    "      - Euclidean distance (single-atom) for first and last CA vs. reference\n",
    "      - RMSD (3 CA) for first three and last three CA vs. reference\n",
    "    \"\"\"\n",
    "    # Re-initialize PyMOL to clear out any previous state\n",
    "    cmd.reinitialize()\n",
    "\n",
    "    # Load reference structure (change path/name if needed)\n",
    "    reference_pdb = \"6UAN_chainD.pdb\"\n",
    "    cmd.load(reference_pdb, \"6UAN_chainD\")\n",
    "\n",
    "    # Create selections for the reference structure\n",
    "    cmd.select(\"6UAN_chainD_first_atom\", \"6UAN_chainD and resi 594 and name CA\")\n",
    "    cmd.select(\"6UAN_chainD_last_atom\", \"6UAN_chainD and resi 623 and name CA\")\n",
    "    print_selection_details(\"6UAN_chainD_first_atom\")\n",
    "    print_selection_details(\"6UAN_chainD_last_atom\")\n",
    "\n",
    "    cmd.select(\"6UAN_chainD_first3\", \"6UAN_chainD and resi 594-596 and name CA\")\n",
    "    cmd.select(\"6UAN_chainD_last3\", \"6UAN_chainD and resi 621-623 and name CA\")\n",
    "    print_selection_details(\"6UAN_chainD_first3\")\n",
    "    print_selection_details(\"6UAN_chainD_last3\")\n",
    "\n",
    "    # Prepare lists to store distances/RMSDs\n",
    "    euclid_first_atom = []   # single CA (treated as Euclidean distance)\n",
    "    euclid_last_atom = []    # single CA (treated as Euclidean distance)\n",
    "    rmsd_first3_atoms = []   # 3 CA\n",
    "    rmsd_last3_atoms = []    # 3 CA\n",
    "\n",
    "    # Gather PDB files in specified fitted directory\n",
    "    fitted_pdb_files = glob(os.path.join(fitted_pdb_dir, \"*.pdb\"))\n",
    "    total_structures = len(fitted_pdb_files)\n",
    "    print(f\"\\nProcessing directory ({directory_label}): {fitted_pdb_dir}\")\n",
    "    print(f\"Found {total_structures} PDB files.\\n\")\n",
    "\n",
    "    # Keep track of structures with large distances on first/last CA\n",
    "    structures_with_large_euclid_first = []\n",
    "    structures_with_large_euclid_last = []\n",
    "\n",
    "    # Loop over each fitted PDB\n",
    "    for fitted_pdb_file in fitted_pdb_files:\n",
    "        print(f\"Processing file: {fitted_pdb_file}\")\n",
    "        pdb_code = os.path.basename(fitted_pdb_file).split('.')[0]\n",
    "\n",
    "        # Load the fitted structure\n",
    "        cmd.load(fitted_pdb_file, pdb_code)\n",
    "\n",
    "        # Selections for single-atom\n",
    "        cmd.select(f\"{pdb_code}_first_atom\", f\"{pdb_code} and resi 593 and name CA\")\n",
    "        cmd.select(f\"{pdb_code}_last_atom\",  f\"{pdb_code} and resi 619 and name CA\")\n",
    "\n",
    "        # Selections for 3-atom group\n",
    "        cmd.select(f\"{pdb_code}_first3_atoms\", f\"{pdb_code} and resi 593-595 and name CA\")\n",
    "        cmd.select(f\"{pdb_code}_last3_atoms\",  f\"{pdb_code} and resi 617-619 and name CA\")\n",
    "\n",
    "        # Print selection details\n",
    "        print_selection_details(f\"{pdb_code}_first_atom\")\n",
    "        print_selection_details(f\"{pdb_code}_last_atom\")\n",
    "        print_selection_details(f\"{pdb_code}_first3_atoms\")\n",
    "        print_selection_details(f\"{pdb_code}_last3_atoms\")\n",
    "\n",
    "        # Calculate Euclidean distance (single-atom RMSD from PyMOL is effectively the direct distance)\n",
    "        # 1) First atom\n",
    "        try:\n",
    "            d_first = cmd.rms_cur(f\"{pdb_code}_first_atom\", \"6UAN_chainD_first_atom\", matchmaker=-1)\n",
    "            print(f\"Euclidean distance (first atom) for {pdb_code} vs. reference: {d_first:.2f} \")\n",
    "            euclid_first_atom.append(d_first)\n",
    "            if d_first > 2.5:\n",
    "                structures_with_large_euclid_first.append(pdb_code)\n",
    "        except:\n",
    "            print(f\"Error calculating distance for {pdb_code}_first_atom\")\n",
    "\n",
    "        # 2) Last atom\n",
    "        try:\n",
    "            d_last = cmd.rms_cur(f\"{pdb_code}_last_atom\", \"6UAN_chainD_last_atom\", matchmaker=-1)\n",
    "            print(f\"Euclidean distance (last atom)  for {pdb_code} vs. reference: {d_last:.2f} \")\n",
    "            euclid_last_atom.append(d_last)\n",
    "            if d_last > 2.5:\n",
    "                structures_with_large_euclid_last.append(pdb_code)\n",
    "        except:\n",
    "            print(f\"Error calculating distance for {pdb_code}_last_atom\")\n",
    "\n",
    "        # Calculate RMSD for 3 atoms\n",
    "        # 3) First three atoms\n",
    "        try:\n",
    "            rms_first3 = cmd.rms_cur(f\"{pdb_code}_first3_atoms\", \"6UAN_chainD_first3\", matchmaker=-1)\n",
    "            print(f\"RMSD (first 3 atoms) for {pdb_code} vs. reference: {rms_first3:.2f} \")\n",
    "            rmsd_first3_atoms.append(rms_first3)\n",
    "        except:\n",
    "            print(f\"Error calculating RMSD for {pdb_code}_first3_atoms\")\n",
    "\n",
    "        # 4) Last three atoms\n",
    "        try:\n",
    "            rms_last3 = cmd.rms_cur(f\"{pdb_code}_last3_atoms\", \"6UAN_chainD_last3\", matchmaker=-1)\n",
    "            print(f\"RMSD (last 3 atoms)  for {pdb_code} vs. reference: {rms_last3:.2f} \")\n",
    "            rmsd_last3_atoms.append(rms_last3)\n",
    "        except:\n",
    "            print(f\"Error calculating RMSD for {pdb_code}_last3_atoms\")\n",
    "\n",
    "        # Remove the loaded structure from PyMOL to keep things tidy\n",
    "        cmd.delete(pdb_code)\n",
    "        cmd.delete(f\"{pdb_code}_first_atom\")\n",
    "        cmd.delete(f\"{pdb_code}_last_atom\")\n",
    "        cmd.delete(f\"{pdb_code}_first3_atoms\")\n",
    "        cmd.delete(f\"{pdb_code}_last3_atoms\")\n",
    "\n",
    "    # Summaries for large distances from single-atom comparisons\n",
    "    print(\"\\nStructures with Euclidean distance > 2.5  (first atom):\")\n",
    "    print(structures_with_large_euclid_first)\n",
    "    print(\"Structures with Euclidean distance > 2.5  (last atom):\")\n",
    "    print(structures_with_large_euclid_last)\n",
    "\n",
    "    # Percentages\n",
    "    num_large_first = len(structures_with_large_euclid_first)\n",
    "    num_large_last = len(structures_with_large_euclid_last)\n",
    "    pct_large_first = (num_large_first / total_structures) * 100 if total_structures > 0 else 0.0\n",
    "    pct_large_last = (num_large_last / total_structures) * 100 if total_structures > 0 else 0.0\n",
    "    print(f\"\\nNumber of structures > 2.5  (first atom): {num_large_first}\")\n",
    "    print(f\"Percentage: {pct_large_first:.2f}%\")\n",
    "    print(f\"Number of structures > 2.5  (last atom): {num_large_last}\")\n",
    "    print(f\"Percentage: {pct_large_last:.2f}%\")\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot 1: Single-atom as Euclidean distance (violin plot)\n",
    "    ###########################################################################\n",
    "    single_atom_data = [euclid_first_atom, euclid_last_atom]\n",
    "    single_atom_labels = [\"First Atom\", \"Last Atom\"]\n",
    "\n",
    "    fig_euclid, ax_euclid = plt.subplots()\n",
    "    ax_euclid.violinplot(single_atom_data)\n",
    "    ax_euclid.set_xticks(np.arange(1, len(single_atom_labels) + 1))\n",
    "    ax_euclid.set_xticklabels(single_atom_labels)\n",
    "    ax_euclid.set_ylabel(\"Euclidean Distance ()\")\n",
    "    ax_euclid.set_title(f\"Single-Atom Comparison\\n{directory_label}\")\n",
    "    plt.show()\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot 2: Three-atom RMSD (violin plot)\n",
    "    ###########################################################################\n",
    "    triple_atom_data = [rmsd_first3_atoms, rmsd_last3_atoms]\n",
    "    triple_atom_labels = [\"First Three Atoms\", \"Last Three Atoms\"]\n",
    "\n",
    "    fig_rmsd, ax_rmsd = plt.subplots()\n",
    "    ax_rmsd.violinplot(triple_atom_data)\n",
    "    ax_rmsd.set_xticks(np.arange(1, len(triple_atom_labels) + 1))\n",
    "    ax_rmsd.set_xticklabels(triple_atom_labels)\n",
    "    ax_rmsd.set_ylabel(\"RMSD ()\")\n",
    "    ax_rmsd.set_title(f\"Three-Atom Comparison\\n{directory_label}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Finally, remove the reference structure so we can re-run for the next directory\n",
    "    cmd.delete(\"6UAN_chainD\")\n",
    "    cmd.delete(\"6UAN_chainD_first_atom\")\n",
    "    cmd.delete(\"6UAN_chainD_last_atom\")\n",
    "    cmd.delete(\"6UAN_chainD_first3\")\n",
    "    cmd.delete(\"6UAN_chainD_last3\")\n",
    "\n",
    "###############################################################################\n",
    "# Main script - process both directories\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories to process\n",
    "    cases = [\n",
    "        (\"Results/fitted_matlab_segments/mustangs_endsRealignment/\", \"Ends Realignment\"),\n",
    "        (\"Results/fitted_matlab_segments/mustangs_noRealignment/\",  \"No Realignment\")\n",
    "    ]\n",
    "\n",
    "    # Process each directory\n",
    "    for dpath, label in cases:\n",
    "        process_directory(dpath, label)\n",
    "\n",
    "    cmd.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fc504-c470-43da-9076-5d83bac25afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "# Start MATLAB\n",
    "#import matlab.engine\n",
    "from tqdm.notebook import tqdm\n",
    "TARGET_DIR = \"Results/fitted_matlab_segments/mustang\"\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "def pdb_id(fp):\n",
    "    return fp.rsplit(\".\",1)[0].rsplit(\"/\",1)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa60c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_PATH = \"Results/activation_segments/CA_segments/mustang/\"\n",
    "#xyz = md.load(\"/home/marmatt/Documents/projects/BRAF/results/activation_segments/CA_segments/mustang/1A9U_A.pdb\")\n",
    "files = glob(PDB_PATH+\"/*.pdb\")\n",
    "ids = [pdb_id(f) for f in files]\n",
    "print(ids)\n",
    "out = [TARGET_DIR + f\"/{i}.pdb\" for i in ids]\n",
    "#print(files[0],ids[0],out[0])\n",
    "\n",
    "for f, n in tqdm(zip(files,out),total=len(files)):\n",
    "    fitting_code(f,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mdtraj as md\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "xyz = md.load(\"Results/activation_segments/CA_segments/mustang/1AD5_A.pdb\")\n",
    "\n",
    "DFG = xyz.top.to_fasta()[0].find(\"DFG\")\n",
    "APE = xyz.top.to_fasta()[0].find(\"APE\")+2\n",
    "atoms = sum([[atom.index for atom in res.atoms if atom.name == \"CA\"] for res in xyz.top._residues[:]],[])\n",
    "\n",
    "coords = xyz.xyz[0,atoms].T\n",
    "x = coords[0]\n",
    "y = coords[1]\n",
    "z = coords[2]\n",
    "new_coords = md.load(\"Results/fitted_matlab_segments/mustang/1AD5_A.pdb\")\n",
    "\n",
    "atoms = sum([[atom.index for atom in res.atoms if atom.name == \"CA\"] for res in new_coords.top._residues[:]],[])\n",
    "new_coords = new_coords.xyz[0,atoms].T\n",
    "xp = new_coords[0]\n",
    "yp = new_coords[1]\n",
    "zp = new_coords[2]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x,y,z, 'blue',marker=\"o\")\n",
    "ax.plot3D(xp, yp, zp, 'red')\n",
    "plt.tick_params(bottom=False, top=False, labelbottom=False)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
