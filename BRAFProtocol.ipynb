{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from urllib.request import urlretrieve as download\n",
    "from glob import glob as g\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from mypdb import PDB_file as mypdb\n",
    "from Bio.Blast.Applications import NcbipsiblastCommandline\n",
    "from time import time as t\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "from time import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to handle and download the hits from a blast search\n",
    "\n",
    "\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    for k2, v2 in dict2.items():\n",
    "        if k2 in dict1:\n",
    "            if isinstance(dict1[k2], list):\n",
    "                dict1[k2].append(v2)\n",
    "            else:\n",
    "                dict1[k2] = [dict1[k2], v2]\n",
    "        else:\n",
    "            print(f\"New Key : {k2}\")\n",
    "            dict1[k2] = v2\n",
    "    return dict1\n",
    "\n",
    "class hit:\n",
    "    def __init__(self, xml_hit):\n",
    "        self.hit_num = xml_hit[0].text\n",
    "        pdb_info = xml_hit[1].text.split(\"|\")[1:]\n",
    "        self.pdb_id = pdb_info[0]\n",
    "        self.pdb_chain = pdb_info[1]\n",
    "        self.description = xml_hit[2].text\n",
    "        data = [[y.tag.replace(\"Hsp_\", \"\"), y.text] for y in xml_hit[5][0]]\n",
    "        self.data = dict(data)\n",
    "        self.pdb = None\n",
    "        self._not_exists = False\n",
    "\n",
    "    @staticmethod\n",
    "    def download2(code, pdir=None):\n",
    "        base_url = \"https://files.rcsb.org/download\"\n",
    "        pdb_url = f\"{base_url}/{code}.pdb\"\n",
    "        f_p = os.path.join(pdir, f\"{code}.pdb\")\n",
    "        try:\n",
    "            download(pdb_url, f_p)\n",
    "            return f_p  # Return the file path if succeeded\n",
    "        except Exception:\n",
    "            print(f\"File {code} not found.\")\n",
    "            return None  # Return None if failed\n",
    "\n",
    "    def _assign(self, f_p):\n",
    "        self.pdb_path = f_p\n",
    "        self.pdb = mypdb(f_p)\n",
    "        \n",
    "    def check_for_pdb(self, pdir=None):\n",
    "        if pdir is None:\n",
    "            pdir = \"./pdbs/\"\n",
    "        elif pdir[-1] != \"/\":\n",
    "            pdir += \"/\"\n",
    "        if not os.path.isdir(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "        matches = g(pdir + f\"{self.pdb_id}*\")\n",
    "        if matches:\n",
    "            if not self.pdb:\n",
    "                self._assign(matches[0])\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2139fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to handle the blast search\n",
    "class blast:\n",
    "    def __init__(self, sequence, name, database, path, program=\"blastp\", hitlen=10000):\n",
    "        self.sequence = sequence\n",
    "        self.program = program\n",
    "        self.database = database\n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.hitlen = hitlen\n",
    "\n",
    "        if program == \"blastp\" and (self.path and os.path.isfile(self.path)):\n",
    "            self.parse_search()\n",
    "        elif program == \"psiblast\":\n",
    "            self.psiblast_search()\n",
    "        else:\n",
    "            self.bsearch()\n",
    "\n",
    "    def bsearch(self):\n",
    "        # Perform the initial BLASTP search\n",
    "        if self.program == \"blastp\":\n",
    "            print(\"Searching BLASTP...\")\n",
    "            t1 = t()\n",
    "            self.results = NCBIWWW.qblast(self.program, self.database, self.sequence, hitlist_size=self.hitlen)\n",
    "            t2 = t()\n",
    "            print(f\"BLASTP took {round(t2-t1,4)} seconds\")\n",
    "            if not self.path:\n",
    "                self.path = f\"{self.name}-blast.xml\"\n",
    "            with open(self.path, \"w\") as output_xml:\n",
    "                output_xml.write(self.results.read())\n",
    "            self.parse_search()\n",
    "\n",
    "    def psiblast_search(self):\n",
    "        # Run local PSI-BLAST using NcbipsiblastCommandline\n",
    "        print(f\"Running PSI-BLAST on {self.name}...\")\n",
    "        input_fasta = f\"{self.name}.fasta\"\n",
    "        with open(input_fasta, \"w\") as f:\n",
    "            f.write(f\">query\\n{self.sequence}\\n\")\n",
    "\n",
    "        psiblast_cline = NcbipsiblastCommandline(\n",
    "            query=input_fasta,\n",
    "            db=\"/home/marmatt/ncbi-blast-2.16.0+/bin/pdbaa\",\n",
    "            evalue=10,\n",
    "            num_iterations=3,\n",
    "            out_ascii_pssm=f\"{self.name}.pssm\",\n",
    "            out=f\"{self.name}-psiblast.xml\",\n",
    "            outfmt=5\n",
    "        )\n",
    "        stdout, stderr = psiblast_cline()\n",
    "        if stderr:\n",
    "            print(f\"PSI-BLAST ERROR: {stderr}\")\n",
    "        else:\n",
    "            print(\"PSI-BLAST search completed.\")\n",
    "            self.parse_search(xml_file=f\"{self.name}-psiblast.xml\")\n",
    "\n",
    "    def parse_search(self, xml_file=None):\n",
    "        xml_file = xml_file or self.path\n",
    "        if not xml_file:\n",
    "            raise Exception(\"No XML file path provided.\")\n",
    "        t1 = t()\n",
    "        tree = ET.parse(xml_file)\n",
    "        iteration = tree.findall(\"./BlastOutput_iterations/Iteration/\")\n",
    "        self.query_length = iteration[3].text\n",
    "        hits = [hit(x) for x in iteration[-2]] #returns hit objects\n",
    "        self.hits = hits\n",
    "        mega_dict = hits[0].data\n",
    "        for x in hits[1:]:\n",
    "            mega_dict = merge_dicts(mega_dict, x.data)\n",
    "        mega_dict[\"PDB ID\"] = [x.pdb_id for x in hits]\n",
    "        mega_dict[\"Chain\"] = [x.pdb_chain for x in hits]\n",
    "        mega_dict[\"Description\"] = [x.description for x in hits]\n",
    "        self.df = pd.DataFrame.from_dict(mega_dict)\n",
    "        print(self.df)\n",
    "        t2 = t()\n",
    "        print(f\"Time taken to parse {t2-t1}\")\n",
    "    \n",
    "    def download_pdbs(self, pdir=None):\n",
    "        default_dir = \"./PDBs\"\n",
    "        pdir = os.path.abspath(pdir if pdir else default_dir)\n",
    "        if not os.path.isdir(pdir):\n",
    "            os.makedirs(pdir, exist_ok=True)\n",
    "        \n",
    "        files = [os.path.splitext(f)[0] for f in os.listdir(pdir)]\n",
    "        hit_bar = tqdm(self.hits, desc=\"Processing Hits\")\n",
    "        \n",
    "        for x in hit_bar:\n",
    "            if x.pdb_id not in files:\n",
    "                hit_bar.set_description(f\"Downloading {x}\")\n",
    "                \n",
    "                try:\n",
    "                    file_path = x.download2(x.pdb_id, pdir=pdir)\n",
    "\n",
    "                    if file_path:\n",
    "                        x._assign(file_path)\n",
    "                    else:\n",
    "                        # In case download2 does not return a valid path\n",
    "                        raise Exception(\"Download failed\")\n",
    "                except Exception as e:\n",
    "                    # Print a message if download fails or file path is invalid\n",
    "                    print(f\"Structure {x.pdb_id} was not found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f2b43",
   "metadata": {},
   "source": [
    "## Actually running the blast search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Here we perform a blast search on:\n",
    " 1.   The BRAF monomer\n",
    "\"\"\"\n",
    "with open(\"./fastas.txt\") as f:\n",
    "    fastas = f.readlines()\n",
    "\n",
    "braf_fasta = fastas[1]\n",
    "name = \"braf\"\n",
    "\n",
    "# Directory for storing blast search results\n",
    "output_dir = \"./blast_search\"\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "res_path = os.path.join(output_dir, f\"{name}-blast.xml\")  # Path for the results file\n",
    "\n",
    "# Check for saved results in the specified directory\n",
    "if os.path.exists(res_path):\n",
    "    bs = blast(braf_fasta, name, database=\"pdb\", path=res_path)\n",
    "else:\n",
    "    bs = blast(braf_fasta, name, database=\"pdb\", path=None)\n",
    "\n",
    "bs.download_pdbs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b84c34d",
   "metadata": {},
   "source": [
    "## Counting the number of pdb files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'PDBs'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af58070b",
   "metadata": {},
   "source": [
    "## Counting the number of pdb hits in the xml file and checking which ones have not been downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0522d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_pdb_files(directory):\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Extract the base filenames (without extension) to compare with PDB IDs\n",
    "    pdb_file_ids = {os.path.splitext(os.path.basename(f))[0] for f in pdb_files}\n",
    "\n",
    "    return pdb_file_ids\n",
    "\n",
    "def find_unique_and_duplicate_pdb_hit_ids(xml_file):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Dictionary to count occurrences of each PDB hit ID\n",
    "    hit_id_counts = defaultdict(int)\n",
    "\n",
    "    # Iterate over all Hit elements in the XML\n",
    "    for hit in root.findall('.//Hit'):\n",
    "        # Extract the Hit_id text\n",
    "        hit_id = hit.find('Hit_id').text\n",
    "\n",
    "        # Assuming the Hit_id format is 'pdb|PDB_ID|Chain', extract the PDB_ID\n",
    "        pdb_id = hit_id.split('|')[1]\n",
    "\n",
    "        # Increment the count for this PDB_ID\n",
    "        hit_id_counts[pdb_id] += 1\n",
    "\n",
    "    # Find all PDB IDs (unique and duplicates)\n",
    "    all_hit_ids = set(hit_id_counts.keys())\n",
    "\n",
    "    return all_hit_ids\n",
    "\n",
    "# Specify the directory and XML file\n",
    "pdb_directory = 'PDBs'\n",
    "xml_file = 'braf-blast.xml'\n",
    "\n",
    "# Get the PDB file IDs from the directory\n",
    "pdb_file_ids = count_pdb_files(pdb_directory)\n",
    "\n",
    "# Get all PDB hit IDs from the XML\n",
    "all_pdb_ids = find_unique_and_duplicate_pdb_hit_ids(xml_file)\n",
    "\n",
    "# Calculate the number of total PDB hits\n",
    "total_pdb_hits = len(all_pdb_ids)\n",
    "\n",
    "# Find PDB IDs in XML that are not in the directory\n",
    "missing_pdb_ids = all_pdb_ids - pdb_file_ids\n",
    "\n",
    "print(f\"There are {total_pdb_hits} total PDB hits in the file '{xml_file}'.\")\n",
    "print(f\"There are {len(missing_pdb_ids)} PDB IDs in the XML not found in the directory '{pdb_directory}':\")\n",
    "print(missing_pdb_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b9660",
   "metadata": {},
   "source": [
    "## Here we are stripping the downloaded pdb files to only contain the chain of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc269f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "from time import time as t\n",
    "!ls\n",
    "def sglob(fp, absolute=True):\n",
    "    fps = sorted(glob(fp))\n",
    "    if absolute:\n",
    "        fps = [os.path.abspath(f) for f in fps]\n",
    "    return fps\n",
    "\n",
    "def strip_to_chain(pdb_file, chain_ID):\n",
    "    u = mda.Universe(pdb_file)\n",
    "    print(f\"Loaded trajectory from {pdb_file} with {len(u.atoms)} atoms.\")\n",
    "\n",
    "    chain = u.select_atoms(f\"protein and chainID {chain_ID}\")\n",
    "    if len(chain) == 0:\n",
    "        print(f\"Chain {chain_ID} not found in {pdb_file}.\")\n",
    "        return None\n",
    "    return chain\n",
    "\n",
    "def post_process(fname):\n",
    "    with open(fname, \"r\") as f_o:\n",
    "        initial_lines = f_o.readlines()\n",
    "\n",
    "    print(f\"File {fname} before post_process, first few lines:\")\n",
    "    print(\"\".join(initial_lines[:20]))\n",
    "\n",
    "    final_lines = initial_lines[-2:].copy()\n",
    "    no_ter = [line for line in initial_lines if line[:3] != \"TER\" or line in final_lines]\n",
    "\n",
    "    if len(no_ter) != len(initial_lines):\n",
    "        with open(fname, \"w\") as f_o:\n",
    "            print(f\"Rewriting {fname}, lines reduced from {len(initial_lines)} to {len(no_ter)}\")\n",
    "            f_o.write(\"\".join(no_ter))\n",
    "\n",
    "    print(f\"File {fname} after post_process, first few lines:\")\n",
    "    with open(fname, \"r\") as f_r:\n",
    "        print(\"\".join(f_r.readlines()[:20]))\n",
    "\n",
    "def parse_xml(xml_file):\n",
    "    hit_id = re.compile(r\"<Hit_id>(.*?)<.Hit_id>\")\n",
    "    with open(xml_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "        results = [h.split(\"|\")[1:] for h in hit_id.findall(text)]\n",
    "        pdb_chain_dict = {}\n",
    "        for r in results:\n",
    "            pdb_chain_dict[r[0]+f\"_{r[1]}\"] = r[1]\n",
    "    return pdb_chain_dict\n",
    "\n",
    "def get_pdb_id(fp):\n",
    "    fp = fp.rsplit(\".\", 1)[0]\n",
    "    if \"/\" in fp:\n",
    "        fp = fp.rsplit(\"/\", 1)[1]\n",
    "    return fp\n",
    "\n",
    "def target_name(fp, target_dir, chain):\n",
    "    orig_path, file_name = fp.rsplit(\"/\", 1)\n",
    "    fp = fp.replace(orig_path, target_dir)\n",
    "    fp = fp.split(\".\")[0] + f\"_{chain}.pdb\"\n",
    "    return fp\n",
    "\n",
    "def find_pdb_file(PDB_chain_id, files):\n",
    "    print(files)\n",
    "    if \"_\" in PDB_chain_id:\n",
    "        PDB_id = PDB_chain_id.split(\"_\")[0]\n",
    "    assert len(PDB_id) == 4\n",
    "    for f in files:\n",
    "        filename = os.path.basename(f).split('.')[0]\n",
    "        if filename.startswith(PDB_id):\n",
    "            print(f\"Found file: {f} for PDB ID: {PDB_id}\")\n",
    "            return f\n",
    "\n",
    "print(\"BEGIN\")\n",
    "t1 = t()\n",
    "xml = \"braf-blast.xml\"\n",
    "pdb_dir = \"PDBs\"\n",
    "target_dir = \"Results/activation_segments/unaligned\"\n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "xml_chain_dict = parse_xml(xml)\n",
    "print(xml_chain_dict)\n",
    "\n",
    "pdb_files = sorted(sglob(\"PDBs/*.pdb\"))\n",
    "print(pdb_files)\n",
    "keys = sorted([*xml_chain_dict.keys()], key=get_pdb_id)\n",
    "\n",
    "print(keys)\n",
    "files = [find_pdb_file(k, pdb_files) for k in keys]\n",
    "print(files)\n",
    "chain_IDs = [xml_chain_dict[k] for k in keys]\n",
    "print(len(chain_IDs))\n",
    "\n",
    "valid_file_chain_pairs = [(f, c) for f, c in zip(files, chain_IDs) if f is not None]\n",
    "print(len(valid_file_chain_pairs))\n",
    "new_file_paths = [target_name(f, target_dir, c) for f, c in valid_file_chain_pairs]\n",
    "print(new_file_paths)\n",
    "file_paths = [f for f, c in valid_file_chain_pairs]\n",
    "print(file_paths)\n",
    "print(len(file_paths))\n",
    "\n",
    "for (fp, chain_ID, tp) in zip(file_paths, chain_IDs, new_file_paths):\n",
    "    if fp is not None:\n",
    "        try:\n",
    "            print(fp, chain_ID, tp)\n",
    "            chain = strip_to_chain(fp, chain_ID)\n",
    "            if chain is not None:\n",
    "                with mda.Writer(tp) as w:\n",
    "                    w.write(chain)\n",
    "                post_process(tp)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {fp} with chain {chain_ID}: {e}\")\n",
    "        \n",
    "t2 = t()\n",
    "t_t = round(t2 - t1, 3) // 60\n",
    "t_t = str((t_t // 60)) + \":\" + str(t_t % 60)\n",
    "print(f\"Time taken {t_t} for sequential processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616153e7",
   "metadata": {},
   "source": [
    "## Counting again how many pdb files are in the directory after stripping the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'Results/activation_segments/unaligned'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e014c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the full sequence from the pdb files, checking if there are any non-natural amino acids and substituting them and selecting only sequences with a maximum gap length of 4 amino acids to be reconstructed\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from Bio.SeqUtils import seq1\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def substitute_non_natural_amino_acid(residue, aligned_atom, index):\n",
    "    \"\"\"Substitute non-natural amino acids with their natural counterparts.\"\"\"\n",
    "    substitutions = {\n",
    "        'X': 'G',  # Glycine\n",
    "        'B': 'N',  # Asparagine\n",
    "        'Z': 'Q',  # Glutamine\n",
    "        'J': 'L'   # Leucine\n",
    "    }\n",
    "    \n",
    "    if residue == 'X':\n",
    "        # Check if 'X' is surrounded by missing amino acids in aligned_atom\n",
    "        if index > 0 and aligned_atom[index - 1] == '-':\n",
    "            return residue\n",
    "        if index < len(aligned_atom) - 1 and aligned_atom[index + 1] == '-':\n",
    "            return residue\n",
    "\n",
    "    return substitutions.get(residue, residue)\n",
    "\n",
    "def extract_seqres_sequence(pdb_file):\n",
    "    \"\"\"Extract SEQRES sequences for each chain from a PDB file.\"\"\"\n",
    "    seq_dict = {}\n",
    "    with open(pdb_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    current_chain = None\n",
    "    current_seq = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"SEQRES\"):\n",
    "            parts = line.split()\n",
    "            chain_id = parts[2]\n",
    "            if chain_id != current_chain:\n",
    "                if current_chain is not None:\n",
    "                    seq_dict[current_chain] = ''.join(seq1(residue) for residue in current_seq)\n",
    "                current_chain = chain_id\n",
    "                current_seq = []\n",
    "            current_seq.extend(parts[4:])\n",
    "\n",
    "    if current_chain is not None:\n",
    "        seq_dict[current_chain] = ''.join(seq1(residue) for residue in current_seq)\n",
    "\n",
    "    return seq_dict\n",
    "\n",
    "def extract_atom_sequence(pdb_file, chain_id):\n",
    "    \"\"\"Extract sequence from atomic coordinates for a specific chain.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        chain = model[chain_id]\n",
    "        \n",
    "        ppb = PPBuilder()\n",
    "        sequence = ''\n",
    "        for pp in ppb.build_peptides(chain):\n",
    "            sequence += pp.get_sequence()\n",
    "        return sequence\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    index = sequence.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "def align_and_highlight_gaps(seqres_segment, atom_segment):\n",
    "    \"\"\"Align SEQRES and ATOM segments and highlight gaps in ATOM.\"\"\"\n",
    "    aligned_seqres = ''\n",
    "    aligned_atom = ''\n",
    "    atom_index = 0\n",
    "    max_gap_length = 0\n",
    "    current_gap_length = 0\n",
    "\n",
    "    for res_seqres in seqres_segment:\n",
    "        if atom_index < len(atom_segment) and res_seqres == atom_segment[atom_index]:\n",
    "            aligned_seqres += res_seqres\n",
    "            aligned_atom += atom_segment[atom_index]\n",
    "            atom_index += 1\n",
    "            current_gap_length = 0\n",
    "        else:\n",
    "            aligned_seqres += res_seqres\n",
    "            aligned_atom += '-'\n",
    "            current_gap_length += 1\n",
    "            max_gap_length = max(max_gap_length, current_gap_length)\n",
    "\n",
    "    return aligned_seqres, aligned_atom, max_gap_length\n",
    "\n",
    "def main():\n",
    "    target_dir = \"Results/activation_segments/unaligned\"\n",
    "    pdb_dir = \"PDBs\"\n",
    "    fasta_output_file = \"seqres_sequences.fasta\"  # File to store full sequences\n",
    "    text_output_file = \"seqres_info.txt\"\n",
    "    aligned_sequences = {}\n",
    "    satisfying_structures_count = 0\n",
    "\n",
    "    pdb_files = glob(os.path.join(target_dir, \"*.pdb\"))\n",
    "\n",
    "    with open(text_output_file, \"w\") as text_output, open(fasta_output_file, \"w\") as fasta_output:\n",
    "        for pdb_file in tqdm(pdb_files, desc=\"Processing PDB files\"):\n",
    "            pdb_name = os.path.basename(pdb_file)\n",
    "            pdb_id, chain_id_file = os.path.splitext(pdb_name)[0].split('_')\n",
    "            chain_id = chain_id_file\n",
    "\n",
    "            full_pdb_path = os.path.join(pdb_dir, pdb_id + '.pdb')\n",
    "            if not os.path.isfile(full_pdb_path):\n",
    "                print(f\"Corresponding full PDB for {pdb_id} not found.\")\n",
    "                continue\n",
    "\n",
    "            seqres_seqs = extract_seqres_sequence(full_pdb_path)\n",
    "            atom_seq = extract_atom_sequence(full_pdb_path, chain_id)\n",
    "\n",
    "            if chain_id in seqres_seqs and atom_seq:\n",
    "                seqres_sequence = seqres_seqs[chain_id]\n",
    "                seqres_dfg_index = find_motif_indices(seqres_sequence, 'DFG')\n",
    "                seqres_ape_index = find_motif_indices(seqres_sequence, 'APE')\n",
    "                atom_dfg_index = find_motif_indices(atom_seq, 'DFG')\n",
    "                atom_ape_index = find_motif_indices(atom_seq, 'APE')\n",
    "\n",
    "                # Determine the start and end indices for the segments\n",
    "                if None not in [seqres_dfg_index, seqres_ape_index, atom_dfg_index, atom_ape_index]:\n",
    "                    seqres_start = min(seqres_dfg_index, seqres_ape_index)\n",
    "                    seqres_end = max(seqres_dfg_index + 3, seqres_ape_index + 3)\n",
    "                    atom_start = min(atom_dfg_index, atom_ape_index)\n",
    "                    atom_end = max(atom_dfg_index + 3, atom_ape_index + 3)\n",
    "\n",
    "                    seqres_segment = seqres_sequence[seqres_start:seqres_end]\n",
    "                    atom_segment = atom_seq[atom_start:atom_end]\n",
    "\n",
    "                    aligned_seqres, aligned_atom, max_gap_length = align_and_highlight_gaps(seqres_segment, atom_segment)\n",
    "                    \n",
    "                    # Check for differences and substitute non-natural amino acids\n",
    "                    exclude_due_to_non_natural_diff = False\n",
    "                    corrected_seqres = ''\n",
    "                    for index, (res_seqres, res_atom) in enumerate(zip(aligned_seqres, aligned_atom)):\n",
    "                        if res_seqres != res_atom:\n",
    "                            corrected_residue = substitute_non_natural_amino_acid(res_seqres, aligned_atom, index)\n",
    "                            corrected_seqres += corrected_residue\n",
    "                            if corrected_residue != res_seqres:\n",
    "                                print(f\"Substituting non-natural amino acid '{res_seqres}' with '{corrected_residue}' in SEQRES for {pdb_id}_{chain_id}.\")\n",
    "                        else:\n",
    "                            corrected_seqres += res_seqres\n",
    "\n",
    "                    if not exclude_due_to_non_natural_diff and max_gap_length <= 4:\n",
    "                        satisfying_structures_count += 1\n",
    "                        info = (f\"Aligned Sequences for {pdb_id}_{chain_id}: (Max gap length: {max_gap_length})\\n\"\n",
    "                                f\"SEQRES Segment: {corrected_seqres}\\n\"\n",
    "                                f\"ATOM Segment:   {aligned_atom}\\n\\n\")\n",
    "                        print(info)\n",
    "                        text_output.write(info)\n",
    "                        \n",
    "                        aligned_sequences[f\"{pdb_id}_{chain_id}_SEQRES\"] = corrected_seqres\n",
    "                        aligned_sequences[f\"{pdb_id}_{chain_id}_ATOM\"] = aligned_atom\n",
    "\n",
    "                        # Write full SEQRES and ATOM sequences to the FASTA file\n",
    "                        fasta_output.write(f\">{pdb_id}_{chain_id}_SEQRES\\n{seqres_sequence}\\n\")\n",
    "                        fasta_output.write(f\">{pdb_id}_{chain_id}_ATOM\\n{atom_seq}\\n\")\n",
    "                    else:\n",
    "                        exclusion_msg = f\"Excluding {pdb_id}_{chain_id} due to gap length: {max_gap_length} or non-natural amino acid difference.\\n\"\n",
    "                        print(exclusion_msg)\n",
    "                        text_output.write(exclusion_msg)\n",
    "                else:\n",
    "                    motif_msg = f\"Motifs not found in {pdb_id}_{chain_id}.\\n\"\n",
    "                    print(motif_msg)\n",
    "                    text_output.write(motif_msg)\n",
    "            else:\n",
    "                chain_msg = f\"Chain {chain_id} not found in SEQRES of {pdb_id} or no atomic sequence available.\\n\"\n",
    "                print(chain_msg)\n",
    "                text_output.write(chain_msg)\n",
    "                \n",
    "        count_msg = f\"Total structures satisfying the condition: {satisfying_structures_count}\"\n",
    "        text_output.write(count_msg)\n",
    "        print(count_msg)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COunting how many seqences are in the fasta file\n",
    "def count_total_pdb_ids(file_path):\n",
    "    total_pdb_ids = 0\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('>'):\n",
    "                total_pdb_ids += 1\n",
    "\n",
    "    print(f\"Total number of PDB IDs: {int(total_pdb_ids/2)} in {file_path}\") #here we divide by 2 because we have two lines per PDB ID\n",
    "\n",
    "# Provide the path to your seqres_sequence.fasta file\n",
    "file_path = \"seqres_sequences.fasta\"\n",
    "count_total_pdb_ids(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to use MODELLER to reconstruct the sequences that have a gap length of 4 or less, if there are no differences between the SEQRES and ATOM sequences, the original PDB file is copied to the target directory\n",
    "# FIX was to eliminate remarks from the pdb file\n",
    "import os\n",
    "import shutil\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "from modeller import *\n",
    "from modeller.automodel import *\n",
    "\n",
    "def read_fasta_sequences(fasta_file):\n",
    "    \"\"\"Read sequences from a FASTA file into a dictionary.\"\"\"\n",
    "    sequences = {}\n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        header = None\n",
    "        sequence = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if header:\n",
    "                    sequences[header] = ''.join(sequence)\n",
    "                header = line[1:]\n",
    "                sequence = []\n",
    "            else:\n",
    "                sequence.append(line)\n",
    "        if header:\n",
    "            sequences[header] = ''.join(sequence)\n",
    "    return sequences\n",
    "\n",
    "def extract_atom_sequence(pdb_file):\n",
    "    \"\"\"Extract sequence from atomic coordinates for the first chain found in the PDB file.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            ppb = PPBuilder()\n",
    "            sequence = ''\n",
    "            for pp in ppb.build_peptides(chain):\n",
    "                sequence += pp.get_sequence()\n",
    "            return str(sequence)\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    index = sequence.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "def reconstruct_with_modeller(pdb_chain_id, pdb_path, target_path, full_sequence, atom_sequence):\n",
    "    print(f\"Processing {pdb_chain_id}\")\n",
    "\n",
    "    # Find indices of the DFG and APE motifs\n",
    "    seqres_dfg_index = find_motif_indices(full_sequence, 'DFG')\n",
    "    seqres_ape_index = find_motif_indices(full_sequence, 'APE')\n",
    "    atom_dfg_index = find_motif_indices(atom_sequence, 'DFG')\n",
    "    atom_ape_index = find_motif_indices(atom_sequence, 'APE')\n",
    "\n",
    "    # Determine if reconstruction is needed\n",
    "    if None not in [seqres_dfg_index, seqres_ape_index, atom_dfg_index, atom_ape_index]:\n",
    "        seqres_start = min(seqres_dfg_index, seqres_ape_index)\n",
    "        seqres_end = max(seqres_dfg_index + 3, seqres_ape_index + 3)\n",
    "        atom_start = min(atom_dfg_index, atom_ape_index)\n",
    "        atom_end = max(atom_dfg_index + 3, atom_ape_index + 3)\n",
    "\n",
    "        seqres_segment = full_sequence[seqres_start:seqres_end]\n",
    "        atom_segment = atom_sequence[atom_start:atom_end]\n",
    "\n",
    "        # Check for differences in the segment\n",
    "        if seqres_segment != atom_segment:\n",
    "            print(f\"Reconstructing full sequence for {pdb_chain_id} using MODELLER\")\n",
    "\n",
    "            # Setting up MODELLER\n",
    "            env = environ()\n",
    "            aln = alignment(env)\n",
    "            \n",
    "            # Read the structure to work on\n",
    "            mdl = model(env, file=pdb_path)\n",
    "            aln.append_model(mdl, align_codes='template', atom_files=pdb_path)\n",
    "\n",
    "            # Append the full target sequence\n",
    "            aln.append_sequence(full_sequence)\n",
    "            aln[-1].code = 'target'\n",
    "            \n",
    "            # Perform the alignment\n",
    "            aln.align2d(max_gap_length=50)\n",
    "\n",
    "            # Create AutoModel object and build models\n",
    "            a = automodel(env, alnfile=aln, knowns='template', sequence='target')\n",
    "            a.starting_model = 1\n",
    "            a.ending_model = 1\n",
    "            \n",
    "            # Build the model\n",
    "            a.make()\n",
    "            \n",
    "            # Save the best model to the target directory\n",
    "            model_path = os.path.join(target_path, f\"{pdb_chain_id}_filled.pdb\")\n",
    "            os.rename(a.outputs[0]['name'], model_path)\n",
    "            print(f\"Reconstruction completed for {pdb_chain_id}. File saved at {model_path}\")\n",
    "\n",
    "            # Post-process to remove REMARK lines\n",
    "            remove_remark_lines(model_path)\n",
    "        else:\n",
    "            # No reconstruction needed, copy original PDB\n",
    "            shutil.copy(pdb_path, os.path.join(target_path, f\"{pdb_chain_id}.pdb\"))\n",
    "            print(f\"No differences found for {pdb_chain_id}. Original PDB copied to target directory.\")\n",
    "    else:\n",
    "        print(f\"Motifs not found in {pdb_chain_id}.\")\n",
    "\n",
    "def remove_remark_lines(pdb_file):\n",
    "    \"\"\"Remove lines starting with 'REMARK' from the PDB file.\"\"\"\n",
    "    with open(pdb_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    with open(pdb_file, 'w') as file:\n",
    "        for line in lines:\n",
    "            if not line.startswith(\"REMARK\"):\n",
    "                file.write(line)\n",
    "    \n",
    "    print(f\"Removed REMARK lines from {pdb_file}\")\n",
    "\n",
    "def main():\n",
    "    seqres_fasta = \"seqres_sequences.fasta\"\n",
    "    pdb_dir = \"Results/activation_segments/unaligned\"\n",
    "    target_dir = \"Results/activation_segments/reconstructedModeller\"\n",
    "\n",
    "    # Read the sequences from the FASTA file\n",
    "    seqres_sequences = read_fasta_sequences(seqres_fasta)\n",
    "\n",
    "    for header, full_sequence in seqres_sequences.items():\n",
    "        if \"_SEQRES\" in header:  # Only consider SEQRES entries\n",
    "            pdb_chain_id = header.replace(\"_SEQRES\", \"\")\n",
    "            pdb_file_path = os.path.join(pdb_dir, f\"{pdb_chain_id}.pdb\")\n",
    "\n",
    "            # Extract the atomic sequence\n",
    "            atom_sequence = extract_atom_sequence(pdb_file_path)\n",
    "\n",
    "            if atom_sequence is None:\n",
    "                print(f\"Could not extract sequence for {pdb_chain_id}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(target_dir):\n",
    "                os.makedirs(target_dir)\n",
    "            \n",
    "            reconstruct_with_modeller(pdb_chain_id, pdb_file_path, target_dir, full_sequence, atom_sequence)\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of pdb files in the directory after reconstruction\n",
    "import glob\n",
    "import os\n",
    "def count_pdb_files(directory):\n",
    "    # Ensure the directory path ends with a slash\n",
    "    directory = os.path.join(directory, '')\n",
    "\n",
    "    # Use glob to find all .pdb files in the directory\n",
    "    pdb_files = glob.glob(os.path.join(directory, '*.pdb'))\n",
    "\n",
    "    # Return the count of .pdb files\n",
    "    return len(pdb_files)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory = 'Results/activation_segments/reconstructedModeller'\n",
    "\n",
    "# Get the count of PDB files\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119237e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fast checking if reconstruction was successful, need to just change the pdb id and chain id\n",
    "import MDAnalysis as mda\n",
    "import nglview as nv\n",
    "from Bio.PDB import PDBParser\n",
    "from MDAnalysis.analysis import align\n",
    "\n",
    "# Dictionary to convert three-letter amino acid codes to one-letter codes\n",
    "three_to_one = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D',\n",
    "    'CYS': 'C', 'GLU': 'E', 'GLN': 'Q', 'GLY': 'G',\n",
    "    'HIS': 'H', 'ILE': 'I', 'LEU': 'L', 'LYS': 'K',\n",
    "    'MET': 'M', 'PHE': 'F', 'PRO': 'P', 'SER': 'S',\n",
    "    'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
    "}\n",
    "\n",
    "def extract_sequence_and_mapping(pdb_file):\n",
    "    \"\"\"Extract sequence and create a mapping from sequence index to PDB residue ID.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    sequence = []\n",
    "    index_to_resid = {}\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                if 'CA' in residue:  # Check if it's an amino acid\n",
    "                    resname = residue.get_resname()\n",
    "                    if resname in three_to_one:\n",
    "                        sequence.append(three_to_one[resname])\n",
    "                        index_to_resid[len(sequence) - 1] = residue.get_id()[1]  # Map sequence index to PDB resid\n",
    "            break\n",
    "        break\n",
    "\n",
    "    return sequence, index_to_resid\n",
    "\n",
    "def find_motif_indices(sequence, motif):\n",
    "    \"\"\"Find the start index of a motif in a sequence.\"\"\"\n",
    "    sequence_str = ''.join(sequence)\n",
    "    index = sequence_str.find(motif)\n",
    "    return index if index != -1 else None\n",
    "\n",
    "# Extract the sequence and mapping for the single chain\n",
    "atom_sequence, index_to_resid = extract_sequence_and_mapping(\"Results/activation_segments/unaligned/7OPO_A.pdb\")\n",
    "\n",
    "# Find indices of DFG and APE motifs\n",
    "dfg_index = find_motif_indices(atom_sequence, 'DFG')\n",
    "ape_index = find_motif_indices(atom_sequence, 'APE')\n",
    "\n",
    "# Ensure indices are found and select the residues between them\n",
    "if dfg_index is not None and ape_index is not None:\n",
    "    # Use the mapping to get the correct residue IDs\n",
    "    dfg_resid = index_to_resid[dfg_index]\n",
    "    ape_resid = index_to_resid[ape_index + 2]  # +2 to include the entire 'APE' motif\n",
    "\n",
    "    u_missing = mda.Universe(\"Results/activation_segments/unaligned/7OPO_A.pdb\")\n",
    "    selected_atoms = u_missing.select_atoms(f\"resid {dfg_resid}:{ape_resid}\")\n",
    "\n",
    "    print(\"Number of Atoms Selected:\", selected_atoms.n_atoms)\n",
    "\n",
    "    u_reconstructed = mda.Universe(\"Results/activation_segments/reconstructedModeller/7OPO_A_filled.pdb\")\n",
    "    print(\"Number of Atoms Reconstructed:\", u_reconstructed.select_atoms(f\"all\").n_atoms)\n",
    "\n",
    "    # Merge the aligned atoms for visualization\n",
    "    merged = mda.Merge(selected_atoms, u_reconstructed.atoms)\n",
    "    print(merged.residues)\n",
    "\n",
    "    # Create NGLView widget\n",
    "    w = nv.show_mdanalysis(merged)\n",
    "\n",
    "    # Add a representation for each residue name with the corresponding color\n",
    "    w.clear()\n",
    "    w.add_cartoon(color=\"resname\")\n",
    "\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"Motifs not found in the sequence.\")\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions and function to run MUSTANG on the reconstructed pdb files\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from glob import glob as g\n",
    "import mdtraj as md\n",
    "from mpi4py import MPI\n",
    "from time import time as t\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def sg(f_p):\n",
    "    return sorted(g(f_p))\n",
    "\n",
    "\n",
    "def find_pdbs(directory):\n",
    "    \"\"\"\n",
    "    Find topologies in a directory.\n",
    "    Currently excludes cif files.\n",
    "    \"\"\"\n",
    "    return sg(directory+\"/*.pdb\")\n",
    "\n",
    "\n",
    "def fname(file):\n",
    "    return file.rsplit(\".\", 1)[0].rsplit(\"/\", 1)[-1]\n",
    "\n",
    "\n",
    "def ifnotmake(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "\n",
    "\n",
    "def run_mustang(f1, f2, name=None):\n",
    "    \"\"\"\n",
    "    Writes a MUSTANG input file which aligns\n",
    "    file1 to file 2.\n",
    "    If no name defaults to the second file.\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = fname(f2)\n",
    "    if not os.path.isdir(f\"./{name}\"):\n",
    "        os.makedirs(f\"./{name}\")\n",
    "    new_fp = f\"./{name}/{name}\"\n",
    "    structs = f\"{f1} {f2} \"\n",
    "    command = f\"/home/marmatt/Downloads/MUSTANG_v3.2.4/bin/mustang-3.2.4 -i {structs} -o {new_fp} -F fasta -s ON\" #here you should change the path to where you install MUSTANG\n",
    "    command = command.split()\n",
    "    new_fp = f\"{new_fp}.pdb\"\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True)\n",
    "        print(f\"Running command: {' '.join(command)}\")\n",
    "        print(f\"STDOUT: {result.stdout}\")\n",
    "        print(f\"STDERR: {result.stderr}\")\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error in MUSTANG execution\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        return None\n",
    "    return new_fp\n",
    "\n",
    "\n",
    "def postprocess(file_path):\n",
    "    \"\"\"\n",
    "    file_path is the name of a pdb file.\n",
    "    It deletes the first chain which is always the alignment structures\n",
    "    \"\"\"\n",
    "    structure = md.load(file_path)\n",
    "    aligned_chain_idx = [[atom.index for atom in res.atoms] for res in\n",
    "                         structure.top._chains[1]._residues]\n",
    "    aligned_chain_idx = sum(aligned_chain_idx, [])\n",
    "    structure = structure.atom_slice(aligned_chain_idx)\n",
    "    structure.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993598b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform MUSTANG alignment on the sequences that have not been reconstructed, I need to fix this bug\n",
    "\n",
    "#from mustang import *\n",
    "import subprocess\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Constants (most to be made variable)\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "# Define the paths explicitly\n",
    "pdb_path = \"Results/activation_segments/reconstructedModeller\"\n",
    "target_dir = \"Results/activation_segments/mustangs\"\n",
    "template_pdb = \"6UAN_chainD.pdb\"\n",
    "# Ensure the target directory exists\n",
    "ifnotmake(target_dir)\n",
    "pdb_path = os.path.abspath(pdb_path)\n",
    "target_dir = os.path.abspath(target_dir)\n",
    "template_pdb = os.path.abspath(template_pdb)\n",
    "print(pdb_path, target_dir, template_pdb)\n",
    "os.chdir(target_dir)\n",
    "os.system(\"pwd\")\n",
    "if rank == 0:\n",
    "    pdbs = find_pdbs(pdb_path)\n",
    "    n_files = len(pdbs)\n",
    "    n_slices = (n_files // size)\n",
    "    step = int(n_files / n_slices)\n",
    "    if n_files % n_slices != 0:\n",
    "        n_slices += 1\n",
    "    slices = [slice(i*n_slices, (i+1)*n_slices) for i in range(step)]\n",
    "    pdbs = [pdbs[s] for s in slices]\n",
    "else:\n",
    "    pdbs = None\n",
    "\n",
    "pdbs = comm.scatter(pdbs, root=0)\n",
    "print(\"RANK:\\t\", rank, \"DATA SIZE:\\t\", len(pdbs))\n",
    "t1 = t()\n",
    "failures = []\n",
    "for pdb in tqdm(pdbs):\n",
    "    name = fname(pdb)\n",
    "    new_fp = run_mustang(template_pdb, pdb, name=name)\n",
    "    if new_fp:\n",
    "        if os.path.isfile(new_fp):\n",
    "            postprocess(new_fp)\n",
    "        else:\n",
    "            failures.append(pdb)\n",
    "    else:\n",
    "        failures.append(pdb)\n",
    "t2 = t()\n",
    "print(\"FINISHED RANK:\\t\", rank, \"DATA SIZE:\\t\", len(pdbs),\n",
    "      \"TIME:\\t\", round(t2-t1, 4))\n",
    "failures = comm.gather(failures, root=0)\n",
    "if rank == 0:\n",
    "    failures = sum(failures, [])\n",
    "    with open(\"./failures.txt\", \"w\") as f_o:\n",
    "        f_o.write(\"\\n\".join(f for f in failures))\n",
    "    t2 = t()\n",
    "    print(round(t2-t1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14388f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of directories representing the number of pdb files that have been aligned\n",
    "import os\n",
    "def count_directories(directory):\n",
    "    # List all entries in the given directory\n",
    "    entries = os.listdir(directory)\n",
    "\n",
    "    # Use os.path.join to get the full path and os.path.isdir to check if it's a directory\n",
    "    directories = [entry for entry in entries if os.path.isdir(os.path.join(directory, entry))]\n",
    "\n",
    "    # Return the count of directories\n",
    "    return len(directories)\n",
    "\n",
    "# Specify the directory\n",
    "directory_path = 'Results/activation_segments/mustangs'\n",
    "\n",
    "# Get the count of directories\n",
    "directory_count = count_directories(directory_path)\n",
    "\n",
    "print(f\"There are {directory_count} directories in the directory '{directory_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of files representing the number of pdb files that did not need reconstruction\n",
    "import os\n",
    "from glob import glob as g\n",
    "\n",
    "def count_non_filled_pdbs(directory):\n",
    "    # Find all PDB files in the directory\n",
    "    pdb_files = g(os.path.join(directory, \"*.pdb\"))\n",
    "\n",
    "    # Filter out files with '_filled' in their names\n",
    "    non_filled_pdbs = [pdb for pdb in pdb_files if \"_filled\" not in os.path.basename(pdb)]\n",
    "\n",
    "    # Return the count of non '_filled' PDB files\n",
    "    return len(non_filled_pdbs)\n",
    "\n",
    "# Specify the directory\n",
    "pdb_directory_path = 'Results/activation_segments/reconstructedModeller'\n",
    "\n",
    "# Get the count of non '_filled' PDB files\n",
    "non_filled_pdb_count = count_non_filled_pdbs(pdb_directory_path)\n",
    "\n",
    "print(f\"There are {non_filled_pdb_count} PDB files without '_filled' in the directory '{pdb_directory_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47016e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from pprint import pprint as pp\n",
    "alignment_dir = \"Results/activation_segments/mustangs\"\n",
    "\n",
    "class alignment:\n",
    "    \"\"\"\n",
    "    Class to hold alignments.\n",
    "    Currently only supports braf_monomers!\n",
    "    \"\"\"\n",
    "    def __init__(self,name,seq1,seq2):\n",
    "        self.name = name\n",
    "        self.seq1 = seq1\n",
    "        self.seq2 = seq2\n",
    "        self.aligned = self.find_aligned()\n",
    "\n",
    "    def find_pdb(self):\n",
    "        pdb_files = []\n",
    "        for root, dirs, files in os.walk(alignment_dir):\n",
    "            pdb_files += [os.path.join(root, file) for file in files if file.endswith('.pdb')]\n",
    "        \n",
    "        pdb = [f for f in pdb_files if self.name in f]\n",
    "        print(pdb)\n",
    "        if len(pdb) == 1:\n",
    "            return pdb[0]\n",
    "        \n",
    "\n",
    "    def find_aligned(self):\n",
    "        aligned = []\n",
    "        for char1, char2 in zip(self.seq1,self.seq2):\n",
    "            if char1 != \"-\":\n",
    "                aligned.append((char1,char2))\n",
    "        return aligned\n",
    "\n",
    "    def aligned_res(self):\n",
    "        seq1, seq2 = self.seq1, self.seq2\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        seq_length = len(aligned)\n",
    "        pdb2_top = self.load_pdb()\n",
    "        full_seq2 = \"\".join(char for char in seq2 if char != \"-\")\n",
    "        residues = pdb2_top.top._residues\n",
    "        n_res = len(residues)\n",
    "        res_counter = 0\n",
    "        for i in range(seq_length):\n",
    "            if res_counter >= n_res:\n",
    "                break\n",
    "            if aligned[i][1] != \"-\":\n",
    "                aligned[i][1] = residues[res_counter]\n",
    "                res_counter += 1\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def aligned_xyz(self):\n",
    "        \"\"\"\n",
    "        Return xyz of aligned residues\n",
    "        \"\"\"\n",
    "        xyz = self.load_pdb()._xyz[0] # Only one frame\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        for k,(_,res) in enumerate(self.residues):\n",
    "            if not isinstance(res,str):\n",
    "                idxs = []\n",
    "                for atom in res._atoms:\n",
    "                    idxs.append(atom.index)\n",
    "                res_xyz = xyz[idxs]\n",
    "                aligned[k][1] = res_xyz\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def aligned_ca_xyz(self):\n",
    "        \"\"\"\n",
    "        Return xyz of aligned residues\n",
    "        \"\"\"\n",
    "        xyz = self.load_pdb()._xyz[0] # Only one frame\n",
    "        aligned = [[*item] for item in self.aligned]\n",
    "        for k,(_,res) in enumerate(self.residues):\n",
    "            if not isinstance(res,str):\n",
    "                for atom in res._atoms:\n",
    "                    if atom.name == \"CA\":\n",
    "                        idxs = atom.index\n",
    "                        break\n",
    "                try:\n",
    "                    res_xyz = xyz[idxs]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(\"ERROR FOR:\")\n",
    "                    print(self.name)\n",
    "                    return None\n",
    "                aligned[k][1] = res_xyz\n",
    "            else:\n",
    "                continue\n",
    "        return [tuple(a) for a in aligned]\n",
    "\n",
    "    def load_pdb(self):\n",
    "        return md.load(self.pdb_file)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return (self.seq1[idx],self.seq2[idx])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "    def find_match_id(self):\n",
    "        seq1, seq2 = self.seq1, self.seq2\n",
    "        full_seq2 = \"\".join(char for char in seq2 if char != \"-\")\n",
    "        pp(full_seq2)\n",
    "        aligned = self.aligned\n",
    "        actv_low = 155\n",
    "        actv_hgh = 181\n",
    "        match_residues = aligned[actv_low:actv_hgh] # These are what we need\n",
    "        seq2_Seq = [a[1] for a in match_residues if a[1] != \"-\"]\n",
    "        begin_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19861f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afasta_parse(file):\n",
    "    \"\"\"\n",
    "    Parse mustang afasta format output file.\n",
    "    Returns two lists of equal length\n",
    "    \"\"\"\n",
    "    with open(file,\"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    names = [l.split(\".\")[0][1:] for l in lines if l[0] == \">\"]\n",
    "    for i in range(1,len(lines)):\n",
    "        if lines[i].isspace():\n",
    "            lines[i] = \"BREAK\"\n",
    "            break\n",
    "        elif lines[i][0] == \">\":\n",
    "            lines[i] = \"BREAK\" + lines[i]\n",
    "            break\n",
    "    lines = [l.strip() for l in lines if l[0] != \">\"]\n",
    "    lines = \"\".join(lines)\n",
    "    fastas = lines.split(\"BREAK\")\n",
    "    fastas = [*filter(None,fastas)]\n",
    "    return fastas[0], fastas[1]\n",
    "\n",
    "def load_alignments(kind=\"mustang\"):\n",
    "    if kind==\"mustang\":\n",
    "        #ppath = \"/home/marmatt/Documents/projects/BRAF/myWork/reproduceBRAFWork/Results/activation_segments/mustangs/mustang_alignments.fasta\"\n",
    "        ppath = \"Results/activation_segments/mustangs/mustang_alignments.fasta\"\n",
    "        \n",
    "        #print(\"Loading pickled alignments...\")\n",
    "    elif kind==\"blast\":\n",
    "        ppath = \"blast_alignments.fasta\"\n",
    "    if os.path.isfile(ppath):\n",
    "        with open(ppath,\"rb\") as pickled:\n",
    "            #print(\"Loading pickled alignments...\")\n",
    "            return p.load(pickled)\n",
    "    else:\n",
    "        make_align_pickle()\n",
    "        #print(\"No pickled alignments found. Creating...\")\n",
    "        return load_alignments()\n",
    "    \n",
    "def make_align_pickle(kind=\"mustang\"):\n",
    "    if kind == \"mustang\":\n",
    "\n",
    "        alignments = []\n",
    "\n",
    "        # Iterate over directories in the alignment directory\n",
    "        for directory_name in os.listdir(alignment_dir):\n",
    "            directory_path = os.path.join(alignment_dir, directory_name)\n",
    "\n",
    "            # Ensure we are working with directories\n",
    "            if os.path.isdir(directory_path):\n",
    "                #print(f\"Processing directory: {directory_name}\")\n",
    "                fasta_files = tqdm(glob(os.path.join(directory_path, \"*.afasta\")), desc=f\"Processing {directory_name} .afasta files\")\n",
    "                \n",
    "                for fasta_file in fasta_files:\n",
    "                    name = os.path.splitext(os.path.basename(fasta_file))[0]\n",
    "                    fasta_files.set_description(f\"Working on {name}\")\n",
    "\n",
    "                    # Simulate the alignment logic\n",
    "                    aligned = alignment(name, *afasta_parse(fasta_file))  # Assuming `alignment` and `afasta_parse` are predefined\n",
    "                    alignments.append(aligned)\n",
    "\n",
    "        # Define a path for the output pickle file\n",
    "        ppath = os.path.join(alignment_dir, \"mustang_alignments.fasta\")\n",
    "        with open(ppath, \"wb\") as pickled:\n",
    "            p.dump(alignments, pickled)\n",
    "    \n",
    "    elif kind == \"blast\":\n",
    "        b = BLAST_results()\n",
    "        alignments = []\n",
    "        for k, dicti in tqdm(b.alignments.items(),total=len(b.alignments)):\n",
    "            seq1 = dicti[\"Query\"]\n",
    "            seq2 = dicti[\"Subject\"]\n",
    "            alignments.append(alignment(k,seq1,seq2))\n",
    "        ppath = \"blast_alignments.fasta\"\n",
    "        with open(ppath, \"wb\") as pickled:\n",
    "            p.dump(alignments, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a51087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from glob import glob as g\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams.update({'font.size': 8})\n",
    "kind = \"mustang\"\n",
    "make_align_pickle(kind) #create MSA file --> important, substituted .p with .html in alignment class\n",
    "aligned = load_alignments(kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a71ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some helper functions to extract the sequence from the pdb files and to extract the sequence from the alignment files\n",
    "def braf_res():\n",
    "    fp = \"./6UAN_chainD.pdb\"\n",
    "    top = md.load(fp).top\n",
    "    return [res_namer(res) for res in top.residues]\n",
    "\n",
    "\n",
    "def res_namer(res):\n",
    "    return f\"{res.name}-{res.resSeq}\"\n",
    "\n",
    "def fname(fp):\n",
    "    return fp.rsplit(\".\",1)[0].rsplit(\"/\",1)[-1]\n",
    "\n",
    "def make_seg(a):\n",
    "    seq = [t for t in a.aligned if t[0] != \"-\"]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting the number of aligned residues over the BRAF BLAST search results to show what are the most conserved residues throughout the alignment\n",
    "\"\"\"\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "print(aligned)\n",
    "seq1mag = len(aligned[0].seq1.replace(\"-\",\"\"))\n",
    "counts = np.zeros(seq1mag)\n",
    "for a in aligned:\n",
    "    segment = make_seg(a)\n",
    "    for i,(b,c) in enumerate(segment):\n",
    "        if c != \"-\":\n",
    "            counts[i] += 1\n",
    "counts =  counts / max(counts)\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "fig,ax = plt.subplots(1,figsize=(10,5))\n",
    "x = [*range(len(counts))]\n",
    "ax.set_xticks(x[::5])\n",
    "x_lbl = braf_res()\n",
    "ax.set_xticklabels(x_lbl[::5],rotation=90,fontsize=7)\n",
    "# subtract 11\n",
    "ax.axvspan(36,48, facecolor='g', alpha=0.5)\n",
    "ax.axvspan(92,100, facecolor='c', alpha=0.5)\n",
    "ax.axvspan(144,168, facecolor='r', alpha=0.5)\n",
    "ax.axvspan(177,186, facecolor='y', alpha=0.5)\n",
    "ax.axvspan(204,215, facecolor='pink', alpha=0.8)\n",
    "ax.axvspan(222,240, facecolor='dodgerblue', alpha=0.8)\n",
    "ax.bar(x,counts,linewidth=0.05,width=1)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n",
    "title = ax.set_title(\"Number of MUSTANG aligned residues over the BRAF BLAST search results\")\n",
    "ax1 = plt.xlabel(\"Resiude Name-Number\")\n",
    "ax1 = plt.ylabel(\"Percent matching in structural alignments\")\n",
    "ax.tick_params(length=2,color=\"black\",direction=\"out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell finds PDBs with DFG and APE motifs aligned.\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Initialize counters and lists\n",
    "count = 0\n",
    "new_aligned = []\n",
    "bad = []\n",
    "\n",
    "# First filtering loop\n",
    "for k, a in enumerate(aligned):\n",
    "    if a.seq1.find(\"DFG\") == -1 or a.seq1.find(\"APE\") == -1:\n",
    "        bad.append(a)\n",
    "        count += 1\n",
    "    else:\n",
    "        new_aligned.append(a)\n",
    "\n",
    "print(count, \"/\", len(aligned), \" don't match.\")\n",
    "print(f\"Continuing with {len(new_aligned)} structures\")\n",
    "\n",
    "# Second filtering and analysis loop\n",
    "counts = {}\n",
    "counter = 0\n",
    "new_aligned2 = []\n",
    "aligning_segs = {}\n",
    "lengths = []\n",
    "\n",
    "for a in new_aligned:\n",
    "    DFG_index = a.seq1.find(\"DFG\")\n",
    "    APE_index = a.seq1.find(\"APE\")\n",
    "    dfg = a.seq2[DFG_index]\n",
    "    ape = a.seq2[APE_index]\n",
    "    \n",
    "    if dfg != \"-\" and ape != \"-\":\n",
    "        counter += 1\n",
    "        length = APE_index - DFG_index\n",
    "        counts.setdefault(length, []).append((a.name, DFG_index, APE_index))\n",
    "        new_aligned2.append(a)\n",
    "        aligning_segs.setdefault(a.name,\n",
    "                                 (a.seq2[DFG_index:DFG_index+3],\n",
    "                                  a.seq2[APE_index:APE_index+3],\n",
    "                                  a.seq2[DFG_index:APE_index+3]))\n",
    "        lengths.append(len(a.seq2[DFG_index:APE_index+3]))\n",
    "\n",
    "print(f\"{counter} structures with an alignment to the D and A\")\n",
    "\n",
    "# Write to CSV\n",
    "with open('alignment_results.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Seq1 Segment', 'Seq2 Segment', 'Status'])\n",
    "\n",
    "    # Write new_aligned2\n",
    "    for a in new_aligned2:\n",
    "        DFG_index = a.seq1.find(\"DFG\")\n",
    "        APE_index = a.seq1.find(\"APE\")\n",
    "        seq1_segment = a.seq1[DFG_index:APE_index+3]\n",
    "        seq2_segment = a.seq2[DFG_index:APE_index+3]\n",
    "        writer.writerow([a.name, seq1_segment, seq2_segment, 'Aligned'])\n",
    "\n",
    "    # Write bad\n",
    "    for a in bad:\n",
    "        writer.writerow([a.name, '', '', 'Not Aligned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5addee",
   "metadata": {},
   "source": [
    "Code for aligning the ends of the activation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymol import cmd\n",
    "from glob import glob\n",
    "import os\n",
    "from Bio.PDB import PDBParser, PPBuilder\n",
    "\n",
    "# Paths\n",
    "pdb_dir = \"Results/activation_segments/mustangs/\"\n",
    "reference_pdb = \"6UAN_chainD.pdb\"\n",
    "output_dir = \"Results/activation_segments/mustangs_realigned/\"\n",
    "image_output_path = \"Results/aligned_loops.png\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the reference structure\n",
    "cmd.load(reference_pdb, \"6UAN_chainD\")\n",
    "\n",
    "# Create selections for the reference structure\n",
    "cmd.select(\"6UAN_chainD_dfg_selection\", \"6UAN_chainD and resi 594-596 and name CA\")\n",
    "cmd.select(\"6UAN_chainD_ape_selection\", \"6UAN_chainD and resi 621-623 and name CA\")\n",
    "cmd.select(\"6UAN_chainD_ends_selection\", \"6UAN_chainD and (resi 594-596 or resi 621-623) and name CA\")\n",
    "\n",
    "def extract_atom_sequence(pdb_file):\n",
    "    \"\"\"Extract sequence from atomic coordinates for the first chain found in the PDB file.\"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('PDB', pdb_file)\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            ppb = PPBuilder()\n",
    "            sequence = ''\n",
    "            for pp in ppb.build_peptides(chain):\n",
    "                sequence += pp.get_sequence()\n",
    "            return str(sequence)\n",
    "    return None\n",
    "\n",
    "def find_motif_indices(seq, motif):\n",
    "    index = seq.find(motif)\n",
    "    if index == -1:\n",
    "        return None\n",
    "    return index, index + len(motif)\n",
    "\n",
    "def print_residues_in_selection(selection_name):\n",
    "    model = cmd.get_model(selection_name)\n",
    "    residues = set((atom.resi, atom.resn) for atom in model.atom)\n",
    "    print(f\"Residues in {selection_name}: {sorted(residues)}\")\n",
    "\n",
    "def process_structure(pdb_file, ref_name=\"6UAN_chainD\"):\n",
    "    pdb_code = os.path.basename(pdb_file).split('.')[0]\n",
    "    cmd.load(pdb_file, pdb_code)\n",
    "\n",
    "    # Extract sequence from atomic coordinates\n",
    "    seq = extract_atom_sequence(pdb_file)\n",
    "    if seq is None:\n",
    "        print(f\"Skipping {pdb_code} due to inability to extract sequence.\")\n",
    "        return\n",
    "    print(f\"Sequence for {pdb_code}: {seq}\")\n",
    "\n",
    "    # Find indices for DFG and APE\n",
    "    dfg_indices = find_motif_indices(seq, \"DFG\")\n",
    "    ape_indices = find_motif_indices(seq, \"APE\")\n",
    "\n",
    "    if not dfg_indices or not ape_indices:\n",
    "        print(f\"Skipping {pdb_code} due to missing motifs.\")\n",
    "        return\n",
    "\n",
    "    # Create selections using sequence indices\n",
    "    dfg_residues = list(range(dfg_indices[0] + 1, dfg_indices[1] + 1))\n",
    "    ape_residues = list(range(ape_indices[0] + 1, ape_indices[1] + 1))\n",
    "\n",
    "    # Create selections\n",
    "    cmd.select(f\"{pdb_code}_dfg_selection\", f\"{pdb_code} and resi {dfg_residues[0]}-{dfg_residues[-1]} and name CA\")\n",
    "    cmd.select(f\"{pdb_code}_ape_selection\", f\"{pdb_code} and resi {ape_residues[0]}-{ape_residues[-1]} and name CA\")\n",
    "    cmd.select(f\"{pdb_code}_ends_selection\", f\"{pdb_code} and (resi {dfg_residues[0]}-{dfg_residues[-1]} or resi {ape_residues[0]}-{ape_residues[-1]}) and name CA\")\n",
    "\n",
    "    # Print residues in selections\n",
    "    print_residues_in_selection(f\"{pdb_code}_dfg_selection\")\n",
    "    print_residues_in_selection(f\"{pdb_code}_ape_selection\")\n",
    "    print_residues_in_selection(f\"{pdb_code}_ends_selection\")\n",
    "\n",
    "    # RMSD before alignment\n",
    "    rms_dfg_before = cmd.rms_cur(f\"{pdb_code}_dfg_selection\", f\"{ref_name}_dfg_selection\", matchmaker=-1)\n",
    "    rms_ape_before = cmd.rms_cur(f\"{pdb_code}_ape_selection\", f\"{ref_name}_ape_selection\", matchmaker=-1)\n",
    "    rms_ends_before = cmd.rms_cur(f\"{pdb_code}_ends_selection\", f\"{ref_name}_ends_selection\", matchmaker=-1)\n",
    "\n",
    "    print(f\"Before alignment RMSD for {pdb_code}: DFG={rms_dfg_before}, APE={rms_ape_before}, ENDS={rms_ends_before}\")\n",
    "\n",
    "    # Aligning\n",
    "    cmd.align(f\"{pdb_code}_ends_selection\", f\"{ref_name}_ends_selection\", cycles=0, transform=1)\n",
    "\n",
    "    # Save aligned structure\n",
    "    aligned_pdb_path = os.path.join(output_dir, f\"{pdb_code}_aligned.pdb\")\n",
    "    cmd.save(aligned_pdb_path, pdb_code)\n",
    "    print(f\"Saved aligned structure to {aligned_pdb_path}\")\n",
    "\n",
    "    # RMSD after alignment\n",
    "    rms_dfg_after = cmd.rms_cur(f\"{pdb_code}_dfg_selection\", f\"{ref_name}_dfg_selection\", matchmaker=-1)\n",
    "    rms_ape_after = cmd.rms_cur(f\"{pdb_code}_ape_selection\", f\"{ref_name}_ape_selection\", matchmaker=-1)\n",
    "    rms_ends_after = cmd.rms_cur(f\"{pdb_code}_ends_selection\", f\"{ref_name}_ends_selection\", matchmaker=-1)\n",
    "\n",
    "    print(f\"After alignment RMSD for {pdb_code}: DFG={rms_dfg_after}, APE={rms_ape_after}, ENDS={rms_ends_after}\")\n",
    "\n",
    "# Load all PDB files and process them\n",
    "fps = glob(pdb_dir + \"/*/*.pdb\")\n",
    "for fp in fps:\n",
    "    print(fp)\n",
    "    process_structure(fp)\n",
    "\n",
    "# Visualize and color\n",
    "cmd.select(\"all_loops\", \"byres 6UAN_chainD_ends_selection\")\n",
    "cmd.color(\"red\", \"6UAN_chainD and 6UAN_chainD_ends_selection\")\n",
    "cmd.color(\"white\", \"not 6UAN_chainD and all_loops\")\n",
    "\n",
    "# Set visualization style\n",
    "cmd.show(\"cartoon\")\n",
    "cmd.hide(\"lines\")\n",
    "\n",
    "# Save the image\n",
    "cmd.png(image_output_path, width=1200, height=800, dpi=300, ray=1)\n",
    "print(f\"Image saved to {image_output_path}\")\n",
    "\n",
    "cmd.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45402a",
   "metadata": {},
   "source": [
    "Code for secondary structure alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "import mdtraj as md\n",
    "from pymol import cmd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "###############################################################################\n",
    "# PART A: Utility functions\n",
    "###############################################################################\n",
    "def find_dfg_motif_index(residues):\n",
    "    \"\"\"\n",
    "    Find the mdtraj residue index of 'ASP' in the DFG motif (ASP,PHE,GLY).\n",
    "    Returns None if not found.\n",
    "    \"\"\"\n",
    "    for i in range(len(residues) - 2):\n",
    "        if (residues[i].name.startswith('ASP') and\n",
    "            residues[i+1].name.startswith('PHE') and\n",
    "            residues[i+2].name.startswith('GLY')):\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def find_ape_motif_index(residues):\n",
    "    \"\"\"\n",
    "    Find the mdtraj residue index of 'GLU' in the APE motif (ALA,PRO,GLU).\n",
    "    Returns None if not found.\n",
    "    \"\"\"\n",
    "    for i in range(len(residues) - 2):\n",
    "        if (residues[i].name.startswith('ALA') and\n",
    "            residues[i+1].name.startswith('PRO') and\n",
    "            residues[i+2].name.startswith('GLU')):\n",
    "            return i + 2\n",
    "    return None\n",
    "\n",
    "def make_resi_selection(object_name, residue_indices):\n",
    "    \"\"\"\n",
    "    Convert a list of PDB residue numbers (not zero-based mdtraj indices)\n",
    "    to a PyMOL selection of CA atoms:\n",
    "    e.g. \"object_name and name CA and (resi 10 or resi 11 or ...)\".\n",
    "    \"\"\"\n",
    "    if not residue_indices:\n",
    "        return f\"{object_name} and name CA\"\n",
    "    parts = [f\"resi {resi_num}\" for resi_num in residue_indices]\n",
    "    joined = \" or \".join(parts)\n",
    "    return f\"{object_name} and name CA and ({joined})\"\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# PART B: Calculate secondary structure conservation with offset logic\n",
    "###############################################################################\n",
    "def calculate_secondary_structure_conservation(\n",
    "    directory,\n",
    "    reference_pdb,\n",
    "    max_structures=800\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load reference, locate D/E => dRef, eRef. \n",
    "       Define slices: [dRef-46..dRef], [eRef..eRef+9].\n",
    "    2) For each structure's D/E => dStruct, eStruct => offsets. \n",
    "       Gather DSSP for that same region in the target.\n",
    "    3) Determine \"conserved residues\" by global frequency > 0.97, not coil, \n",
    "       matching reference assignment.\n",
    "    4) Return a dict => { \"pdbfile\": { \n",
    "         \"ref_indices\": [...], \"targ_indices\": [...],\n",
    "         \"conserved_info\": [ (refIdx, refName, structIdx, structName), ... ] \n",
    "       } }\n",
    "    5) Also copy those PDBs into \"Results/activation_segments/mustangs_conserved_secondary\".\n",
    "    6) Print the per-residue index & name for both reference and each structure \n",
    "       that passes the final test.\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Loading reference structure...\")\n",
    "    ref_traj = md.load(reference_pdb)\n",
    "    ref_residues = list(ref_traj.topology.residues)\n",
    "    ref_dssp = md.compute_dssp(ref_traj)[0]\n",
    "    \n",
    "    # DEBUG\n",
    "    print(f\"[DEBUG] Reference has {len(ref_residues)} residues.\")\n",
    "    \n",
    "    # Find D/E in the reference\n",
    "    dRef = find_dfg_motif_index(ref_residues)\n",
    "    eRef = find_ape_motif_index(ref_residues)\n",
    "    print(f\"[DEBUG] Found DFG motif at ref residue index = {dRef}\")\n",
    "    print(f\"[DEBUG] Found APE motif at ref residue index = {eRef}\")\n",
    "    \n",
    "    if dRef is None or eRef is None:\n",
    "        print(\"[WARNING] Reference doesn't have DFG or APE motif. Aborting.\")\n",
    "        return {}\n",
    "\n",
    "    if dRef < 46 or (eRef + 9) >= len(ref_residues):\n",
    "        print(\"[WARNING] Reference is too short for D-46..D, E..E+9 slices. Aborting.\")\n",
    "        return {}\n",
    "\n",
    "    # Reference slices\n",
    "    ref_d_slice = ref_residues[dRef - 46 : dRef + 1]  # e.g. 46 residues plus D\n",
    "    ref_e_slice = ref_residues[eRef : eRef + 10]      # E..E+9\n",
    "    all_ref_slice = ref_d_slice + ref_e_slice\n",
    "    \n",
    "    # DEBUG\n",
    "    print(\"[DEBUG] Reference D-slice (indices):\", [r.index for r in ref_d_slice])\n",
    "    print(\"[DEBUG] Reference E-slice (indices):\", [r.index for r in ref_e_slice])\n",
    "\n",
    "    # Store reference assignment & names. Also store PDB numbering for each residue.\n",
    "    reference_assignments = {}\n",
    "    reference_names = {}\n",
    "    reference_resnums = {}\n",
    "    for r in all_ref_slice:\n",
    "        reference_assignments[r] = ref_dssp[r.index]\n",
    "        reference_names[r] = r.name\n",
    "        # This is the actual PDB residue number from the topology\n",
    "        reference_resnums[r] = r.resSeq\n",
    "\n",
    "    # DEBUG: Print out the reference secondary structure for that slice\n",
    "    for r in all_ref_slice:\n",
    "        print(f\"[DEBUG] Ref Slice Residue: idx={r.index}, name={r.name}, SS={reference_assignments[r]}\")\n",
    "\n",
    "    structure_assignments = {}\n",
    "    structure_resnames = {}\n",
    "    motif_map = {}\n",
    "    structure_resnums = {}  # (sid, referenceResidue) -> target's PDB numbering\n",
    "\n",
    "    def enough_structures_so_far():\n",
    "        # If we have enough structures for the first residue in ref_d_slice\n",
    "        if not ref_d_slice:\n",
    "            return True\n",
    "        first_r = ref_d_slice[0]\n",
    "        used_ids = {sid for (sid, rr) in structure_assignments if rr == first_r}\n",
    "        return len(used_ids) >= max_structures\n",
    "\n",
    "    # Collect from directory\n",
    "    subdirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    print(f\"[DEBUG] Found {len(subdirs)} subdirectories in {directory}\")\n",
    "\n",
    "    for subdir in subdirs:\n",
    "        if enough_structures_so_far():\n",
    "            break\n",
    "        sub_path = os.path.join(directory, subdir)\n",
    "        pdb_files = [f for f in os.listdir(sub_path) if f.endswith(\".pdb\")]\n",
    "        \n",
    "        # DEBUG\n",
    "        print(f\"[DEBUG] Checking subdirectory: {subdir}, found {len(pdb_files)} pdb files\")\n",
    "\n",
    "        for pdb_file in pdb_files:\n",
    "            if enough_structures_so_far():\n",
    "                break\n",
    "\n",
    "            full_path = os.path.join(sub_path, pdb_file)\n",
    "            traj = md.load(full_path)\n",
    "            residues = list(traj.topology.residues)\n",
    "\n",
    "            # DEBUG\n",
    "            print(f\"[DEBUG] Loading {pdb_file}: total residues={len(residues)}\")\n",
    "\n",
    "            dS = find_dfg_motif_index(residues)\n",
    "            eS = find_ape_motif_index(residues)\n",
    "            if dS is None or eS is None:\n",
    "                print(f\"[DEBUG] {pdb_file}: missing DFG or APE motif.\")\n",
    "                continue\n",
    "            if dS < 46 or (eS + 9) >= len(residues):\n",
    "                print(f\"[DEBUG] {pdb_file}: cannot slice [D-46..D], [E..E+9], skipping.\")\n",
    "                continue\n",
    "\n",
    "            dssp_vals = md.compute_dssp(traj)[0]\n",
    "            struct_d_slice = residues[dS - 46 : dS + 1]\n",
    "            struct_e_slice = residues[eS : eS + 10]\n",
    "\n",
    "            # DEBUG\n",
    "            print(f\"[DEBUG] {pdb_file}: D-slice indices={[r.index for r in struct_d_slice]}\")\n",
    "            print(f\"[DEBUG] {pdb_file}: E-slice indices={[r.index for r in struct_e_slice]}\")\n",
    "\n",
    "            sid = f\"{subdir}/{pdb_file}\"\n",
    "            motif_map[sid] = (dS, eS)\n",
    "\n",
    "            # Build mapping from the reference's D/E slice to the target's\n",
    "            for i, rRef in enumerate(ref_d_slice):\n",
    "                sRes = struct_d_slice[i]\n",
    "                structure_assignments[(sid, rRef)] = dssp_vals[sRes.index]\n",
    "                structure_resnames[(sid, rRef)] = (sRes.name, sRes.index)\n",
    "                # Store the PDB residue number for actual PyMOL selection\n",
    "                structure_resnums[(sid, rRef)] = sRes.resSeq\n",
    "\n",
    "            for i, rRef in enumerate(ref_e_slice):\n",
    "                sRes = struct_e_slice[i]\n",
    "                structure_assignments[(sid, rRef)] = dssp_vals[sRes.index]\n",
    "                structure_resnames[(sid, rRef)] = (sRes.name, sRes.index)\n",
    "                # Store the PDB residue number\n",
    "                structure_resnums[(sid, rRef)] = sRes.resSeq\n",
    "\n",
    "    # Analyze global frequency\n",
    "    residue_to_assigns = defaultdict(list)\n",
    "    for (sid, rRef), dchar in structure_assignments.items():\n",
    "        if rRef in all_ref_slice:\n",
    "            residue_to_assigns[rRef].append(dchar)\n",
    "\n",
    "    print(\"[DEBUG] Counting assignments per reference residue across all structures...\")\n",
    "\n",
    "    def analyze_conservation(data):\n",
    "        out = {}\n",
    "        for r, assigns in data.items():\n",
    "            if not assigns:\n",
    "                out[r] = {\"most_common\": \"-\", \"frequency\": 0.0}\n",
    "            else:\n",
    "                c = Counter(assigns)\n",
    "                mc, cnt = c.most_common(1)[0]\n",
    "                freq = cnt / len(assigns)\n",
    "                out[r] = {\"most_common\": mc, \"frequency\": freq}\n",
    "        return out\n",
    "\n",
    "    conservation = analyze_conservation(residue_to_assigns)\n",
    "\n",
    "    # Decide which residues are \"conserved\"\n",
    "    conserved_residues = []\n",
    "    for rRef in all_ref_slice:\n",
    "        if rRef not in conservation:\n",
    "            continue\n",
    "        mc = conservation[rRef][\"most_common\"]\n",
    "        freq = conservation[rRef][\"frequency\"]\n",
    "        print(f\"[DEBUG] Residue rRef.idx={rRef.index}, refSS={reference_assignments[rRef]}, most_common={mc}, freq={freq:.3f}\")\n",
    "\n",
    "        if freq > 0.97 and mc != 'C' and mc == reference_assignments[rRef]:\n",
    "            conserved_residues.append(rRef)\n",
    "    conserved_residues.sort(key=lambda x: x.index)\n",
    "\n",
    "    print(\"\\n[INFO] Conserved residues in the reference:\")\n",
    "    for rRef in conserved_residues:\n",
    "        mc = conservation[rRef][\"most_common\"]\n",
    "        freq = conservation[rRef][\"frequency\"]\n",
    "        print(f\"  RefIndex={rRef.index}, Name={rRef.name}, freq={freq:.3f}, SS={mc}\")\n",
    "\n",
    "    target_dir = \"Results/activation_segments/mustangs_conserved_secondary\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    all_struct_ids = {s for (s, rr) in structure_assignments}\n",
    "    selected_structures_dict = {}\n",
    "\n",
    "    dRef_start = dRef - 46\n",
    "    dRef_end   = dRef\n",
    "    eRef_start = eRef\n",
    "    eRef_end   = eRef + 9\n",
    "    \n",
    "    print(f\"[DEBUG] Reference slices: D-slice=({dRef_start}..{dRef_end}), E-slice=({eRef_start}..{eRef_end})\")\n",
    "\n",
    "    # Check each structure for \"pass\"\n",
    "    for sid in all_struct_ids:\n",
    "        pass_all = True\n",
    "        for rRef in conserved_residues:\n",
    "            if (sid, rRef) not in structure_assignments:\n",
    "                pass_all = False\n",
    "                break\n",
    "            if structure_assignments[(sid, rRef)] != reference_assignments[rRef]:\n",
    "                pass_all = False\n",
    "                break\n",
    "        \n",
    "        if not pass_all:\n",
    "            continue\n",
    "\n",
    "        subd, pdbfile = sid.split('/', 1)\n",
    "        dS, eS = motif_map[sid]\n",
    "        offsetD = dS - dRef\n",
    "        offsetE = eS - eRef\n",
    "        \n",
    "        print(f\"[DEBUG] PASSING structure: {sid}; dS={dS}, eS={eS}, offsetD={offsetD}, offsetE={offsetE}\")\n",
    "\n",
    "        ref_list  = []\n",
    "        targ_list = []\n",
    "        info_list = []\n",
    "\n",
    "        for rRef in conserved_residues:\n",
    "            iRef = rRef.index\n",
    "            rRefName = rRef.name\n",
    "\n",
    "            # Original offset logic to see which slice it's in\n",
    "            if dRef_start <= iRef <= dRef_end:\n",
    "                iTarg = iRef + offsetD\n",
    "            elif eRef_start <= iRef <= eRef_end:\n",
    "                iTarg = iRef + offsetE\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Instead of the zero-based indices, we put the actual PDB numbering:\n",
    "            pdbRefNum = reference_resnums[rRef]\n",
    "            if (sid, rRef) in structure_resnums:\n",
    "                pdbTargNum = structure_resnums[(sid, rRef)]\n",
    "            else:\n",
    "                pdbTargNum = iTarg  # fallback (unlikely)\n",
    "\n",
    "            # For PyMOL alignment, we want the real PDB numbers\n",
    "            ref_list.append(pdbRefNum)\n",
    "            targ_list.append(pdbTargNum)\n",
    "\n",
    "            # Keep the same print statement data\n",
    "            if (sid, rRef) in structure_resnames:\n",
    "                (targetName, targetIdx) = structure_resnames[(sid, rRef)]\n",
    "            else:\n",
    "                targetName = \"???\"\n",
    "                targetIdx = iTarg\n",
    "\n",
    "            # info_list is the data in your final print statement\n",
    "            info_list.append((iRef, rRefName, targetIdx, targetName))\n",
    "\n",
    "        selected_structures_dict[pdbfile] = {\n",
    "            \"ref_indices\": ref_list,\n",
    "            \"targ_indices\": targ_list,\n",
    "            \"conserved_info\": info_list\n",
    "        }\n",
    "\n",
    "        src_path = os.path.join(directory, subd, pdbfile)\n",
    "        dst_path = os.path.join(target_dir, pdbfile)\n",
    "        if os.path.isfile(src_path):\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(\"\\n[INFO] Final conserved structures & residues (reference -> target):\")\n",
    "    for pdbfile, val in selected_structures_dict.items():\n",
    "        print(f\"  Structure: {pdbfile}\")\n",
    "        for (iRef, nmRef, iTarg, nmTarg) in val[\"conserved_info\"]:\n",
    "            print(f\"    RefIdx={iRef}({nmRef}) => TargIdx={iTarg}({nmTarg})\")\n",
    "\n",
    "    return selected_structures_dict\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# PART C: Realign structures in PyMOL using the offset-based indices\n",
    "###############################################################################\n",
    "def realign_conserved_structures(\n",
    "    selected_structures_dict,\n",
    "    reference_pdb,\n",
    "    source_dir=\"Results/activation_segments/mustangs_conserved_secondary\",\n",
    "    output_dir=\"Results/activation_segments/mustangs_realigned_secondary\",\n",
    "    reference_object_name=\"reference\"\n",
    "):\n",
    "    \"\"\"\n",
    "    For each structure in 'selected_structures_dict':\n",
    "      - We have a list of 'ref_indices' for the reference \n",
    "        and 'targ_indices' for the target. \n",
    "      - Load them in PyMOL, build two selections, do alignment, save.\n",
    "    Also prints the before/after RMS.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # DEBUG\n",
    "    print(f\"[DEBUG] Realign: reference={reference_pdb}, source={source_dir}, output={output_dir}\")\n",
    "\n",
    "    # Load reference\n",
    "    cmd.load(reference_pdb, reference_object_name)\n",
    "    print(\"Num atoms in ref sel:\", cmd.count_atoms(f\"{reference_object_name} and name CA\"))\n",
    "    cmd.select(\"checkIndex\", f\"{reference_object_name} and resi 500 and name CA\")\n",
    "    print(\"Check residue 500:\", cmd.count_atoms(\"checkIndex\"))\n",
    "\n",
    "    cmd.hide(\"everything\", reference_object_name)\n",
    "    cmd.show(\"cartoon\", reference_object_name)\n",
    "    cmd.color(\"cyan\", reference_object_name)\n",
    "\n",
    "    all_pdbs = glob(os.path.join(source_dir, \"*.pdb\"))\n",
    "    print(f\"[INFO] Found {len(all_pdbs)} .pdb in {source_dir}\")\n",
    "\n",
    "    def process_structure(pdb_path, ref_list, targ_list):\n",
    "        pdb_file = os.path.basename(pdb_path)\n",
    "        obj_name = os.path.splitext(pdb_file)[0]\n",
    "        cmd.load(pdb_path, obj_name)\n",
    "\n",
    "        # Now ref_list/targ_list are PDB residue numbers. \n",
    "        sel_ref  = make_resi_selection(reference_object_name, ref_list)\n",
    "        sel_targ = make_resi_selection(obj_name, targ_list)\n",
    "\n",
    "        # DEBUG\n",
    "        print(f\"[DEBUG] process_structure => {pdb_file}\")\n",
    "        print(f\"[DEBUG] - ref_list={ref_list}\")\n",
    "        print(f\"[DEBUG] - targ_list={targ_list}\")\n",
    "        print(f\"[DEBUG] - sel_ref={sel_ref}\")\n",
    "        print(f\"[DEBUG] - sel_targ={sel_targ}\")\n",
    "\n",
    "        n_ref  = cmd.count_atoms(sel_ref)\n",
    "        n_targ = cmd.count_atoms(sel_targ)\n",
    "        if n_ref != n_targ or n_ref == 0:\n",
    "            print(f\"  [WARNING] {pdb_file}: mismatch in selection sizes ({n_ref} vs {n_targ}). Skipping.\")\n",
    "            cmd.delete(obj_name)\n",
    "            return\n",
    "\n",
    "        rms_before = cmd.rms_cur(sel_targ, sel_ref, matchmaker=-1)\n",
    "        print(f\"  RMS before alignment for {pdb_file}: {rms_before:.3f} \")\n",
    "\n",
    "        cmd.super(sel_targ, sel_ref)\n",
    "\n",
    "        rms_after = cmd.rms_cur(sel_targ, sel_ref, matchmaker=-1)\n",
    "        print(f\"  RMS after alignment for {pdb_file}: {rms_after:.3f} \")\n",
    "\n",
    "        out_pdb = os.path.join(output_dir, pdb_file)\n",
    "        cmd.save(out_pdb, obj_name)\n",
    "        print(f\"  [INFO] Saved aligned => {out_pdb}\")\n",
    "\n",
    "        cmd.delete(obj_name)\n",
    "\n",
    "    for pdb_path in all_pdbs:\n",
    "        pdb_file = os.path.basename(pdb_path)\n",
    "        if pdb_file in selected_structures_dict:\n",
    "            ref_list  = selected_structures_dict[pdb_file][\"ref_indices\"]\n",
    "            targ_list = selected_structures_dict[pdb_file][\"targ_indices\"]\n",
    "            print(f\"\\n[INFO] Aligning {pdb_file} with {len(ref_list)} 'conserved' residues.\")\n",
    "            process_structure(pdb_path, ref_list, targ_list)\n",
    "        else:\n",
    "            print(f\"[INFO] Skipping {pdb_file} (not in dictionary).\")\n",
    "\n",
    "    out_img = os.path.join(output_dir, \"aligned_loops_offset.png\")\n",
    "    cmd.png(out_img, width=1200, height=800, dpi=300, ray=1)\n",
    "    print(f\"\\n[INFO] Saved alignment image => {out_img}\")\n",
    "\n",
    "    # If you prefer to quit PyMOL automatically, uncomment the next line:\n",
    "    # cmd.quit()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# PART D: Example usage\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1) Identify \"conserved\" residues across all structures\n",
    "    #    and produce offset-based indexes for each structure.\n",
    "    selected_structs = calculate_secondary_structure_conservation(\n",
    "        directory=\"Results/activation_segments/mustangs\",\n",
    "        reference_pdb=\"6UAN_chainD.pdb\",\n",
    "        max_structures=800\n",
    "    )\n",
    "\n",
    "    # 2) Realign those structures in PyMOL\n",
    "    realign_conserved_structures(\n",
    "        selected_structures_dict=selected_structs,\n",
    "        reference_pdb=\"6UAN_chainD.pdb\",\n",
    "        source_dir=\"Results/activation_segments/mustangs_conserved_secondary\",\n",
    "        output_dir=\"Results/activation_segments/mustangs_realigned_secondary\",\n",
    "        reference_object_name=\"reference\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import biobox as bb\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "################################################################################\n",
    "# Code to save only the first and last CA atoms in activation segment PDB files \n",
    "# both before and after Mustang realignment, for quick verification.\n",
    "################################################################################\n",
    "# --- BEFORE ENDS REALIGNMENT ---\n",
    "files_before = f\"Results/activation_segments/mustangs/*/*.pdb\"\n",
    "\n",
    "start_before = []\n",
    "end_before = []\n",
    "\n",
    "for f in glob.glob(files_before):\n",
    "    M = bb.Molecule(f)\n",
    "    pts = M.atomselect(\"*\", \"*\", \"CA\")\n",
    "    start_before.append(pts[0])\n",
    "    end_before.append(pts[-1])\n",
    "\n",
    "M_start_before = bb.Structure(p=np.array(start_before))\n",
    "M_end_before = bb.Structure(p=np.array(end_before))\n",
    "\n",
    "M_start_before.write_pdb(\"start_CA_beforeReAlign.pdb\")\n",
    "M_end_before.write_pdb(\"end_CA_beforeReAlign.pdb\")\n",
    "\n",
    "# --- AFTER ENDS REALIGNMENT ---\n",
    "files_after = f\"Results/activation_segments/mustangs_realigned/*.pdb\"\n",
    "start_after = []\n",
    "end_after = []\n",
    "\n",
    "for f in glob.glob(files_after):\n",
    "    M = bb.Molecule(f)\n",
    "    pts = M.atomselect(\"*\", \"*\", \"CA\")\n",
    "    start_after.append(pts[0])\n",
    "    end_after.append(pts[-1])\n",
    "\n",
    "M_start_after = bb.Structure(p=np.array(start_after))\n",
    "M_end_after = bb.Structure(p=np.array(end_after))\n",
    "\n",
    "M_start_after.write_pdb(\"start_CA_afterReAlign.pdb\")\n",
    "M_end_after.write_pdb(\"end_CA_afterReAlign.pdb\")\n",
    "\n",
    "# --- AFTER SCONDARY STRUCTURE REALIGNMENT ---\n",
    "files_after = f\"Results/activation_segments/mustangs_conserved_secondary/*.pdb\"\n",
    "start_after = []\n",
    "end_after = []\n",
    "\n",
    "for f in glob.glob(files_after):\n",
    "    M = bb.Molecule(f)\n",
    "    pts = M.atomselect(\"*\", \"*\", \"CA\")\n",
    "    start_after.append(pts[0])\n",
    "    end_after.append(pts[-1])\n",
    "\n",
    "M_start_after = bb.Structure(p=np.array(start_after))\n",
    "M_end_after = bb.Structure(p=np.array(end_after))\n",
    "\n",
    "M_start_after.write_pdb(\"start_CA_afterReAlign_secondary.pdb\")\n",
    "M_end_after.write_pdb(\"end_CA_afterReAlign.pdb_secondary.pdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f99703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "# Function to load a PDB file and strip it down to CA atoms from a specified\n",
    "# start_residue to an end_residue (indices are 0-based).\n",
    "###############################################################################\n",
    "def strip_to_ca(pdb_path, start_residue, end_residue):\n",
    "    \"\"\"\n",
    "    Loads the PDB, extracts CA atoms for residues in the specified slice,\n",
    "    and returns an mdtraj.Trajectory object with just those atoms.\n",
    "    \"\"\"\n",
    "    pdb = md.load(pdb_path)\n",
    "    print(f\"Loaded PDB: {pdb_path}\")\n",
    "\n",
    "    # Extract CA atoms within the specified range of residues\n",
    "    atom_indices = [\n",
    "        atom.index \n",
    "        for res in pdb.top._residues[start_residue:end_residue] \n",
    "        for atom in res.atoms \n",
    "        if atom.name == \"CA\"\n",
    "    ]\n",
    "    print(f\"Atom indices for CA: {atom_indices}\")\n",
    "\n",
    "    return pdb.atom_slice(atom_indices)\n",
    "\n",
    "###############################################################################\n",
    "# Function to process alignments by looking for the DFG and APE motifs in seq1,\n",
    "# verifying alignment in seq2, and saving out the PDB stripped to CA atoms only.\n",
    "###############################################################################\n",
    "def process_alignments(pdb_dir, target_dir, alignments):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdb_dir : str\n",
    "        Directory where the PDB files are stored (e.g., mustangs or mustangs_realigned).\n",
    "    target_dir : str\n",
    "        Directory where stripped PDBs will be saved.\n",
    "    alignments : list\n",
    "        A list of alignment objects, each with attributes like:\n",
    "          - name\n",
    "          - seq1 (aligned sequence 1)\n",
    "          - seq2 (aligned sequence 2)\n",
    "    \"\"\"\n",
    "    # Ensure the target directory exists\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"\\nProcessing:\\n  PDB directory = {pdb_dir}\\n  Target directory = {target_dir}\")\n",
    "\n",
    "    # Find all PDB files in pdb_dir\n",
    "    pdb_files = glob(os.path.join(pdb_dir, \"*.pdb\"))\n",
    "    print(f\"Found PDB files: {pdb_files}\")\n",
    "\n",
    "    # Process each alignment in alignments\n",
    "    for align_obj in tqdm(alignments, desc=\"Processing alignments\"):\n",
    "        print(f\"\\nChecking alignment: {align_obj.name}\")\n",
    "        match_found = False\n",
    "        matching_fp = \"\"\n",
    "\n",
    "        # Look for a matching PDB file\n",
    "        for fp in pdb_files:\n",
    "            if align_obj.name in fp and \"pdb\" in fp:\n",
    "                match_found = True\n",
    "                matching_fp = fp\n",
    "                print(f\"Match found for {align_obj.name} in file: {fp}\")\n",
    "                break\n",
    "        \n",
    "        if not match_found:\n",
    "            print(f\"No match found for {align_obj.name}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Indices of DFG and APE in seq1\n",
    "        DFG_index = align_obj.seq1.find(\"DFG\")\n",
    "        APE_index = align_obj.seq1.find(\"APE\")\n",
    "        print(f\"DFG index (in seq1): {DFG_index} | APE index (in seq1): {APE_index}\")\n",
    "\n",
    "        # Skip if DFG/APE motifs are not found\n",
    "        if DFG_index == -1 or APE_index == -1:\n",
    "            print(f\"Skipping file {matching_fp} as it does not contain the DFG or APE motifs in seq1.\")\n",
    "            continue\n",
    "\n",
    "        # Check if the same segment (DFG) is also aligned in seq2\n",
    "        # (i.e., we want seq2 to also have the substring \"DFG\" in the same place):\n",
    "        if align_obj.seq2[DFG_index:DFG_index + 3] != \"DFG\":\n",
    "            print(f\"Skipping file {matching_fp} as the DFG motif is not aligned properly in seq2.\")\n",
    "            continue\n",
    "\n",
    "        # Adjust DFG and APE indices to account for gaps in seq2\n",
    "        DFG_index_adjusted = DFG_index - sum([1 for a in align_obj.seq2[:DFG_index] if a == \"-\"])\n",
    "        APE_index_adjusted = APE_index - sum([1 for a in align_obj.seq2[:APE_index] if a == \"-\"])\n",
    "        print(f\"Adjusted DFG index: {DFG_index_adjusted} | Adjusted APE index: {APE_index_adjusted}\")\n",
    "\n",
    "        # Strip to CA atoms between the adjusted DFG and APE (including the 3 residues of \"APE\")\n",
    "        stripped = strip_to_ca(matching_fp, DFG_index_adjusted, APE_index_adjusted + 3)\n",
    "\n",
    "        # Construct the file name and save the stripped pdb\n",
    "        new_name = os.path.join(target_dir, os.path.basename(matching_fp))\n",
    "        print(f\"Saving stripped PDB to: {new_name}\")\n",
    "        stripped.save(new_name)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Example usage\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    # 1) Process for the ends realigned directory\n",
    "    pdb_dir_realigned = \"Results/activation_segments/mustangs_realigned/\"\n",
    "    target_dir_realigned = f\"Results/activation_segments/CA_segments/mustangs_endsRealignment\"\n",
    "    process_alignments(pdb_dir_realigned, target_dir_realigned, new_aligned2)\n",
    "\n",
    "    # 2) Process for the non-realigned Mustang directory\n",
    "    pdb_dir_no_realign = \"Results/activation_segments/mustangs/*/\"\n",
    "    target_dir_no_realign = \"Results/activation_segments/CA_segments/mustangs_noRealignment\"\n",
    "    process_alignments(pdb_dir_no_realign, target_dir_no_realign, new_aligned2)\n",
    "\n",
    "    # 3) Process for the secondary structure realigned directory\n",
    "    pdb_dir_second_realign = \"Results/activation_segments/mustangs_conserved_secondary/\"\n",
    "    target_dir_second_realign = \"Results/activation_segments/CA_segments/mustangs_secondaryRealignment\"\n",
    "    process_alignments(pdb_dir_second_realign, target_dir_second_realign, new_aligned2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d358bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import PDB\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import tempfile\n",
    "import mdtraj as md\n",
    "\n",
    "def fitting_code(fp_or_traj, save_path):\n",
    "    # Function to read PDB file or trajectory object and get model\n",
    "    def read_structure(input_data):\n",
    "        if isinstance(input_data, str):\n",
    "            # If input is a string, treat it as a file path\n",
    "            parser = PDB.PDBParser(QUIET=True)\n",
    "            structure = parser.get_structure('structure', input_data)\n",
    "        elif isinstance(input_data, md.Trajectory):\n",
    "            # If input is a trajectory, save to temp PDB and read\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".pdb\", delete=False) as tmpfile:\n",
    "                input_data.save(tmpfile.name)\n",
    "                tmpfile.close()\n",
    "                parser = PDB.PDBParser(QUIET=True)\n",
    "                structure = parser.get_structure('structure', tmpfile.name)\n",
    "            os.unlink(tmpfile.name)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported input type. Provide a file path or md.Trajectory.\")\n",
    "        return structure[0]\n",
    "\n",
    "    # Read the template for CA atoms configuration\n",
    "    template_model = read_structure('template.pdb')\n",
    "    Nnew = len([atom for atom in template_model.get_atoms() if atom.get_id() == 'CA'])\n",
    "\n",
    "    # Read input PDB file or trajectory\n",
    "    my_model = read_structure(fp_or_traj)\n",
    "    atom_list = [atom for atom in my_model.get_atoms() if atom.get_id() == 'CA']\n",
    "\n",
    "    # Coordinates for each CA atom\n",
    "    avg = np.array([atom.coord for atom in atom_list])\n",
    "    n = len(atom_list)\n",
    "\n",
    "    # Fit cubic interpolation for each axis (x, y, z)\n",
    "    dims = ['x', 'y', 'z']\n",
    "    fits = {}\n",
    "    for j, dim in enumerate(dims):\n",
    "        fits[dim] = interp1d(np.arange(n), avg[:, j], kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "    # Create a finer grid of points (X) for interpolation\n",
    "    X = np.arange(0, n - 1, 0.1) \n",
    "    # Gradient in each dimension\n",
    "    dYdX = {dim: np.gradient(fits[dim](X)) for dim in dims}\n",
    "    # Speed along path (magnitude of the gradient)\n",
    "    Y = np.sqrt(sum(np.square(dYdX[dim]) for dim in dims))\n",
    "    # Total arc length (area under the speed curve)\n",
    "    L = np.trapz(Y, X)\n",
    "\n",
    "    # Create an evenly spaced set of arc lengths (Li)\n",
    "    Li = np.linspace(0, L, Nnew)\n",
    "\n",
    "    # Precompute partial arc length at each step in X\n",
    "    flen = np.array([np.trapz(Y[:ibig], X[:ibig]) for ibig in range(1, len(X))])\n",
    "\n",
    "    # For each required point (Nnew), find the corresponding index in X\n",
    "    pt = np.zeros(Nnew, dtype=int)\n",
    "    for i in range(Nnew):\n",
    "        pt[i] = np.argmin(np.abs(flen - Li[i]))\n",
    "\n",
    "    # Interpolated 3D coordinates for each new point\n",
    "    new_coords = np.array([[fits[dim](X[pt[i]]) for dim in dims] for i in range(Nnew)])\n",
    "\n",
    "    # Update the template CA positions with interpolated coordinates\n",
    "    ca_index = 0\n",
    "    for atom in template_model.get_atoms():\n",
    "        if atom.get_id() == 'CA':\n",
    "            atom.set_coord(new_coords[ca_index])\n",
    "            ca_index += 1\n",
    "\n",
    "    # Save the updated structure\n",
    "    try:\n",
    "        with open(save_path, \"w\") as file:\n",
    "            io = PDB.PDBIO()\n",
    "            io.set_structure(template_model)\n",
    "            io.save(file)\n",
    "        print(f'Successfully saved the structure to {save_path}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during file save: {e}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Process two different input/output directory pairs:\n",
    "#   1) mustangs_endsRealignment\n",
    "#   2) mustangs_noRealignment\n",
    "###############################################################################\n",
    "\n",
    "cases = [\n",
    "    (\n",
    "       \"Results/activation_segments/CA_segments/mustangs_endsRealignment/\",\n",
    "       \"Results/fitted_matlab_segments/mustangs_endsRealignment/\"\n",
    "    ),\n",
    "    (\n",
    "       \"Results/activation_segments/CA_segments/mustangs_noRealignment/\",\n",
    "       \"Results/fitted_matlab_segments/mustangs_noRealignment/\"\n",
    "    ),\n",
    "    (\n",
    "       \"Results/activation_segments/CA_segments/mustangs_secondaryRealignment\",\n",
    "       \"Results/fitted_matlab_segments/mustangs_conserved_secondary/\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for input_dir, output_dir in cases:\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\nProcessing directory:\\n  Input:  {input_dir}\\n  Output: {output_dir}\")\n",
    "\n",
    "    # Process each PDB file in the input directory\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.pdb'):\n",
    "            input_file_path = os.path.join(input_dir, file_name)\n",
    "            output_file_path = os.path.join(output_dir, file_name)\n",
    "            fitting_code(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code requires same numbering of residues in all structures and that is why it is run after fitting\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mdtraj as md\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "xyz = md.load(\"Results/activation_segments/CA_segments/mustang/1AD5_A.pdb\")\n",
    "\n",
    "DFG = xyz.top.to_fasta()[0].find(\"DFG\")\n",
    "APE = xyz.top.to_fasta()[0].find(\"APE\")+2\n",
    "atoms = sum([[atom.index for atom in res.atoms if atom.name == \"CA\"] for res in xyz.top._residues[:]],[])\n",
    "\n",
    "coords = xyz.xyz[0,atoms].T\n",
    "x = coords[0]\n",
    "y = coords[1]\n",
    "z = coords[2]\n",
    "new_coords = md.load(\"Results/fitted_matlab_segments/mustang/1AD5_A.pdb\")\n",
    "\n",
    "atoms = sum([[atom.index for atom in res.atoms if atom.name == \"CA\"] for res in new_coords.top._residues[:]],[])\n",
    "new_coords = new_coords.xyz[0,atoms].T\n",
    "xp = new_coords[0]\n",
    "yp = new_coords[1]\n",
    "zp = new_coords[2]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x,y,z, 'blue',marker=\"o\")\n",
    "ax.plot3D(xp, yp, zp, 'red')\n",
    "plt.tick_params(bottom=False, top=False, labelbottom=False)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_zticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99814eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymol import cmd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil  # For copying non-outlier PDB files\n",
    "\n",
    "###############################################################################\n",
    "# Function to print details of a selection\n",
    "###############################################################################\n",
    "def print_selection_details(selection_name):\n",
    "    print(f\"Details for selection: {selection_name}\")\n",
    "    cmd.iterate(selection_name, \"print(resi, resn, name)\")\n",
    "    count = cmd.count_atoms(selection_name)\n",
    "    print(f\"Number of atoms in {selection_name}: {count}\")\n",
    "\n",
    "###############################################################################\n",
    "# Function to process a single directory of fitted PDBs\n",
    "###############################################################################\n",
    "def process_directory(fitted_pdb_dir, directory_label):\n",
    "    \"\"\"\n",
    "    Processes all PDB files in 'fitted_pdb_dir', measuring:\n",
    "      - Euclidean distance (single-atom) for first and last CA vs. reference\n",
    "      - RMSD (3 CA) for first three and last three CA vs. reference\n",
    "    \n",
    "    If 'directory_label' is \"Ends Realignment\" or \"Secondary structure Realignment\", then:\n",
    "      - Create a new directory with all\n",
    "        structures from 'fitted_pdb_dir' except those that have distance > 3 \n",
    "        for either the first or the last atom. The directory name will be:\n",
    "          \"mustangs_endsRealignment_noOutliers/\" for \"Ends Realignment\"\n",
    "          \"mustangs_conserved_secondary_noOutliers/\" for \"Secondary structure Realignment\"\n",
    "      - Mark y=3  on the single-atom violin plot with a red dotted line labeled \"outlier cutoff\".\n",
    "    \"\"\"\n",
    "    # Re-initialize PyMOL to clear out any previous state\n",
    "    cmd.reinitialize()\n",
    "\n",
    "    # Load reference structure (change path/name if needed)\n",
    "    reference_pdb = \"6UAN_chainD.pdb\"\n",
    "    cmd.load(reference_pdb, \"6UAN_chainD\")\n",
    "\n",
    "    # Create selections for the reference structure\n",
    "    cmd.select(\"6UAN_chainD_first_atom\", \"6UAN_chainD and resi 594 and name CA\")\n",
    "    cmd.select(\"6UAN_chainD_last_atom\",  \"6UAN_chainD and resi 623 and name CA\")\n",
    "    print_selection_details(\"6UAN_chainD_first_atom\")\n",
    "    print_selection_details(\"6UAN_chainD_last_atom\")\n",
    "\n",
    "    cmd.select(\"6UAN_chainD_first3\", \"6UAN_chainD and resi 594-596 and name CA\")\n",
    "    cmd.select(\"6UAN_chainD_last3\",  \"6UAN_chainD and resi 621-623 and name CA\")\n",
    "    print_selection_details(\"6UAN_chainD_first3\")\n",
    "    print_selection_details(\"6UAN_chainD_last3\")\n",
    "\n",
    "    # Prepare lists to store distances/RMSDs\n",
    "    euclid_first_atom = []   # single CA (treated as Euclidean distance)\n",
    "    euclid_last_atom  = []   # single CA (treated as Euclidean distance)\n",
    "    rmsd_first3_atoms = []   # 3 CA\n",
    "    rmsd_last3_atoms  = []   # 3 CA\n",
    "\n",
    "    # Gather PDB files in specified fitted directory\n",
    "    fitted_pdb_files = glob(os.path.join(fitted_pdb_dir, \"*.pdb\"))\n",
    "    total_structures = len(fitted_pdb_files)\n",
    "    print(f\"\\nProcessing directory ({directory_label}): {fitted_pdb_dir}\")\n",
    "    print(f\"Found {total_structures} PDB files.\\n\")\n",
    "\n",
    "    # Keep track of structures with large distances on first/last CA (> 2.5 , as originally coded)\n",
    "    structures_with_large_euclid_first = []\n",
    "    structures_with_large_euclid_last  = []\n",
    "\n",
    "    # Loop over each fitted PDB\n",
    "    for fitted_pdb_file in fitted_pdb_files:\n",
    "        print(f\"Processing file: {fitted_pdb_file}\")\n",
    "        pdb_code = os.path.basename(fitted_pdb_file).split('.')[0]\n",
    "\n",
    "        # Load the fitted structure\n",
    "        cmd.load(fitted_pdb_file, pdb_code)\n",
    "\n",
    "        # Selections for single-atom\n",
    "        cmd.select(f\"{pdb_code}_first_atom\", f\"{pdb_code} and resi 593 and name CA\")\n",
    "        cmd.select(f\"{pdb_code}_last_atom\",  f\"{pdb_code} and resi 619 and name CA\")\n",
    "\n",
    "        # Selections for 3-atom group\n",
    "        cmd.select(f\"{pdb_code}_first3_atoms\", f\"{pdb_code} and resi 593-595 and name CA\")\n",
    "        cmd.select(f\"{pdb_code}_last3_atoms\",  f\"{pdb_code} and resi 617-619 and name CA\")\n",
    "\n",
    "        # Print selection details\n",
    "        print_selection_details(f\"{pdb_code}_first_atom\")\n",
    "        print_selection_details(f\"{pdb_code}_last_atom\")\n",
    "        print_selection_details(f\"{pdb_code}_first3_atoms\")\n",
    "        print_selection_details(f\"{pdb_code}_last3_atoms\")\n",
    "\n",
    "        # Calculate Euclidean distance (single-atom RMSD from PyMOL is effectively the direct distance)\n",
    "        # 1) First atom\n",
    "        try:\n",
    "            d_first = cmd.rms_cur(f\"{pdb_code}_first_atom\", \"6UAN_chainD_first_atom\", matchmaker=-1)\n",
    "            print(f\"Euclidean distance (first atom) for {pdb_code} vs. reference: {d_first:.2f} \")\n",
    "            euclid_first_atom.append(d_first)\n",
    "            if d_first > 2.5:\n",
    "                structures_with_large_euclid_first.append(pdb_code)\n",
    "        except:\n",
    "            print(f\"Error calculating distance for {pdb_code}_first_atom\")\n",
    "            euclid_first_atom.append(np.nan)\n",
    "\n",
    "        # 2) Last atom\n",
    "        try:\n",
    "            d_last = cmd.rms_cur(f\"{pdb_code}_last_atom\", \"6UAN_chainD_last_atom\", matchmaker=-1)\n",
    "            print(f\"Euclidean distance (last atom)  for {pdb_code} vs. reference: {d_last:.2f} \")\n",
    "            euclid_last_atom.append(d_last)\n",
    "            if d_last > 2.5:\n",
    "                structures_with_large_euclid_last.append(pdb_code)\n",
    "        except:\n",
    "            print(f\"Error calculating distance for {pdb_code}_last_atom\")\n",
    "            euclid_last_atom.append(np.nan)\n",
    "\n",
    "        # Calculate RMSD for 3 atoms\n",
    "        # 3) First three atoms\n",
    "        try:\n",
    "            rms_first3 = cmd.rms_cur(f\"{pdb_code}_first3_atoms\", \"6UAN_chainD_first3\", matchmaker=-1)\n",
    "            print(f\"RMSD (first 3 atoms) for {pdb_code} vs. reference: {rms_first3:.2f} \")\n",
    "            rmsd_first3_atoms.append(rms_first3)\n",
    "        except:\n",
    "            print(f\"Error calculating RMSD for {pdb_code}_first3_atoms\")\n",
    "            rmsd_first3_atoms.append(np.nan)\n",
    "\n",
    "        # 4) Last three atoms\n",
    "        try:\n",
    "            rms_last3 = cmd.rms_cur(f\"{pdb_code}_last3_atoms\", \"6UAN_chainD_last3\", matchmaker=-1)\n",
    "            print(f\"RMSD (last 3 atoms)  for {pdb_code} vs. reference: {rms_last3:.2f} \")\n",
    "            rmsd_last3_atoms.append(rms_last3)\n",
    "        except:\n",
    "            print(f\"Error calculating RMSD for {pdb_code}_last3_atoms\")\n",
    "            rmsd_last3_atoms.append(np.nan)\n",
    "\n",
    "        \n",
    "    # Summaries for large distances from single-atom comparisons\n",
    "    print(\"\\nStructures with Euclidean distance > 2.5  (first atom):\")\n",
    "    print(structures_with_large_euclid_first)\n",
    "    print(\"Structures with Euclidean distance > 2.5  (last atom):\")\n",
    "    print(structures_with_large_euclid_last)\n",
    "\n",
    "    # Percentages\n",
    "    num_large_first = len(structures_with_large_euclid_first)\n",
    "    num_large_last  = len(structures_with_large_euclid_last)\n",
    "    pct_large_first = (num_large_first / total_structures) * 100 if total_structures > 0 else 0.0\n",
    "    pct_large_last  = (num_large_last / total_structures) * 100 if total_structures > 0 else 0.0\n",
    "    print(f\"\\nNumber of structures > 2.5  (first atom): {num_large_first}\")\n",
    "    print(f\"Percentage: {pct_large_first:.2f}%\")\n",
    "    print(f\"Number of structures > 2.5  (last atom): {num_large_last}\")\n",
    "    print(f\"Percentage: {pct_large_last:.2f}%\")\n",
    "\n",
    "    ############################################################################\n",
    "    # Identify outliers for the 3  cutoff and copy non-outliers if needed\n",
    "    ############################################################################\n",
    "    # We'll mark any structure outlier if first_atom > 3.0 OR last_atom > 3.0.\n",
    "    outliers_3A = []\n",
    "    for i, pdb_file in enumerate(fitted_pdb_files):\n",
    "        if euclid_first_atom[i] > 3.0 or euclid_last_atom[i] > 3.0:\n",
    "            outliers_3A.append(pdb_file)\n",
    "    \n",
    "    # Make a new directory for non-outliers if the directory label is either \"Ends Realignment\"\n",
    "    # or \"Secondary structure Realignment\"\n",
    "    if directory_label == \"Ends Realignment\":\n",
    "        no_outliers_dir = \"Results/fitted_matlab_segments/mustangs_endsRealignment_noOutliers/\"\n",
    "        os.makedirs(no_outliers_dir, exist_ok=True)\n",
    "        # Copy all pdb files that are NOT outliers\n",
    "        for i, pdb_file in enumerate(fitted_pdb_files):\n",
    "            if pdb_file not in outliers_3A:\n",
    "                shutil.copy2(pdb_file, no_outliers_dir)\n",
    "    elif directory_label == \"Secondary structure Realignment\":\n",
    "        no_outliers_dir = \"Results/fitted_matlab_segments/mustangs_conserved_secondary_noOutliers/\"\n",
    "        os.makedirs(no_outliers_dir, exist_ok=True)\n",
    "        # Copy all pdb files that are NOT outliers\n",
    "        for i, pdb_file in enumerate(fitted_pdb_files):\n",
    "            if pdb_file not in outliers_3A:\n",
    "                shutil.copy2(pdb_file, no_outliers_dir)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot 1: Single-atom as Euclidean distance (violin plot)\n",
    "    ###########################################################################\n",
    "    single_atom_data = [euclid_first_atom, euclid_last_atom]\n",
    "    single_atom_labels = [\"First Atom\", \"Last Atom\"]\n",
    "\n",
    "    fig_euclid, ax_euclid = plt.subplots()\n",
    "    ax_euclid.violinplot(single_atom_data)\n",
    "    ax_euclid.set_xticks(np.arange(1, len(single_atom_labels) + 1))\n",
    "    ax_euclid.set_xticklabels(single_atom_labels)\n",
    "    ax_euclid.set_ylabel(\"Euclidean Distance ()\")\n",
    "    ax_euclid.set_title(f\"Single-Atom Comparison\\n{directory_label}\")\n",
    "    ax_euclid.set_ylim(0, 5)  # Focus on 05 \n",
    "\n",
    "    # If Ends Realignment or Secondary structure Realignment, add a dotted red line at 3  labeled \"outlier cutoff\"\n",
    "    if directory_label == \"Ends Realignment\" or directory_label == \"Secondary structure Realignment\":\n",
    "        ax_euclid.axhline(y=3.0, color='red', linestyle=':', label='Outlier cutoff')\n",
    "        ax_euclid.legend()\n",
    "\n",
    "    # Save the figure in the same directory\n",
    "    euclid_plot_path = os.path.join(fitted_pdb_dir, \"single_atom_violin.png\")\n",
    "    fig_euclid.savefig(euclid_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_euclid)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot 2: Three-atom RMSD (violin plot)\n",
    "    ###########################################################################\n",
    "    triple_atom_data = [rmsd_first3_atoms, rmsd_last3_atoms]\n",
    "    triple_atom_labels = [\"First Three Atoms\", \"Last Three Atoms\"]\n",
    "\n",
    "    fig_rmsd, ax_rmsd = plt.subplots()\n",
    "    ax_rmsd.violinplot(triple_atom_data)\n",
    "    ax_rmsd.set_xticks(np.arange(1, len(triple_atom_labels) + 1))\n",
    "    ax_rmsd.set_xticklabels(triple_atom_labels)\n",
    "    ax_rmsd.set_ylabel(\"RMSD ()\")\n",
    "    ax_rmsd.set_title(f\"Three-Atom Comparison\\n{directory_label}\")\n",
    "    ax_rmsd.set_ylim(0, 5)\n",
    "\n",
    "    # Save the figure in the same directory\n",
    "    rmsd_plot_path = os.path.join(fitted_pdb_dir, \"three_atom_violin.png\")\n",
    "    fig_rmsd.savefig(rmsd_plot_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig_rmsd)\n",
    "\n",
    "    \n",
    "\n",
    "###############################################################################\n",
    "# Main script - process directories\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    cases = [\n",
    "        (\"Results/fitted_matlab_segments/mustangs_endsRealignment/\", \"Ends Realignment\"),\n",
    "        (\"Results/fitted_matlab_segments/mustangs_noRealignment/\",  \"No Realignment\"),\n",
    "        (\"Results/fitted_matlab_segments/mustangs_conserved_secondary/\", \"Secondary structure Realignment\")\n",
    "    ]\n",
    "\n",
    "    # Process each directory\n",
    "    for dpath, label in cases:\n",
    "        process_directory(dpath, label)\n",
    "\n",
    "    cmd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifnotmake(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from numpy.linalg import norm\n",
    "import shutil\n",
    "\n",
    "###############################################################################\n",
    "# Helper functions\n",
    "###############################################################################\n",
    "def pdb_id(fp):\n",
    "    \"\"\"\n",
    "    Extract just the base name (no extension, no path) from a filepath.\n",
    "    Example:\n",
    "        /path/to/file/abcd.pdb --> abcd\n",
    "    \"\"\"\n",
    "    return os.path.splitext(os.path.basename(fp))[0]\n",
    "\n",
    "def braf_res(folder):\n",
    "    \"\"\"\n",
    "    Return a sorted list of unique residue names in all PDB files of a folder,\n",
    "    excluding any whose name contains 'combined'.\n",
    "    \"\"\"\n",
    "    pdb_files = glob(os.path.join(folder, \"*.pdb\"))\n",
    "    # Skip files that contain 'combined' in their basename\n",
    "    filtered_pdb_files = [fp for fp in pdb_files if \"combined\" not in os.path.basename(fp)]\n",
    "    if not filtered_pdb_files:\n",
    "        raise IOError(f\"No .pdb files found in '{folder}' (after excluding 'combined' files).\")\n",
    "    \n",
    "    all_residues = set()\n",
    "    for fp in filtered_pdb_files:\n",
    "        traj = md.load(fp)\n",
    "        top = traj.topology\n",
    "        for res in top.residues:\n",
    "            all_residues.add(res.name)\n",
    "    return sorted(list(all_residues))\n",
    "\n",
    "def PCA_on_files(file_list, n_components=4):\n",
    "    \"\"\"\n",
    "    Perform PCA on a list of PDB files and return (pca_model, projections).\n",
    "\n",
    "    file_list : list[str]\n",
    "        Paths to PDB files. We assume each is either single-frame or you want all frames.\n",
    "    \"\"\"\n",
    "    if len(file_list) == 0:\n",
    "        raise ValueError(\"No files provided for PCA (check filtering).\")\n",
    "    print(\"Number of PDB files to load:\", len(file_list))\n",
    "    for f in file_list:\n",
    "        print(\"  \", os.path.basename(f))\n",
    "\n",
    "    # Load all frames from the list of files\n",
    "    traj = md.join([md.load(f) for f in file_list])\n",
    "\n",
    "    # Reshape to (#frames, 3 * #atoms)\n",
    "    xyz = traj.xyz.reshape(-1, 3 * traj.n_atoms)\n",
    "\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    proj = pca.fit_transform(xyz)\n",
    "    return pca, proj\n",
    "\n",
    "def plot_scores(cur_pca, n_comps=4, folder_for_residues=None, figure_title=\"PCA scores\"):\n",
    "    \"\"\"\n",
    "    Plots the PCA 'scores' (i.e., loadings per residue) for up to n_comps components.\n",
    "    Adjust '27' in the reshape if needed for the number of atoms/residues in your system.\n",
    "    \"\"\"\n",
    "    # Reshape the PCA components to [n_atoms, 3], repeated for each principal component.\n",
    "    # Adjust 27 if needed for your system.\n",
    "    scores = cur_pca.components_.reshape(-1, 27, 3)[:n_comps]\n",
    "\n",
    "    x_labels = braf_res(folder_for_residues)\n",
    "    x_vals = list(range(len(scores[0])))\n",
    "    col = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "    exp_var = cur_pca.explained_variance_\n",
    "\n",
    "    fig, axes = plt.subplots(n_comps, sharex=True, figsize=(8, 8))\n",
    "    if n_comps == 1:\n",
    "        axes = [axes]  # Ensure axes is always iterable if only 1 PC.\n",
    "\n",
    "    for i in range(n_comps):\n",
    "        # Calculate norm of loadings * explained variance\n",
    "        load_val = norm(scores[i] * exp_var[i], axis=1)\n",
    "        axes[i].plot(x_vals, load_val, marker=\".\", c=col[i % len(col)])\n",
    "        axes[i].set_ylabel(f\"PC{i+1} load\")\n",
    "\n",
    "    axes[-1].set_xticks(range(len(x_labels)))\n",
    "    axes[-1].set_xticklabels(x_labels, rotation=90)\n",
    "    axes[0].set_title(figure_title)\n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "def cluster_and_plot(\n",
    "    all_trans,\n",
    "    n_clusters=2,\n",
    "    highlight_indices=None,\n",
    "    highlight_color='red',\n",
    "    main_title=\"Hierarchical Clustering in PC1PC2\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on the first two PCs (all_trans[:, :2]).\n",
    "    highlight_indices : which points to circle in the plot (e.g. outliers).\n",
    "    return_labels : if True, returns the cluster label array.\n",
    "    \"\"\"\n",
    "    # Fit hierarchical clustering on the first two PCs\n",
    "    hier_clust = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    print('ciao')\n",
    "    print(np.shape(all_trans[:, :2]))\n",
    "    cluster_labels = hier_clust.fit_predict(all_trans[:, :2])\n",
    "\n",
    "    # Prepare discrete color boundaries\n",
    "    bounds = list(range(n_clusters + 1))\n",
    "    norm_bound = BoundaryNorm(bounds, ncolors=n_clusters, clip=True)\n",
    "\n",
    "    # Plot PC1 vs. PC2, colored by cluster label\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    scatter = ax.scatter(\n",
    "        all_trans[:, 0],\n",
    "        all_trans[:, 1],\n",
    "        c=cluster_labels,\n",
    "        cmap='tab10',\n",
    "        norm=norm_bound,\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    # Highlight the outliers by circling them\n",
    "    if highlight_indices is not None and len(highlight_indices) > 0:\n",
    "        ax.scatter(\n",
    "            all_trans[highlight_indices, 0],\n",
    "            all_trans[highlight_indices, 1],\n",
    "            facecolors='none',\n",
    "            edgecolors=highlight_color,\n",
    "            s=100,\n",
    "            linewidths=1.5,\n",
    "            label=\"Excluded in noOutliers\"\n",
    "        )\n",
    "\n",
    "    # Create a discrete colorbar\n",
    "    tick_positions = [x + 0.5 for x in range(n_clusters)]\n",
    "    cbar = plt.colorbar(scatter, spacing=\"proportional\", ticks=tick_positions)\n",
    "    cbar.ax.set_yticklabels([f\"Cluster {i}\" for i in range(n_clusters)])\n",
    "    cbar.set_label(\"Cluster ID\")\n",
    "\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_title(main_title)\n",
    "    if highlight_indices:\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax, cluster_labels\n",
    "\n",
    "###############################################################################\n",
    "# Core function to run the PCA+clustering logic for a single alignment type\n",
    "###############################################################################\n",
    "def run_pca_workflow(\n",
    "    alignment_label,\n",
    "    FITTED_PDB_PATH_ALL,\n",
    "    FITTED_PDB_PATH_NOOUT,\n",
    "    OUT_CLUST0,\n",
    "    OUT_CLUST1,\n",
    "    figure_prefix\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform PCA + outlier detection + clustering for a given realignment folder set.\n",
    "    'combined' files are excluded from the entire pipeline.\n",
    "    \"\"\"\n",
    "    # --------------------------------------------------------------------\n",
    "    # 1) Gather .pdb files, EXCLUDING any with 'combined' in their name\n",
    "    # --------------------------------------------------------------------\n",
    "    all_pdbs = sorted(\n",
    "        fp for fp in glob(os.path.join(FITTED_PDB_PATH_ALL, \"*.pdb\"))\n",
    "        if \"combined\" not in os.path.basename(fp)\n",
    "    )\n",
    "    noout_pdbs = sorted(\n",
    "        fp for fp in glob(os.path.join(FITTED_PDB_PATH_NOOUT, \"*.pdb\"))\n",
    "        if \"combined\" not in os.path.basename(fp)\n",
    "    )\n",
    "\n",
    "    names_all = [pdb_id(fp) for fp in all_pdbs]\n",
    "    names_no_outliers = [pdb_id(fp) for fp in noout_pdbs]\n",
    "\n",
    "    print(f\"\\n======= {alignment_label.upper()} REALIGNMENT =======\")\n",
    "    print(\"All structures:\", names_all)\n",
    "    print(\"No-outlier structures:\", names_no_outliers)\n",
    "\n",
    "    # 2) Identify outliers\n",
    "    excluded_names = set(names_all) - set(names_no_outliers)\n",
    "    excluded_indices = [i for i, nm in enumerate(names_all) if nm in excluded_names]\n",
    "    print(\"Excluded structures (OUTLIERS):\", excluded_names)\n",
    "\n",
    "    # 3) PCA on the FULL set\n",
    "    if len(all_pdbs) == 0:\n",
    "        print(f\"WARNING: No PDBs found for {alignment_label} (All) after excluding 'combined'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    all_pca, all_trans = PCA_on_files(all_pdbs, n_components=4)\n",
    "\n",
    "    # Plot PCA loadings across residues\n",
    "    all_fig, all_axes = plot_scores(\n",
    "        all_pca,\n",
    "        n_comps=4,\n",
    "        folder_for_residues=FITTED_PDB_PATH_ALL,\n",
    "        figure_title=f\"PCA scores - {alignment_label} (ALL)\"\n",
    "    )\n",
    "    savefile_all_scores = f\"scores_all_{figure_prefix}.png\"\n",
    "    plt.savefig(savefile_all_scores, bbox_inches='tight', dpi=300)\n",
    "    plt.close(all_fig)\n",
    "\n",
    "    # Print explained variance ratios\n",
    "    explained_var_ratios_all = all_pca.explained_variance_ratio_ * 100\n",
    "    for i, ratio in enumerate(explained_var_ratios_all, 1):\n",
    "        print(f\"PC{i} (All, {alignment_label}) explains {ratio:.2f}% of variance.\")\n",
    "\n",
    "    fig_all, ax_all, cluster_labels_all = cluster_and_plot(\n",
    "        all_trans,\n",
    "        n_clusters=2,\n",
    "        highlight_indices=excluded_indices,\n",
    "        highlight_color='red',\n",
    "        main_title=f\"PC1 vs PC2 - All Data ({alignment_label}) [Red = Outliers]\"\n",
    "    )\n",
    "    cluster_out_all = f\"cluster_all_{figure_prefix}_with_outliers_highlight.png\"\n",
    "    fig_all.savefig(cluster_out_all, dpi=300)\n",
    "    plt.close(fig_all)\n",
    "\n",
    "    # 4) PCA on the NO_OUTLIERS set\n",
    "    if len(noout_pdbs) == 0:\n",
    "        print(f\"WARNING: No PDBs found for {alignment_label} (NoOutliers) after excluding 'combined'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    noout_pca, noout_trans = PCA_on_files(noout_pdbs, n_components=4)\n",
    "\n",
    "    # Plot PCA loadings across residues\n",
    "    noout_fig, noout_axes = plot_scores(\n",
    "        noout_pca,\n",
    "        n_comps=4,\n",
    "        folder_for_residues=FITTED_PDB_PATH_NOOUT,\n",
    "        figure_title=f\"PCA scores - {alignment_label} (NoOutliers)\"\n",
    "    )\n",
    "    savefile_noout_scores = f\"scores_noOutliers_{figure_prefix}.png\"\n",
    "    plt.savefig(savefile_noout_scores, bbox_inches='tight', dpi=300)\n",
    "    plt.close(noout_fig)\n",
    "\n",
    "    explained_var_ratios_noout = noout_pca.explained_variance_ratio_ * 100\n",
    "    for i, ratio in enumerate(explained_var_ratios_noout, 1):\n",
    "        print(f\"PC{i} (NoOutliers, {alignment_label}) explains {ratio:.2f}% of variance.\")\n",
    "\n",
    "    # Cluster the NO-OUTLIERS set\n",
    "    fig_noout, ax_noout, cluster_labels_noout = cluster_and_plot(\n",
    "        noout_trans,\n",
    "        n_clusters=2,\n",
    "        main_title=f\"PC1 vs PC2 - No-Outlier Data ({alignment_label})\"\n",
    "    )\n",
    "    cluster_out_noout = f\"cluster_noOutliers_{figure_prefix}.png\"\n",
    "    fig_noout.savefig(cluster_out_noout, dpi=300)\n",
    "    plt.close(fig_noout)\n",
    "\n",
    "    # 5) Copy structures to cluster-based directories\n",
    "    os.makedirs(OUT_CLUST0, exist_ok=True)\n",
    "    os.makedirs(OUT_CLUST1, exist_ok=True)\n",
    "\n",
    "    # Because we loaded 1 row per file, cluster_labels_noout should match noout_pdbs\n",
    "    for i, label in enumerate(cluster_labels_noout):\n",
    "        src_file = noout_pdbs[i]\n",
    "        # No check for 'combined' needed because we never included them in noout_pdbs\n",
    "        if label == 0:\n",
    "            shutil.copy(src_file, OUT_CLUST0)\n",
    "        else:\n",
    "            shutil.copy(src_file, OUT_CLUST1)\n",
    "\n",
    "    print(\"Done! Structures have been copied to:\")\n",
    "    print(f\"  Cluster 0  {OUT_CLUST0}\")\n",
    "    print(f\"  Cluster 1  {OUT_CLUST1}\")\n",
    "\n",
    "###############################################################################\n",
    "# Main script - does BOTH Ends Realignment and Secondary Realignment\n",
    "###############################################################################\n",
    "def main():\n",
    "    # --- Ends Realignment ---\n",
    "    run_pca_workflow(\n",
    "        alignment_label=\"Ends\",\n",
    "        FITTED_PDB_PATH_ALL=\"Results/fitted_matlab_segments/mustangs_endsRealignment\",\n",
    "        FITTED_PDB_PATH_NOOUT=\"Results/fitted_matlab_segments/mustangs_endsRealignment_noOutliers\",\n",
    "        OUT_CLUST0=\"Results/fitted_matlab_segments/mustangs_ends_noOutlier_pca_clust0\",\n",
    "        OUT_CLUST1=\"Results/fitted_matlab_segments/mustangs_ends_noOutlier_pca_clust1\",\n",
    "        figure_prefix=\"ends\"\n",
    "    )\n",
    "\n",
    "    # --- Secondary Realignment ---\n",
    "    run_pca_workflow(\n",
    "        alignment_label=\"Secondary\",\n",
    "        FITTED_PDB_PATH_ALL=\"Results/fitted_matlab_segments/mustangs_conserved_secondary\",\n",
    "        FITTED_PDB_PATH_NOOUT=\"Results/fitted_matlab_segments/mustangs_conserved_secondary_noOutliers\",\n",
    "        OUT_CLUST0=\"Results/fitted_matlab_segments/mustangs_secondary_noOutlier_pca_clust0\",\n",
    "        OUT_CLUST1=\"Results/fitted_matlab_segments/mustangs_secondary_noOutlier_pca_clust1\",\n",
    "        figure_prefix=\"secondary\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04171a73",
   "metadata": {},
   "source": [
    "Molearn step for ends realignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.path.abspath(os.pardir),'src'))\n",
    "from molearn.data import PDBData\n",
    "from molearn.trainers import Trainer\n",
    "from molearn.models.small_foldingnet import Small_AutoEncoder\n",
    "import torch\n",
    "import MDAnalysis as mda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data = PDBData()\n",
    "    folder_name = 'Results/fitted_matlab_segments/mustangs_endsRealignment_noOutliers'\n",
    "\n",
    "    # Get a sorted list of all files in the directory, EXCLUDING combined.pdb\n",
    "    files = sorted([\n",
    "        f for f in os.listdir(folder_name) \n",
    "        if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "    ])\n",
    "\n",
    "    combined_file_path = os.path.join(folder_name, 'combined.pdb')\n",
    "\n",
    "    # Open in write mode to overwrite or create a fresh combined.pdb\n",
    "    with open(combined_file_path, 'w') as combined_file:\n",
    "        for i, filename in enumerate(files):\n",
    "            file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "            # Read content while filtering out lines starting with \"MODEL\" or \"END\"\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                lines = [line for line in lines if not line.startswith((\"MODEL\", \"END\"))]\n",
    "\n",
    "            # Write \"MODEL i\", then filtered lines, then \"ENDMDL\"\n",
    "            combined_file.write(f'MODEL {i}\\n')\n",
    "            combined_file.writelines(lines)\n",
    "            combined_file.write('ENDMDL\\n')\n",
    "        \n",
    "        combined_file.write('END\\n')\n",
    "\n",
    "    # Now import combined.pdb without having to delete it each run\n",
    "    data.import_pdb(filename=combined_file_path)\n",
    "    data.fix_terminal()\n",
    "    data.atomselect(atoms=['CA', 'C', 'N', 'CB', 'O'])\n",
    "    data.prepare_dataset()\n",
    "    print(data._mol)\n",
    "\n",
    "    ##### Prepare Trainer #####\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trainer = Trainer(device=device)\n",
    "    trainer.set_data(data, batch_size=8, validation_split=0.1, manual_seed=25)\n",
    "    trainer.set_autoencoder(Small_AutoEncoder, out_points=data.dataset.shape[-1])\n",
    "    trainer.prepare_optimiser()\n",
    "\n",
    "    ##### Training Loop #####\n",
    "    # Keep training until loss does not improve for 32 consecutive epochs\n",
    "    runkwargs = dict(\n",
    "        log_filename='log_file.dat',\n",
    "        log_folder='Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/xbb_foldingnet_checkpoints',\n",
    "        checkpoint_folder='Results/run_trial_BRAFActivationLoop_postalign_checkpoint1',\n",
    "    )\n",
    "    best = 1e24\n",
    "    while True:\n",
    "        trainer.run(max_epochs=32 + trainer.epoch, **runkwargs)\n",
    "        if not best > trainer.best:\n",
    "            break\n",
    "        best = trainer.best\n",
    "    print(f'best {trainer.best}, best_filename {trainer.best_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a881e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import molearn\n",
    "from molearn.analysis.analyser import MolearnAnalysis\n",
    "from molearn.data import PDBData\n",
    "from molearn.models.small_foldingnet import Small_AutoEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import biobox as bb\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "\n",
    "import MDAnalysis as mda\n",
    "file_pattern = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/checkpoint*.ckpt'\n",
    "\n",
    "print(glob.glob(file_pattern))\n",
    "# Get a sorted list of all matching checkpoint files\n",
    "matching_files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "# Check if we found any matches\n",
    "if len(matching_files) == 0:\n",
    "    raise FileNotFoundError(f\"No files matched the pattern: {file_pattern}\")\n",
    "\n",
    "# Example 1: Grab the first matching file\n",
    "networkfile = matching_files[0]\n",
    "\n",
    "# Example 2 (Alternative): Grab the last matching file (e.g., if its the most recent)\n",
    "# networkfile = matching_files[-1]\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(networkfile, map_location=torch.device('cpu'))\n",
    "net = Small_AutoEncoder(**checkpoint['network_kwargs'])\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Print some info\n",
    "print(\"Matched files:\", matching_files)\n",
    "print(\"Using file:\", networkfile)\n",
    "print(\"Network kwargs:\", checkpoint['network_kwargs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data = PDBData()\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_endsRealignment_noOutliers'\n",
    "combined_file_path = os.path.join(folder_name, 'combined.pdb')\n",
    "data.import_pdb(filename=combined_file_path)\n",
    "#data.import_pdb(f'data{os.sep}MurD_open_selection.pdb')\n",
    "data.fix_terminal()\n",
    "data.atomselect(atoms = ['CA', 'C', 'N', 'CB', 'O'])\n",
    "data.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb27973",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA = MolearnAnalysis()\n",
    "MA.set_network(net)\n",
    "data_train, data_valid = data.split(manual_seed=25)\n",
    "MA.set_dataset(\"training\", data_train)\n",
    "MA.set_dataset(\"validation\", data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = data.get_datasets(manual_seed=25)\n",
    "\n",
    "indices = data.indices.numpy()\n",
    "\n",
    "\n",
    "train_indices = indices[:np.shape(data_train)[0]]\n",
    "valid_indices = indices[np.shape(data_train)[0]:]\n",
    "\n",
    "print(sorted(np.append(train_indices, valid_indices)))\n",
    "\n",
    "ifnotmake('Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial/')\n",
    "for i, index in enumerate(train_indices):\n",
    "\n",
    "    data._mol.trajectory[index]  # Set the trajectory to the specific frame\n",
    "\n",
    "    data._mol.select_atoms(\"name CA\").write(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial/s{i}.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c04c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/decoded_train'\n",
    "\n",
    "ifnotmake(directory_name)\n",
    "# Use generate method which is public (no underscore)\n",
    "\n",
    "latent_coords = MA.get_encoded('training')  # Example latent space coordinates\n",
    "\n",
    "print(latent_coords)\n",
    "\n",
    "crd_ref = MA.generate(latent_coords.numpy().reshape(1, len(latent_coords), 2), directory_name, relax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/decoded_valid'\n",
    "\n",
    "ifnotmake(directory_name)\n",
    "# Use generate method which is public (no underscore)\n",
    "\n",
    "latent_coords = MA.get_encoded('validation')  # Example latent space coordinates\n",
    "\n",
    "print(latent_coords)\n",
    "\n",
    "crd_ref = MA.generate(latent_coords.numpy().reshape(1, len(latent_coords), 2), directory_name, relax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dd4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "import MDAnalysis.analysis.rms as rms\n",
    "\n",
    "# Directories\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial/'\n",
    "decoded_train_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/decoded_train'\n",
    "\n",
    "# List all files in the directories\n",
    "getDatasetTrial_files = os.listdir(getDatasetTrial_dir)\n",
    "decoded_train_files = os.listdir(decoded_train_dir)\n",
    "\n",
    "# Filter files that start with 's' in decoded_train\n",
    "decoded_train_files = [f for f in decoded_train_files if f.startswith('s')]\n",
    "\n",
    "# Function to extract the numerical part from a file name\n",
    "def extract_number(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    return int(match.group()) if match else float('inf')\n",
    "\n",
    "# Sort the file names based on the numerical part\n",
    "getDatasetTrial_files.sort(key=extract_number)\n",
    "decoded_train_files.sort(key=extract_number)\n",
    "\n",
    "# Calculate RMSD for files with the same name\n",
    "rmsd_values = {}\n",
    "\n",
    "for file_name in getDatasetTrial_files:\n",
    "    if file_name in decoded_train_files:\n",
    "        # Load the universes\n",
    "        u1 = mda.Universe(os.path.join(getDatasetTrial_dir, file_name))\n",
    "        u2 = mda.Universe(os.path.join(decoded_train_dir, file_name))\n",
    "        \n",
    "        # Select CA atoms\n",
    "        ag1 = u1.select_atoms(\"name CA\")\n",
    "        ag2 = u2.select_atoms(\"name CA\")\n",
    "        \n",
    "        # Calculate RMSD\n",
    "        rmsd_value = rms.rmsd(ag1.positions, ag2.positions)\n",
    "        rmsd_values[file_name] = rmsd_value\n",
    "\n",
    "# Print RMSD values in sorted order\n",
    "for file_name in sorted(rmsd_values.keys(), key=extract_number):\n",
    "    print(f\"RMSD for {file_name}: {rmsd_values[file_name]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b8364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(MA.get_error('training', align=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a4750",
   "metadata": {},
   "source": [
    "Renaming input and decoded datasets with PDB IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b089b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1) Gather the original files again (same sorting you used earlier):\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_endsRealignment_noOutliers'\n",
    "files = sorted([\n",
    "    f for f in os.listdir(folder_name)\n",
    "    if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "])\n",
    "\n",
    "# 2) Define the directories that have \"s{i}.pdb\"\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial'\n",
    "decoded_train_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/decoded_train'\n",
    "\n",
    "mapping_filepath = 'train_index_mapping.csv'\n",
    "with open(mapping_filepath, 'w') as mapping_file:\n",
    "    # Write a header (optional)\n",
    "    mapping_file.write(\"loop_index,train_index,pdb_filename\\n\")\n",
    "\n",
    "    # Suppose you already have train_indices from your dataset split\n",
    "    # We'll loop over each entry in train_indices:\n",
    "    for i, index in enumerate(train_indices):\n",
    "        original_filename = files[index]  # the PDB name from the original folder\n",
    "\n",
    "        # Write this mapping to file:\n",
    "        mapping_file.write(f\"{i},{index},{original_filename}\\n\")\n",
    "\n",
    "        # 3) Rename the file in getDatasetTrial:\n",
    "        old_file_getDataset = os.path.join(getDatasetTrial_dir, f's{i}.pdb')\n",
    "        new_file_getDataset = os.path.join(getDatasetTrial_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_getDataset):\n",
    "            os.rename(old_file_getDataset, new_file_getDataset)\n",
    "            print(f'Renamed: {old_file_getDataset} -> {new_file_getDataset}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_getDataset}')\n",
    "\n",
    "        # 4) Rename the file in decoded_train:\n",
    "        old_file_decoded = os.path.join(decoded_train_dir, f's{i}.pdb')\n",
    "        new_file_decoded = os.path.join(decoded_train_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_decoded):\n",
    "            os.rename(old_file_decoded, new_file_decoded)\n",
    "            print(f'Renamed: {old_file_decoded} -> {new_file_decoded}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_decoded}')\n",
    "\n",
    "print(f\"\\nMapping file saved at: {mapping_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1) Gather the original files again (same sorting you used earlier):\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_endsRealignment_noOutliers'\n",
    "files = sorted([\n",
    "    f for f in os.listdir(folder_name)\n",
    "    if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "])\n",
    "\n",
    "# 2) Define the directories that have \"s{i}.pdb\"\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial'\n",
    "decoded_valid_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/decoded_valid'\n",
    "\n",
    "\n",
    "mapping_filepath = 'valid_index_mapping.csv'\n",
    "with open(mapping_filepath, 'w') as mapping_file:\n",
    "    # Write a header (optional)\n",
    "    mapping_file.write(\"loop_index,valid_index,pdb_filename\\n\")\n",
    "    print(len(files))\n",
    "    # Suppose you already have valid_indices from your dataset split\n",
    "    # We'll loop over each entry in valid_indices:\n",
    "    for i, index in enumerate(valid_indices):\n",
    "        print(index)\n",
    "        original_filename = files[index]  # the PDB name from the original folder\n",
    "\n",
    "        # Write this mapping to file:\n",
    "        mapping_file.write(f\"{i},{index},{original_filename}\\n\")\n",
    "\n",
    "        # 3) Rename the file in getDatasetTrial:\n",
    "        old_file_getDataset = os.path.join(getDatasetTrial_dir, f's{i}.pdb')\n",
    "        new_file_getDataset = os.path.join(getDatasetTrial_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_getDataset):\n",
    "            os.rename(old_file_getDataset, new_file_getDataset)\n",
    "            print(f'Renamed: {old_file_getDataset} -> {new_file_getDataset}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_getDataset}')\n",
    "\n",
    "        # 4) Rename the file in decoded_valid:\n",
    "        old_file_decoded = os.path.join(decoded_valid_dir, f's{i}.pdb')\n",
    "        new_file_decoded = os.path.join(decoded_valid_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_decoded):\n",
    "            os.rename(old_file_decoded, new_file_decoded)\n",
    "            print(f'Renamed: {old_file_decoded} -> {new_file_decoded}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_decoded}')\n",
    "\n",
    "print(f\"\\nMapping file saved at: {mapping_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA.batch_size = 8\n",
    "MA.processes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = '_foldingnet_checkpoint'\n",
    "\n",
    "#saveName = directory.replace('_foldingnet_checkpoint', '')\n",
    "err_train = MA.get_error('training')\n",
    "df_err_train = pd.DataFrame(err_train, columns=['err_train'])\n",
    "df_err_train.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_train_{saveName}.csv', index=False)\n",
    "\n",
    "err_test = MA.get_error('validation')\n",
    "df_err_test = pd.DataFrame(err_test, columns=['err_test'])\n",
    "df_err_test.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_valid_{saveName}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62795b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "data = [err_train]\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data)\n",
    "plt.ylabel('RMSD [$\\AA$]')\n",
    "plt.title('Reconstruction error between encoded and decoded training dataset')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_train_{saveName}.png', dpi=300)\n",
    "\n",
    "\n",
    "data = [err_test]\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data)\n",
    "plt.ylabel('RMSD [$\\AA$]')\n",
    "plt.title('Reconstruction error between encoded and decoded validation dataset')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_test_{saveName}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df668aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Define paths, read in the data\n",
    "saveName = '_foldingnet_checkpoint'\n",
    "train_err_csv = f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_train_{saveName}.csv'\n",
    "valid_err_csv = f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_valid_{saveName}.csv'\n",
    "\n",
    "df_err_train = pd.read_csv(train_err_csv)   # columns: ['err_train']\n",
    "df_err_valid = pd.read_csv(valid_err_csv)   # columns: ['err_test']\n",
    "\n",
    "# 2) Identify outliers where RMSD > 4\n",
    "train_outlier_indices = df_err_train.index[df_err_train['err_train'] > 4].to_list()\n",
    "valid_outlier_indices = df_err_valid.index[df_err_valid['err_test'] > 4].to_list()\n",
    "\n",
    "# 3) Gather original filenames and the directories containing your re-labeled PDBs\n",
    "#    - \"files\": the sorted list of PDB filenames from the original folder \n",
    "#               (the same you used to create combined.pdb, excluding 'combined.pdb').\n",
    "\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_endsRealignment_noOutliers'\n",
    "files = sorted([\n",
    "    f for f in os.listdir(folder_name)\n",
    "    if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "])\n",
    "\n",
    "#    - \"train_indices\" (and possibly \"valid_indices\") map each row in df_err_train/df_err_valid \n",
    "#      to the correct frame index. For simple splits, you might only need train_indices \n",
    "#      if you used data.split() from your PDBData object. (Adjust this code if valid set \n",
    "#      also has a separate set of indices or if you used a different approach.)\n",
    "# Suppose something like:\n",
    "# train_indices, valid_indices = data.get_datasets_indices(manual_seed=25)\n",
    "# or similar. Make sure these match the order used in MA.get_error('training') and .get_error('validation').\n",
    "\n",
    "# 4) Create output folders for outliers\n",
    "outlier_dir_train = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/rmsd_outliers_train'\n",
    "outlier_dir_valid = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/rmsd_outliers_valid'\n",
    "os.makedirs(outlier_dir_train, exist_ok=True)\n",
    "os.makedirs(outlier_dir_valid, exist_ok=True)\n",
    "\n",
    "# 5) Copy outlier files for training\n",
    "source_dir_train = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial'\n",
    "for row_i in train_outlier_indices:\n",
    "    frame_idx = train_indices[row_i]          # The actual frame index\n",
    "    original_filename = files[frame_idx]      # The original PDB name\n",
    "\n",
    "    src = os.path.join(source_dir_train, original_filename)\n",
    "    dst = os.path.join(outlier_dir_train, original_filename)\n",
    "\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f'Copied outlier (train): {src} -> {dst}')\n",
    "    else:\n",
    "        print(f'Skipping missing file (train): {src}')\n",
    "\n",
    "# 6) Copy outlier files for validation\n",
    "#    If valid set also uses a \"valid_indices\" array, map row_i -> valid_indices[row_i] -> files[...].\n",
    "#    If you used a single \"indices\" array and just split it, adjust accordingly.\n",
    "source_dir_valid = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial'\n",
    "for row_i in valid_outlier_indices:\n",
    "    frame_idx = valid_indices[row_i]\n",
    "    original_filename = files[frame_idx]\n",
    "\n",
    "    src = os.path.join(source_dir_valid, original_filename)\n",
    "    dst = os.path.join(outlier_dir_valid, original_filename)\n",
    "\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f'Copied outlier (valid): {src} -> {dst}')\n",
    "    else:\n",
    "        print(f'Skipping missing file (valid): {src}')\n",
    "\n",
    "# 7) Plot RMSD for training with a red dotted cutoff line at 4 \n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data=[df_err_train['err_train']])\n",
    "plt.axhline(y=4.0, color='red', linestyle='--', label='Outlier cutoff')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSD [$\\\\AA$]')\n",
    "plt.title('Reconstruction error (training)')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_train_{saveName}.png', dpi=300)\n",
    "\n",
    "# 8) Plot RMSD for validation with a red dotted cutoff line at 4 \n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data=[df_err_valid['err_test']])\n",
    "plt.axhline(y=4.0, color='red', linestyle='--', label='Outlier cutoff')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSD [$\\\\AA$]')\n",
    "plt.title('Reconstruction error (validation)')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/err_test_{saveName}.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA.setup_grid(30)\n",
    "landscape_err_latent, landscape_err_3d, xaxis, yaxis = MA.scan_error()\n",
    "df_landscape_err_latent = pd.DataFrame(landscape_err_latent)\n",
    "df_landscape_err_latent.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_err_latent_{saveName}.csv', index=False)\n",
    "'''\n",
    "df= pd.read_csv(f'landscape_err_latent_{saveName}.csv')\n",
    "print(df.shape)\n",
    "'''\n",
    "\n",
    "df_landscape_err_3d = pd.DataFrame(landscape_err_3d)\n",
    "df_landscape_err_3d.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_err_3d_{saveName}.csv', index=False)\n",
    "\n",
    "df_xaxis = pd.DataFrame(xaxis)\n",
    "df_xaxis.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_err_xaxis_{saveName}.csv', index=False)\n",
    "\n",
    "df_yaxis = pd.DataFrame(yaxis)\n",
    "df_yaxis.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_err_yaxis_{saveName}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90685bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molearn.analysis import MolearnGUI\n",
    "MolearnGUI(MA);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f8c10",
   "metadata": {},
   "source": [
    "Projection into latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = net.encode(data_train.float())\n",
    "\n",
    "z_training = z.data.cpu().numpy()[:, :, 0]\n",
    "df_z_train = pd.DataFrame(z_training)\n",
    "df_z_train.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_encoded_train_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d77e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = net.encode(data_valid.float())\n",
    "z_valid = z.data.cpu().numpy()[:, :, 0]\n",
    "df_z_valid = pd.DataFrame(z_valid)\n",
    "df_z_valid.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_encoded_valid_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d5a53",
   "metadata": {},
   "source": [
    "Do HDBSCAN and show landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f43682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "from numpy.linalg import inv\n",
    "\n",
    "x_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_err_xaxis__foldingnet_checkpoint.csv\")\n",
    "y_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_err_yaxis__foldingnet_checkpoint.csv\")\n",
    "z_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_err_latent__foldingnet_checkpoint.csv\")\n",
    "\n",
    "encoded_train_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_encoded_train_coordinates.csv\")\n",
    "encoded_valid_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/landscape_encoded_valid_coordinates.csv\")\n",
    "df_encoded_train = [pd.read_csv(file, header=None).to_numpy() for file in encoded_train_file]\n",
    "df_encoded_valid = [pd.read_csv(file, header=None).to_numpy() for file in encoded_valid_file]\n",
    "'encdoded coordinates are in the same order as decoded coordinates'\n",
    "encoded_train = [array[:, :] for array in df_encoded_train]\n",
    "encoded_valid = [array[:, :] for array in df_encoded_valid]\n",
    "\n",
    "df_x = [pd.read_csv(file, header=None).to_numpy() for file in x_file]\n",
    "df_y = [pd.read_csv(file, header=None).to_numpy() for file in y_file]\n",
    "df_z = [pd.read_csv(file).to_numpy() for file in z_file]\n",
    "# Need fliplr because heatmap displays data differently from how array stores it\n",
    "df_z=np.squeeze(np.fliplr(df_z))\n",
    "\n",
    "\n",
    "x_encoded_train = [array[:, 0] for array in df_encoded_train]\n",
    "y_encoded_train = [array[:, 1] for array in df_encoded_train]\n",
    "x_encoded_valid = [array[:, 0] for array in df_encoded_valid]\n",
    "y_encoded_valid = [array[:, 1] for array in df_encoded_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose encoded_train is your feature array for training\n",
    "X_train = np.squeeze(encoded_train)\n",
    "X_valid = np.squeeze(encoded_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a927427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define parameter ranges you want to try\n",
    "min_cluster_size_list = [2, 5, 10, 20, 30, 35, 40]\n",
    "min_samples_list = [None, 1, 5, 10, 20, 30, 35, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -1\n",
    "best_params = (None, None)\n",
    "results = []  # to store (min_cluster_size, min_samples, silhouette_score, n_clusters)\n",
    "\n",
    "for min_cluster_size in min_cluster_size_list:\n",
    "    for min_samples in min_samples_list:\n",
    "        # Create and fit HDBSCAN\n",
    "        hdb = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "        labels_train = hdb.fit_predict(X_train)\n",
    "\n",
    "        # If HDBSCAN assigns all points to the same cluster (or all -1), \n",
    "        # silhouette score is not meaningful, so we skip it\n",
    "        unique_labels = set(labels_train)\n",
    "        if len(unique_labels) < 2:\n",
    "            # All points are either noise or in a single cluster  skip\n",
    "            score = float('nan')\n",
    "            n_clusters = 0  # effectively zero meaningful clusters\n",
    "        else:\n",
    "            # Compute silhouette score\n",
    "            score = silhouette_score(X_train, labels_train)\n",
    "            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "\n",
    "            # Update best combo\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = (min_cluster_size, min_samples)\n",
    "\n",
    "        # Store results\n",
    "        results.append((min_cluster_size, min_samples, score, n_clusters))\n",
    "\n",
    "# Print best results\n",
    "print(f\"Best Silhouette Score: {best_score:0.3f}\")\n",
    "print(f\"Best Parameters: min_cluster_size={best_params[0]}, min_samples={best_params[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955249c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\n",
    "    'min_cluster_size', 'min_samples', 'silhouette_score', 'num_clusters'\n",
    "])\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=40, min_samples=40)\n",
    "\n",
    "labels_train = hdb.fit_predict(X_train)\n",
    "labels_valid = hdb.fit_predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53732cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil  # we'll copy files rather than move them\n",
    "\n",
    "# The directory where your structures now live (renamed or partially renamed).\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial'\n",
    "\n",
    "# Directory where you want to create subfolders for each cluster.\n",
    "labelledData_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/hdbscan_labels_train'\n",
    "\n",
    "mapping_file = 'train_index_mapping.csv'\n",
    "\n",
    "with open(mapping_file, 'r') as f:\n",
    "    # Skip header if present\n",
    "    header = next(f).strip()  # e.g. \"loop_index,train_index,pdb_filename\"\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Parse the columns: loop_index, train_index, pdb_filename\n",
    "        loop_index_str, train_index_str, pdb_filename = line.split(',')\n",
    "        loop_index = int(loop_index_str)\n",
    "        train_index = int(train_index_str)\n",
    "\n",
    "        # Extract the 4-letter PDB code, e.g. \"1A9U\" from \"1A9U_A_filled.pdb\"\n",
    "        # (Assumes the file always starts with that 4-letter code.)\n",
    "        pdb_code_4 = pdb_filename[:4]\n",
    "\n",
    "        # Get the label from labels_train using the same loop index\n",
    "        label = labels_train[loop_index]\n",
    "        \n",
    "        # Decide the subfolder name based on label\n",
    "        if label == -1:\n",
    "            subfolder = \"noise\"  # or \"outliers\"\n",
    "        else:\n",
    "            subfolder = f\"cluster_{label}\"\n",
    "\n",
    "        # Create the label-specific subfolder, if it doesn't exist\n",
    "        label_folder = os.path.join(labelledData_dir, subfolder)\n",
    "        os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "        # Search getDatasetTrial_dir for any file that starts with these 4 letters\n",
    "        matched_file_path = None\n",
    "        for fname in os.listdir(getDatasetTrial_dir):\n",
    "            # e.g. we want to match \"1A9U_A_filled_aligned.pdb\" if it starts with \"1A9U\"\n",
    "            if fname.startswith(pdb_code_4):\n",
    "                possible_path = os.path.join(getDatasetTrial_dir, fname)\n",
    "                if os.path.isfile(possible_path):\n",
    "                    matched_file_path = possible_path\n",
    "                    break  # we found one match, so stop searching\n",
    "\n",
    "        if matched_file_path is not None:\n",
    "            # Construct the new path in the subfolder\n",
    "            new_path = os.path.join(label_folder, os.path.basename(matched_file_path))\n",
    "            shutil.copy2(matched_file_path, new_path)\n",
    "            print(f\"Copied {matched_file_path} -> {new_path}\")\n",
    "        else:\n",
    "            print(f\"No file found in {getDatasetTrial_dir} starting with '{pdb_code_4}'\") #The files that are not found are the validation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil  # we'll copy files rather than move them\n",
    "\n",
    "# The directory where your structures now live (renamed or partially renamed).\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/getDatasetTrial'\n",
    "  \n",
    "# Directory where you want to create subfolders for each cluster.\n",
    "labelledData = 'Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/hdbscan_labels_valid'\n",
    "\n",
    "mapping_file = 'valid_index_mapping.csv'\n",
    "\n",
    "with open(mapping_file, 'r') as f:\n",
    "    # Skip header if present\n",
    "    header = next(f).strip()  # e.g. \"loop_index,valid_index,pdb_filename\"\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Parse the columns: loop_index, valid_index, pdb_filename\n",
    "        loop_index_str, valid_index_str, pdb_filename = line.split(',')\n",
    "        loop_index = int(loop_index_str)\n",
    "        valid_index = int(valid_index_str)\n",
    "\n",
    "        # Extract the 4-letter PDB code, e.g. \"1A9U\" from \"1A9U_A_filled.pdb\"\n",
    "        # (Assumes the file always starts with that 4-letter code.)\n",
    "        pdb_code_4 = pdb_filename[:4]\n",
    "\n",
    "        # Get the label from labels_valid using the same loop index\n",
    "        label = labels_valid[loop_index]\n",
    "        \n",
    "        # Decide the subfolder name based on label\n",
    "        if label == -1:\n",
    "            subfolder = \"noise\"  # or \"outliers\"\n",
    "        else:\n",
    "            subfolder = f\"cluster_{label}\"\n",
    "\n",
    "        # Create the label-specific subfolder, if it doesn't exist\n",
    "        label_folder = os.path.join(labelledData_dir, subfolder)\n",
    "        os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "        # Search getDatasetTrial_dir for any file that starts with these 4 letters\n",
    "        matched_file_path = None\n",
    "        for fname in os.listdir(getDatasetTrial_dir):\n",
    "            # e.g. we want to match \"1A9U_A_filled_aligned.pdb\" if it starts with \"1A9U\"\n",
    "            if fname.startswith(pdb_code_4):\n",
    "                possible_path = os.path.join(getDatasetTrial_dir, fname)\n",
    "                if os.path.isfile(possible_path):\n",
    "                    matched_file_path = possible_path\n",
    "                    break  # we found one match, so stop searching\n",
    "\n",
    "        if matched_file_path is not None:\n",
    "            # Construct the new path in the subfolder\n",
    "            new_path = os.path.join(label_folder, os.path.basename(matched_file_path))\n",
    "            shutil.copy2(matched_file_path, new_path)\n",
    "            print(f\"Copied {matched_file_path} -> {new_path}\")\n",
    "        else:\n",
    "            print(f\"No file found in {getDatasetTrial_dir} starting with '{pdb_code_4}'\") #The files that are not found are the validation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56127f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Create 2 square subplots \n",
    "# Specify the position of the subplots in the figure\n",
    "ax1 = fig.add_axes([0.0, 0.1, 0.35, 0.35])\n",
    "ax1.imshow(df_z, cmap='viridis', vmin=0., vmax=10., extent=[np.min(df_x), np.max(df_x), np.min(df_y), np.max(df_y)])\n",
    "ax1.scatter(x_encoded_train, y_encoded_train, c=labels_train, marker='.', cmap='Paired')\n",
    "\n",
    "ax2 = fig.add_axes([0.38, 0.1, 0.35, 0.35])\n",
    "im = ax2.imshow(df_z, cmap='viridis', vmin=0., vmax=10., extent=[np.min(df_x), np.max(df_x), np.min(df_y), np.max(df_y)])\n",
    "ax2.scatter(x_encoded_valid, y_encoded_valid, c=labels_valid, marker='.', cmap='Paired')\n",
    "\n",
    "# Create 2 vertical colorbar axes for the subplots\n",
    "# Specify the position of the colorbar axes relative to the subplots\n",
    "cbar_ax2 = fig.add_axes([0.755,0.1,0.02,0.35])\n",
    "cbar_ax2.tick_params(left=False, labelleft=False, right=True, labelright=True, labelbottom=False, bottom = False)\n",
    "\n",
    "# Create colorbar\n",
    "cbar = fig.colorbar(im, cax=cbar_ax2, label='RMSD [$\\AA$]')\n",
    "cbar_ticks = np.linspace(0, 10, 7)\n",
    "cbar.set_ticks(cbar_ticks)\n",
    "cbar.set_ticklabels([f'{tick:.0f}' for tick in cbar_ticks])\n",
    "\n",
    "# Set axes properties\n",
    "ax1.tick_params(direction='inout', labelbottom=True, top=False, bottom=True)\n",
    "ax2.tick_params(direction='inout', labelbottom=True, top=False, bottom=True, left=False, labelleft=False)\n",
    "\n",
    "# Set labels\n",
    "ax1.set_xlabel('Latent vector 1')\n",
    "ax1.set_ylabel('Latent vector 2')\n",
    "ax2.set_xlabel('Latent vector 1')\n",
    "\n",
    "#Set titles\n",
    "ax1.set_title('Training dataset')\n",
    "ax2.set_title('Validation dataset')\n",
    "\n",
    "plt.savefig('Results/run_trial_BRAFActivationLoop_postalign_checkpoint1/RMSDlandscapesOnePlot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8532881",
   "metadata": {},
   "source": [
    "Molearn step for secondary structure realignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f06c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    data = PDBData()\n",
    "    folder_name = 'Results/fitted_matlab_segments/mustangs_conserved_secondary_noOutliers'\n",
    "\n",
    "    # Get a sorted list of all files in the directory, EXCLUDING combined.pdb\n",
    "    files = sorted([\n",
    "        f for f in os.listdir(folder_name) \n",
    "        if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "    ])\n",
    "\n",
    "    combined_file_path = os.path.join(folder_name, 'combined.pdb')\n",
    "\n",
    "    # Open in write mode to overwrite or create a fresh combined.pdb\n",
    "    with open(combined_file_path, 'w') as combined_file:\n",
    "        for i, filename in enumerate(files):\n",
    "            file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "            # Read content while filtering out lines starting with \"MODEL\" or \"END\"\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                lines = [line for line in lines if not line.startswith((\"MODEL\", \"END\"))]\n",
    "\n",
    "            # Write \"MODEL i\", then filtered lines, then \"ENDMDL\"\n",
    "            combined_file.write(f'MODEL {i}\\n')\n",
    "            combined_file.writelines(lines)\n",
    "            combined_file.write('ENDMDL\\n')\n",
    "        \n",
    "        combined_file.write('END\\n')\n",
    "\n",
    "    # Now import combined.pdb without having to delete it each run\n",
    "    data.import_pdb(filename=combined_file_path)\n",
    "    data.fix_terminal()\n",
    "    data.atomselect(atoms=['CA', 'C', 'N', 'CB', 'O'])\n",
    "    data.prepare_dataset()\n",
    "    print(data._mol)\n",
    "\n",
    "    ##### Prepare Trainer #####\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    trainer = Trainer(device=device)\n",
    "    trainer.set_data(data, batch_size=8, validation_split=0.1, manual_seed=25)\n",
    "    trainer.set_autoencoder(Small_AutoEncoder, out_points=data.dataset.shape[-1])\n",
    "    trainer.prepare_optimiser()\n",
    "\n",
    "    ##### Training Loop #####\n",
    "    # Keep training until loss does not improve for 32 consecutive epochs\n",
    "    runkwargs = dict(\n",
    "        log_filename='log_file.dat',\n",
    "        log_folder='Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/xbb_foldingnet_checkpoints',\n",
    "        checkpoint_folder='Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1',\n",
    "    )\n",
    "    best = 1e24\n",
    "    while True:\n",
    "        trainer.run(max_epochs=32 + trainer.epoch, **runkwargs)\n",
    "        if not best > trainer.best:\n",
    "            break\n",
    "        best = trainer.best\n",
    "    print(f'best {trainer.best}, best_filename {trainer.best_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import molearn\n",
    "from molearn.analysis.analyser import MolearnAnalysis\n",
    "from molearn.data import PDBData\n",
    "from molearn.models.small_foldingnet import Small_AutoEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import biobox as bb\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "\n",
    "import MDAnalysis as mda\n",
    "file_pattern = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/checkpoint*.ckpt'\n",
    "\n",
    "print(glob.glob(file_pattern))\n",
    "# Get a sorted list of all matching checkpoint files\n",
    "matching_files = sorted(glob.glob(file_pattern))\n",
    "\n",
    "# Check if we found any matches\n",
    "if len(matching_files) == 0:\n",
    "    raise FileNotFoundError(f\"No files matched the pattern: {file_pattern}\")\n",
    "\n",
    "# Example 1: Grab the first matching file\n",
    "networkfile = matching_files[0]\n",
    "\n",
    "# Example 2 (Alternative): Grab the last matching file (e.g., if its the most recent)\n",
    "# networkfile = matching_files[-1]\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(networkfile, map_location=torch.device('cpu'))\n",
    "net = Small_AutoEncoder(**checkpoint['network_kwargs'])\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Print some info\n",
    "print(\"Matched files:\", matching_files)\n",
    "print(\"Using file:\", networkfile)\n",
    "print(\"Network kwargs:\", checkpoint['network_kwargs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55009f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data = PDBData()\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_conserved_secondary_noOutliers'\n",
    "combined_file_path = os.path.join(folder_name, 'combined.pdb')\n",
    "data.import_pdb(filename=combined_file_path)\n",
    "#data.import_pdb(f'data{os.sep}MurD_open_selection.pdb')\n",
    "data.fix_terminal()\n",
    "data.atomselect(atoms = ['CA', 'C', 'N', 'CB', 'O'])\n",
    "data.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA = MolearnAnalysis()\n",
    "MA.set_network(net)\n",
    "data_train, data_valid = data.split(manual_seed=25)\n",
    "MA.set_dataset(\"training\", data_train)\n",
    "MA.set_dataset(\"validation\", data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifnotmake(dir_path):\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    return dir_path\n",
    "data_train, data_valid = data.get_datasets(manual_seed=25)\n",
    "\n",
    "indices = data.indices.numpy()\n",
    "\n",
    "print(indices)\n",
    "\n",
    "print(np.shape(data_train)[0])\n",
    "\n",
    "train_indices = indices[:np.shape(data_train)[0]]\n",
    "valid_indices = indices[np.shape(data_train)[0]:]\n",
    "\n",
    "print(train_indices)\n",
    "\n",
    "print(np.shape(data_train.numpy()))\n",
    "ifnotmake('Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial/')\n",
    "for i, index in enumerate(train_indices):\n",
    "\n",
    "    print(index)\n",
    "\n",
    "    data._mol.trajectory[index]  # Set the trajectory to the specific frame\n",
    "\n",
    "    data._mol.select_atoms(\"name CA\").write(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial/s{i}.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a10c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/decoded_train'\n",
    "\n",
    "ifnotmake(directory_name)\n",
    "# Use generate method which is public (no underscore)\n",
    "\n",
    "latent_coords = MA.get_encoded('training')  # Example latent space coordinates\n",
    "\n",
    "#print(np.shape(np.squeeze(latent_coords.numpy())))\n",
    "\n",
    "crd_ref = MA.generate(latent_coords.numpy().reshape(1, len(latent_coords), 2), directory_name, relax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_name = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/decoded_valid'\n",
    "\n",
    "ifnotmake(directory_name)\n",
    "# Use generate method which is public (no underscore)\n",
    "\n",
    "latent_coords = MA.get_encoded('validation')  # Example latent space coordinates\n",
    "\n",
    "#print(np.shape(np.squeeze(latent_coords.numpy())))\n",
    "\n",
    "crd_ref = MA.generate(latent_coords.numpy().reshape(1, len(latent_coords), 2), directory_name, relax=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a760d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import MDAnalysis as mda\n",
    "import MDAnalysis.analysis.rms as rms\n",
    "\n",
    "# Directories\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial/'\n",
    "decoded_train_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/decoded_train'\n",
    "\n",
    "# List all files in the directories\n",
    "getDatasetTrial_files = os.listdir(getDatasetTrial_dir)\n",
    "decoded_train_files = os.listdir(decoded_train_dir)\n",
    "\n",
    "# Filter files that start with 's' in decoded_train\n",
    "decoded_train_files = [f for f in decoded_train_files if f.startswith('s')]\n",
    "\n",
    "# Function to extract the numerical part from a file name\n",
    "def extract_number(file_name):\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    return int(match.group()) if match else float('inf')\n",
    "\n",
    "# Sort the file names based on the numerical part\n",
    "getDatasetTrial_files.sort(key=extract_number)\n",
    "decoded_train_files.sort(key=extract_number)\n",
    "\n",
    "# Calculate RMSD for files with the same name\n",
    "rmsd_values = {}\n",
    "\n",
    "for file_name in getDatasetTrial_files:\n",
    "    if file_name in decoded_train_files:\n",
    "        # Load the universes\n",
    "        u1 = mda.Universe(os.path.join(getDatasetTrial_dir, file_name))\n",
    "        u2 = mda.Universe(os.path.join(decoded_train_dir, file_name))\n",
    "        \n",
    "        # Select CA atoms\n",
    "        ag1 = u1.select_atoms(\"name CA\")\n",
    "        ag2 = u2.select_atoms(\"name CA\")\n",
    "        \n",
    "        # Calculate RMSD\n",
    "        rmsd_value = rms.rmsd(ag1.positions, ag2.positions)\n",
    "        rmsd_values[file_name] = rmsd_value\n",
    "\n",
    "# Print RMSD values in sorted order\n",
    "for file_name in sorted(rmsd_values.keys(), key=extract_number):\n",
    "    print(f\"RMSD for {file_name}: {rmsd_values[file_name]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7958ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(MA.get_error('training', align=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca95016",
   "metadata": {},
   "source": [
    "Renaming input and decoded datasets with PDB IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1) Gather the original files again (same sorting you used earlier):\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_conserved_secondary_noOutliers'\n",
    "files = sorted([\n",
    "    f for f in os.listdir(folder_name)\n",
    "    if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "])\n",
    "\n",
    "# 2) Define the directories that have \"s{i}.pdb\"\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial'\n",
    "decoded_train_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/decoded_train'\n",
    "\n",
    "\n",
    "mapping_filepath = 'train_index_mapping_secondary.csv'\n",
    "with open(mapping_filepath, 'w') as mapping_file:\n",
    "    # Write a header (optional)\n",
    "    mapping_file.write(\"loop_index,train_index,pdb_filename\\n\")\n",
    "\n",
    "    # Suppose you already have train_indices from your dataset split\n",
    "    # We'll loop over each entry in train_indices:\n",
    "    for i, index in enumerate(train_indices):\n",
    "        original_filename = files[index]  # the PDB name from the original folder\n",
    "\n",
    "        # Write this mapping to file:\n",
    "        mapping_file.write(f\"{i},{index},{original_filename}\\n\")\n",
    "\n",
    "        # 3) Rename the file in getDatasetTrial:\n",
    "        old_file_getDataset = os.path.join(getDatasetTrial_dir, f's{i}.pdb')\n",
    "        new_file_getDataset = os.path.join(getDatasetTrial_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_getDataset):\n",
    "            os.rename(old_file_getDataset, new_file_getDataset)\n",
    "            print(f'Renamed: {old_file_getDataset} -> {new_file_getDataset}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_getDataset}')\n",
    "\n",
    "        # 4) Rename the file in decoded_train:\n",
    "        old_file_decoded = os.path.join(decoded_train_dir, f's{i}.pdb')\n",
    "        new_file_decoded = os.path.join(decoded_train_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_decoded):\n",
    "            os.rename(old_file_decoded, new_file_decoded)\n",
    "            print(f'Renamed: {old_file_decoded} -> {new_file_decoded}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_decoded}')\n",
    "\n",
    "print(f\"\\nMapping file saved at: {mapping_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a09b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1) Gather the original files again (same sorting you used earlier):\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_conserved_secondary_noOutliers'\n",
    "files = sorted([\n",
    "    f for f in os.listdir(folder_name)\n",
    "    if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "])\n",
    "\n",
    "# 2) Define the directories that have \"s{i}.pdb\"\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial'\n",
    "decoded_valid_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/decoded_valid'\n",
    "\n",
    "\n",
    "mapping_filepath = 'valid_index_mapping_secondary.csv'\n",
    "with open(mapping_filepath, 'w') as mapping_file:\n",
    "    # Write a header (optional)\n",
    "    mapping_file.write(\"loop_index,valid_index,pdb_filename\\n\")\n",
    "\n",
    "    # Suppose you already have valid_indices from your dataset split\n",
    "    # We'll loop over each entry in valid_indices:\n",
    "    for i, index in enumerate(valid_indices):\n",
    "        original_filename = files[index]  # the PDB name from the original folder\n",
    "\n",
    "        # Write this mapping to file:\n",
    "        mapping_file.write(f\"{i},{index},{original_filename}\\n\")\n",
    "\n",
    "        # 3) Rename the file in getDatasetTrial:\n",
    "        old_file_getDataset = os.path.join(getDatasetTrial_dir, f's{i}.pdb')\n",
    "        new_file_getDataset = os.path.join(getDatasetTrial_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_getDataset):\n",
    "            os.rename(old_file_getDataset, new_file_getDataset)\n",
    "            print(f'Renamed: {old_file_getDataset} -> {new_file_getDataset}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_getDataset}')\n",
    "\n",
    "        # 4) Rename the file in decoded_valid:\n",
    "        old_file_decoded = os.path.join(decoded_valid_dir, f's{i}.pdb')\n",
    "        new_file_decoded = os.path.join(decoded_valid_dir, original_filename)\n",
    "        \n",
    "        if os.path.exists(old_file_decoded):\n",
    "            os.rename(old_file_decoded, new_file_decoded)\n",
    "            print(f'Renamed: {old_file_decoded} -> {new_file_decoded}')\n",
    "        else:\n",
    "            print(f'File not found (skipping): {old_file_decoded}')\n",
    "\n",
    "print(f\"\\nMapping file saved at: {mapping_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e227c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA.batch_size = 8\n",
    "MA.processes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveName = '_foldingnet_checkpoint'\n",
    "\n",
    "#saveName = directory.replace('_foldingnet_checkpoint', '')\n",
    "err_train = MA.get_error('training')\n",
    "df_err_train = pd.DataFrame(err_train, columns=['err_train'])\n",
    "df_err_train.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_train_{saveName}.csv', index=False)\n",
    "\n",
    "err_test = MA.get_error('validation')\n",
    "df_err_test = pd.DataFrame(err_test, columns=['err_test'])\n",
    "df_err_test.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_valid_{saveName}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "data = [err_train]\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data)\n",
    "plt.ylabel('RMSD [$\\AA$]')\n",
    "plt.title('Reconstruction error between encoded and decoded training dataset')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_train_{saveName}.png', dpi=300)\n",
    "\n",
    "\n",
    "data = [err_test]\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data)\n",
    "plt.ylabel('RMSD [$\\AA$]')\n",
    "plt.title('Reconstruction error between encoded and decoded validation dataset')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_test_{saveName}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9486b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Define paths, read in the data\n",
    "saveName = '_foldingnet_checkpoint'\n",
    "train_err_csv = f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_train_{saveName}.csv'\n",
    "valid_err_csv = f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_valid_{saveName}.csv'\n",
    "\n",
    "df_err_train = pd.read_csv(train_err_csv)   # columns: ['err_train']\n",
    "df_err_valid = pd.read_csv(valid_err_csv)   # columns: ['err_test']\n",
    "\n",
    "# 2) Identify outliers where RMSD > 4\n",
    "train_outlier_indices = df_err_train.index[df_err_train['err_train'] > 4].to_list()\n",
    "valid_outlier_indices = df_err_valid.index[df_err_valid['err_test'] > 4].to_list()\n",
    "\n",
    "# 3) Gather original filenames and the directories containing your re-labeled PDBs\n",
    "#    - \"files\": the sorted list of PDB filenames from the original folder \n",
    "#               (the same you used to create combined.pdb, excluding 'combined.pdb').\n",
    "\n",
    "folder_name = 'Results/fitted_matlab_segments/mustangs_conserved_secondary_noOutliers'\n",
    "files = sorted([\n",
    "    f for f in os.listdir(folder_name)\n",
    "    if os.path.isfile(os.path.join(folder_name, f)) and f != 'combined.pdb'\n",
    "])\n",
    "\n",
    "#    - \"train_indices\" (and possibly \"valid_indices\") map each row in df_err_train/df_err_valid \n",
    "#      to the correct frame index. For simple splits, you might only need train_indices \n",
    "#      if you used data.split() from your PDBData object. (Adjust this code if valid set \n",
    "#      also has a separate set of indices or if you used a different approach.)\n",
    "# Suppose something like:\n",
    "# train_indices, valid_indices = data.get_datasets_indices(manual_seed=25)\n",
    "# or similar. Make sure these match the order used in MA.get_error('training') and .get_error('validation').\n",
    "\n",
    "# 4) Create output folders for outliers\n",
    "outlier_dir_train = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/rmsd_outliers_train'\n",
    "outlier_dir_valid = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/rmsd_outliers_valid'\n",
    "os.makedirs(outlier_dir_train, exist_ok=True)\n",
    "os.makedirs(outlier_dir_valid, exist_ok=True)\n",
    "\n",
    "# 5) Copy outlier files for training\n",
    "source_dir_train = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial'\n",
    "for row_i in train_outlier_indices:\n",
    "    frame_idx = train_indices[row_i]          # The actual frame index\n",
    "    original_filename = files[frame_idx]      # The original PDB name\n",
    "\n",
    "    src = os.path.join(source_dir_train, original_filename)\n",
    "    dst = os.path.join(outlier_dir_train, original_filename)\n",
    "\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f'Copied outlier (train): {src} -> {dst}')\n",
    "    else:\n",
    "        print(f'Skipping missing file (train): {src}')\n",
    "\n",
    "# 6) Copy outlier files for validation\n",
    "#    If valid set also uses a \"valid_indices\" array, map row_i -> valid_indices[row_i] -> files[...].\n",
    "#    If you used a single \"indices\" array and just split it, adjust accordingly.\n",
    "source_dir_valid = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial'\n",
    "for row_i in valid_outlier_indices:\n",
    "    frame_idx = valid_indices[row_i]\n",
    "    original_filename = files[frame_idx]\n",
    "\n",
    "    src = os.path.join(source_dir_valid, original_filename)\n",
    "    dst = os.path.join(outlier_dir_valid, original_filename)\n",
    "\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        print(f'Copied outlier (valid): {src} -> {dst}')\n",
    "    else:\n",
    "        print(f'Skipping missing file (valid): {src}')\n",
    "\n",
    "# 7) Plot RMSD for training with a red dotted cutoff line at 4 \n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data=[df_err_train['err_train']])\n",
    "plt.axhline(y=4.0, color='red', linestyle='--', label='Outlier cutoff')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSD [$\\\\AA$]')\n",
    "plt.title('Reconstruction error (training)')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_train_{saveName}.png', dpi=300)\n",
    "\n",
    "# 8) Plot RMSD for validation with a red dotted cutoff line at 4 \n",
    "f = plt.figure(figsize=(10, 10))\n",
    "sns.violinplot(data=[df_err_valid['err_test']])\n",
    "plt.axhline(y=4.0, color='red', linestyle='--', label='Outlier cutoff')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSD [$\\\\AA$]')\n",
    "plt.title('Reconstruction error (validation)')\n",
    "plt.show()\n",
    "f.savefig(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/err_test_{saveName}.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68906287",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA.setup_grid(30)\n",
    "landscape_err_latent, landscape_err_3d, xaxis, yaxis = MA.scan_error()\n",
    "df_landscape_err_latent = pd.DataFrame(landscape_err_latent)\n",
    "df_landscape_err_latent.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_err_latent_{saveName}.csv', index=False)\n",
    "'''\n",
    "df= pd.read_csv(f'landscape_err_latent_{saveName}.csv')\n",
    "print(df.shape)\n",
    "'''\n",
    "\n",
    "df_landscape_err_3d = pd.DataFrame(landscape_err_3d)\n",
    "df_landscape_err_3d.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_err_3d_{saveName}.csv', index=False)\n",
    "\n",
    "df_xaxis = pd.DataFrame(xaxis)\n",
    "df_xaxis.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_err_xaxis_{saveName}.csv', index=False)\n",
    "\n",
    "df_yaxis = pd.DataFrame(yaxis)\n",
    "df_yaxis.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_err_yaxis_{saveName}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ac575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from molearn.analysis import MolearnGUI\n",
    "MolearnGUI(MA);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71118f5c",
   "metadata": {},
   "source": [
    "Projection into latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40894240",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = net.encode(data_train.float())\n",
    "\n",
    "z_training = z.data.cpu().numpy()[:, :, 0]\n",
    "df_z_train = pd.DataFrame(z_training)\n",
    "df_z_train.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_encoded_train_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84389040",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = net.encode(data_valid.float())\n",
    "z_valid = z.data.cpu().numpy()[:, :, 0]\n",
    "df_z_valid = pd.DataFrame(z_valid)\n",
    "df_z_valid.to_csv(f'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_encoded_valid_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69e645",
   "metadata": {},
   "source": [
    "Do HDBSCAN and show landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c08599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import rotate\n",
    "from numpy.linalg import inv\n",
    "\n",
    "x_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_err_xaxis__foldingnet_checkpoint.csv\")\n",
    "y_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_err_yaxis__foldingnet_checkpoint.csv\")\n",
    "z_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_err_latent__foldingnet_checkpoint.csv\")\n",
    "\n",
    "encoded_train_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_encoded_train_coordinates.csv\")\n",
    "encoded_valid_file = glob(\"Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/landscape_encoded_valid_coordinates.csv\")\n",
    "df_encoded_train = [pd.read_csv(file, header=None).to_numpy() for file in encoded_train_file]\n",
    "df_encoded_valid = [pd.read_csv(file, header=None).to_numpy() for file in encoded_valid_file]\n",
    "'encdoded coordinates are in the same order as decoded coordinates'\n",
    "encoded_train = [array[:, :] for array in df_encoded_train]\n",
    "encoded_valid = [array[:, :] for array in df_encoded_valid]\n",
    "\n",
    "df_x = [pd.read_csv(file, header=None).to_numpy() for file in x_file]\n",
    "df_y = [pd.read_csv(file, header=None).to_numpy() for file in y_file]\n",
    "df_z = [pd.read_csv(file).to_numpy() for file in z_file]\n",
    "# Need fliplr because heatmap displays data differently from how array stores it\n",
    "df_z=np.squeeze(np.fliplr(df_z))\n",
    "\n",
    "\n",
    "\n",
    "x_encoded_train = [array[:, 0] for array in df_encoded_train]\n",
    "y_encoded_train = [array[:, 1] for array in df_encoded_train]\n",
    "x_encoded_valid = [array[:, 0] for array in df_encoded_valid]\n",
    "y_encoded_valid = [array[:, 1] for array in df_encoded_valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0ae794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose encoded_train is your feature array for training\n",
    "X_train = np.squeeze(encoded_train)\n",
    "X_valid = np.squeeze(encoded_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d23cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define parameter ranges you want to try\n",
    "min_cluster_size_list = [2, 5, 10, 20, 30]\n",
    "min_samples_list = [None, 1, 5, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -1\n",
    "best_params = (None, None)\n",
    "results = []  # to store (min_cluster_size, min_samples, silhouette_score, n_clusters)\n",
    "\n",
    "for min_cluster_size in min_cluster_size_list:\n",
    "    for min_samples in min_samples_list:\n",
    "        # Create and fit HDBSCAN\n",
    "        hdb = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "        labels_train = hdb.fit_predict(X_train)\n",
    "\n",
    "        # If HDBSCAN assigns all points to the same cluster (or all -1), \n",
    "        # silhouette score is not meaningful, so we skip it\n",
    "        unique_labels = set(labels_train)\n",
    "        if len(unique_labels) < 2:\n",
    "            # All points are either noise or in a single cluster  skip\n",
    "            score = float('nan')\n",
    "            n_clusters = 0  # effectively zero meaningful clusters\n",
    "        else:\n",
    "            # Compute silhouette score\n",
    "            score = silhouette_score(X_train, labels_train)\n",
    "            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "\n",
    "            # Update best combo\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = (min_cluster_size, min_samples)\n",
    "\n",
    "        # Store results\n",
    "        results.append((min_cluster_size, min_samples, score, n_clusters))\n",
    "\n",
    "# Print best results\n",
    "print(f\"Best Silhouette Score: {best_score:0.3f}\")\n",
    "print(f\"Best Parameters: min_cluster_size={best_params[0]}, min_samples={best_params[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970c36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\n",
    "    'min_cluster_size', 'min_samples', 'silhouette_score', 'num_clusters'\n",
    "])\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f76660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdb = HDBSCAN(min_cluster_size=10, min_samples=1)\n",
    "\n",
    "labels_train = hdb.fit_predict(X_train)\n",
    "labels_valid = hdb.fit_predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2f419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil  # we'll copy files rather than move them\n",
    "\n",
    "# The directory where your structures now live (renamed or partially renamed).\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial'\n",
    "\n",
    "# Directory where you want to create subfolders for each cluster.\n",
    "labelledData_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/hdbscan_labels_train'\n",
    "\n",
    "mapping_file = 'train_index_mapping_secondary.csv'\n",
    "\n",
    "with open(mapping_file, 'r') as f:\n",
    "    # Skip header if present\n",
    "    header = next(f).strip()  # e.g. \"loop_index,train_index,pdb_filename\"\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Parse the columns: loop_index, train_index, pdb_filename\n",
    "        loop_index_str, train_index_str, pdb_filename = line.split(',')\n",
    "        loop_index = int(loop_index_str)\n",
    "        train_index = int(train_index_str)\n",
    "\n",
    "        # Extract the 4-letter PDB code, e.g. \"1A9U\" from \"1A9U_A_filled.pdb\"\n",
    "        # (Assumes the file always starts with that 4-letter code.)\n",
    "        pdb_code_4 = pdb_filename[:4]\n",
    "\n",
    "        # Get the label from labels_train using the same loop index\n",
    "        label = labels_train[loop_index]\n",
    "        \n",
    "        # Decide the subfolder name based on label\n",
    "        if label == -1:\n",
    "            subfolder = \"noise\"  # or \"outliers\"\n",
    "        else:\n",
    "            subfolder = f\"cluster_{label}\"\n",
    "\n",
    "        # Create the label-specific subfolder, if it doesn't exist\n",
    "        label_folder = os.path.join(labelledData_dir, subfolder)\n",
    "        os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "        # Search getDatasetTrial_dir for any file that starts with these 4 letters\n",
    "        matched_file_path = None\n",
    "        for fname in os.listdir(getDatasetTrial_dir):\n",
    "            # e.g. we want to match \"1A9U_A_filled_aligned.pdb\" if it starts with \"1A9U\"\n",
    "            if fname.startswith(pdb_code_4):\n",
    "                possible_path = os.path.join(getDatasetTrial_dir, fname)\n",
    "                if os.path.isfile(possible_path):\n",
    "                    matched_file_path = possible_path\n",
    "                    break  # we found one match, so stop searching\n",
    "\n",
    "        if matched_file_path is not None:\n",
    "            # Construct the new path in the subfolder\n",
    "            new_path = os.path.join(label_folder, os.path.basename(matched_file_path))\n",
    "            shutil.copy2(matched_file_path, new_path)\n",
    "            print(f\"Copied {matched_file_path} -> {new_path}\")\n",
    "        else:\n",
    "            print(f\"No file found in {getDatasetTrial_dir} starting with '{pdb_code_4}'\") #The files that are not found are the validation files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil  # we'll copy files rather than move them\n",
    "\n",
    "# The directory where your structures now live (renamed or partially renamed).\n",
    "getDatasetTrial_dir = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/getDatasetTrial'\n",
    "  \n",
    "# Directory where you want to create subfolders for each cluster.\n",
    "labelledData = 'Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/hdbscan_labels_valid'\n",
    "\n",
    "mapping_file = 'valid_index_mapping_secondary.csv'\n",
    "\n",
    "with open(mapping_file, 'r') as f:\n",
    "    # Skip header if present\n",
    "    header = next(f).strip()  # e.g. \"loop_index,valid_index,pdb_filename\"\n",
    "    \n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Parse the columns: loop_index, valid_index, pdb_filename\n",
    "        loop_index_str, valid_index_str, pdb_filename = line.split(',')\n",
    "        loop_index = int(loop_index_str)\n",
    "        valid_index = int(valid_index_str)\n",
    "\n",
    "        # Extract the 4-letter PDB code, e.g. \"1A9U\" from \"1A9U_A_filled.pdb\"\n",
    "        # (Assumes the file always starts with that 4-letter code.)\n",
    "        pdb_code_4 = pdb_filename[:4]\n",
    "\n",
    "        # Get the label from labels_valid using the same loop index\n",
    "        label = labels_valid[loop_index]\n",
    "        \n",
    "        # Decide the subfolder name based on label\n",
    "        if label == -1:\n",
    "            subfolder = \"noise\"  # or \"outliers\"\n",
    "        else:\n",
    "            subfolder = f\"cluster_{label}\"\n",
    "\n",
    "        # Create the label-specific subfolder, if it doesn't exist\n",
    "        label_folder = os.path.join(labelledData_dir, subfolder)\n",
    "        os.makedirs(label_folder, exist_ok=True)\n",
    "\n",
    "        # Search getDatasetTrial_dir for any file that starts with these 4 letters\n",
    "        matched_file_path = None\n",
    "        for fname in os.listdir(getDatasetTrial_dir):\n",
    "            # e.g. we want to match \"1A9U_A_filled_aligned.pdb\" if it starts with \"1A9U\"\n",
    "            if fname.startswith(pdb_code_4):\n",
    "                possible_path = os.path.join(getDatasetTrial_dir, fname)\n",
    "                if os.path.isfile(possible_path):\n",
    "                    matched_file_path = possible_path\n",
    "                    break  # we found one match, so stop searching\n",
    "\n",
    "        if matched_file_path is not None:\n",
    "            # Construct the new path in the subfolder\n",
    "            new_path = os.path.join(label_folder, os.path.basename(matched_file_path))\n",
    "            shutil.copy2(matched_file_path, new_path)\n",
    "            print(f\"Copied {matched_file_path} -> {new_path}\")\n",
    "        else:\n",
    "            print(f\"No file found in {getDatasetTrial_dir} starting with '{pdb_code_4}'\") #The files that are not found are the validation files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Create 2 square subplots \n",
    "# Specify the position of the subplots in the figure\n",
    "ax1 = fig.add_axes([0.0, 0.1, 0.35, 0.35])\n",
    "ax1.imshow(df_z, cmap='viridis', vmin=0., vmax=10., extent=[np.min(df_x), np.max(df_x), np.min(df_y), np.max(df_y)])\n",
    "ax1.scatter(x_encoded_train, y_encoded_train, c=labels_train, marker='.', cmap='Paired')\n",
    "\n",
    "ax2 = fig.add_axes([0.38, 0.1, 0.35, 0.35])\n",
    "im = ax2.imshow(df_z, cmap='viridis', vmin=0., vmax=10., extent=[np.min(df_x), np.max(df_x), np.min(df_y), np.max(df_y)])\n",
    "ax2.scatter(x_encoded_valid, y_encoded_valid, c=labels_valid, marker='.', cmap='Paired')\n",
    "\n",
    "# Create 2 vertical colorbar axes for the subplots\n",
    "# Specify the position of the colorbar axes relative to the subplots\n",
    "cbar_ax2 = fig.add_axes([0.755,0.1,0.02,0.35])\n",
    "cbar_ax2.tick_params(left=False, labelleft=False, right=True, labelright=True, labelbottom=False, bottom = False)\n",
    "\n",
    "# Create colorbar\n",
    "cbar = fig.colorbar(im, cax=cbar_ax2, label='RMSD [$\\AA$]')\n",
    "cbar_ticks = np.linspace(0, 10, 7)\n",
    "cbar.set_ticks(cbar_ticks)\n",
    "cbar.set_ticklabels([f'{tick:.0f}' for tick in cbar_ticks])\n",
    "\n",
    "# Set axes properties\n",
    "ax1.tick_params(direction='inout', labelbottom=True, top=False, bottom=True)\n",
    "ax2.tick_params(direction='inout', labelbottom=True, top=False, bottom=True, left=False, labelleft=False)\n",
    "\n",
    "# Set labels\n",
    "ax1.set_xlabel('Latent vector 1')\n",
    "ax1.set_ylabel('Latent vector 2')\n",
    "ax2.set_xlabel('Latent vector 1')\n",
    "\n",
    "#Set titles\n",
    "ax1.set_title('Training dataset')\n",
    "ax2.set_title('Validation dataset')\n",
    "\n",
    "plt.savefig('Results/run_trial_BRAFActivationLoop_postalign_secondary_checkpoint1/RMSDlandscapesOnePlot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
