{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10169af4",
   "metadata": {},
   "source": [
    "# Characterization of loop dynamics in kinases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf65ee-cc85-4737-82cd-4d83671fc202",
   "metadata": {},
   "source": [
    "We present a workflow to discover protein conformational features associated with kinase loop rearrangments. The purpose of this notebook is to describe the necessary steps adopted in our study. Implementations of the described steps are included as `.py` files within the folder `workflow`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c282ca68-c04b-4a67-ad28-f2230f1a7b7c",
   "metadata": {},
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee469ade",
   "metadata": {},
   "source": [
    "Our modelling pipeline is subdivided in the following sections: \n",
    "1. [Dataset creation](#1)   \n",
    "    1.1. [The data](#11)   \n",
    "    1.2. [Data download](#12)   \n",
    "    1.3. [Kinase taxonomy](#13)   \n",
    "2. [Dataset curation](#2)  \n",
    "    2.1. [Extracting protein chains](#21)   \n",
    "    2.2. [Filtering for activation loop](#22)   \n",
    "    2.3. [Conformational classification](#23)   \n",
    "    2.4. [Structural conservation](#24)   \n",
    "    2.5. [Reconstructing small loop segments](#25)   \n",
    "    2.6. [Coarse-graining activation loops](#26)   \n",
    "3. [Dimensionality reduction](#3)   \n",
    "    3.1. [Low-dimensional representation](#31)   \n",
    "    3.2. [Clustering](#32)   \n",
    "    3.3. [Analysis](#33)   \n",
    "4. [Feature definition](#4)   \n",
    "    4.1. [Feature matrix](#41)  \n",
    "5. [Feature selection](#5)\n",
    "6. [Feature classification](#6)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b9168-5ebf-48f9-abba-5ec7632b0403",
   "metadata": {},
   "source": [
    "The overall pipeline, implemented in the sections hereafter, is represented according to the following schematic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a69178",
   "metadata": {},
   "source": [
    "![State of the workflow](images/fullPipelineSchematic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c73ca9",
   "metadata": {},
   "source": [
    "To get started, let's load some packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and system operations\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from glob import glob\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "\n",
    "# Network and parallel processing\n",
    "import requests\n",
    "import time\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# custom utility functions and class\n",
    "from workflow.utilities import count_pdb_files, braf_res, clear_and_make, make_seg, copy_filtered_pdbs\n",
    "from workflow.utilities import PDBDownloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f28d9",
   "metadata": {},
   "source": [
    "# 1. Dataset creation <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a843ab-aa5f-4587-b85d-2b253a12334f",
   "metadata": {},
   "source": [
    "In this section we will parse the Protein Data Bank (PDB) for kinase structures and download them into a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28396176",
   "metadata": {},
   "source": [
    "## 1.1 The data <a id=\"11\"></a>\n",
    "Our approach involves searching for protein homologs to the reference sequence: a BRAF kinase (PDB code: 6UAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3f140",
   "metadata": {},
   "source": [
    "BRAF is a key part of the MAPK/ERK pathway. This pathway relays signals from outside the cell, like growth factors binding to receptor tyrosine kinases, to control cell growth, division, survival, and differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e39c746",
   "metadata": {},
   "source": [
    "The active site sits in a cleft between the small N‑terminal lobe and larger C‑terminal lobe of the kinase domain. \n",
    "ATP binds in a pocket on the N‑lobe side of the kinase domain, at the P-binding loop. \n",
    "The protein substrate, mainly MEK kinase, contacts a broad surface on the C‑lobe of BRAF. \n",
    "Activation loop and αC helix interact with residues in the binding site, regulating activation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdef5df",
   "metadata": {},
   "source": [
    "![State of the workflow](images/BRAFSlide1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b125c3",
   "metadata": {},
   "source": [
    "When creating our dataset, we take as structural reference the BRAF structure since we are familiar with its typical regulatory role during phosphorilation. We query the InterPro database online at https://www.ebi.ac.uk/interpro/ to find structures in the PDB that match the protein kinase-like domain family. InterPro is a database that classifies protein sequences into families and predicts the presence of domains and important sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50023e87",
   "metadata": {},
   "source": [
    "Our query input is the BRAF sequence and we filter for structures that are part of the \"Protein kinase-like domain superfamily\" (IPR011009) and that are included in the PDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d831738",
   "metadata": {},
   "source": [
    "## 1.2 Data download <a id=\"12\"></a>\n",
    "Here we download the structures output from the InterPro query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f677ed4",
   "metadata": {},
   "source": [
    "Let's start by writing all PDB codes to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6da995",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_path = 'structure-matching-IPR011009.tsv'\n",
    "pdb_data = pd.read_csv(structure_path, sep = \"\\t\", header=0, engine='python')\n",
    "pdb_data['Accession'] = pdb_data['Accession'].str.upper()\n",
    "pdb_ids = pdb_data['Accession'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460578d6",
   "metadata": {},
   "source": [
    "The class `PDBDownloader` enables carrying out multi-threaded PDB download. It uses up to 2 CPU cores. We now download the PDB structures listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1cab52-ff65-44a1-93f0-0cb077ab7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = PDBDownloader()\n",
    "downloader.parallel_download(pdb_ids, \"Results/InterProPDBs\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef274f",
   "metadata": {},
   "source": [
    "We can now check how many structures from the InterPro query were actually downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeccb38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Results/InterProPDBs\"\n",
    "file_names = [os.path.splitext(f)[0] for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "pdb_raw = pd.DataFrame({\"PDBs\": file_names})\n",
    "\n",
    "pdb_data['Downloaded'] = pdb_data['Accession'].str.upper().isin(pdb_raw['PDBs']).map({True: True, False: False})\n",
    "\n",
    "counts = pdb_data['Downloaded'].value_counts().to_dict()\n",
    "print(f\"Downloaded: {counts[True]}, Failed: {counts[False]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107316df",
   "metadata": {},
   "source": [
    "We can save the names of failed PDB downloads for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150fa949",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_list = pdb_data[pdb_data['Downloaded']==False]\n",
    "fail_list.to_csv('fail_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67c72b",
   "metadata": {},
   "source": [
    "## 1.3 Kinase taxonomy <a id=\"13\"></a>\n",
    "We seek to annotate our dataset with kinase family, species, and class information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046b24d",
   "metadata": {},
   "source": [
    "The class `KinaseGroupLabeller` enables extracting metadata from UniProt to investigate what kinase families and which species are represented in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad50a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.kinaseGroupLabelling import KinaseGroupLabeller\n",
    "\n",
    "lab = KinaseGroupLabeller()\n",
    "annot = lab.run()  # Auto-discovers PDBs from Results/InterProPDBs\n",
    "display(annot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1cab8e",
   "metadata": {},
   "source": [
    "We now use the plotting method `plot_distribution_bars()` to visualise the parsed metadata as histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc1a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plots for kinase families, species, and classes\n",
    "# Using horizontal bar charts for readable labels\n",
    "fig_family = lab.plot_distribution_bars(annot, 'family', top_n=10)\n",
    "fig_species = lab.plot_distribution_bars(annot, 'species', top_n=15)\n",
    "fig_class = lab.plot_distribution_bars(annot, 'kinase_class')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6403146",
   "metadata": {},
   "source": [
    "Our dataset includes only Kinase-like structures as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28e737",
   "metadata": {},
   "source": [
    "# 2. Dataset curation <a id=\"2\"></a>\n",
    "In this section we will curate our kinase dataset for input into conformational analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61d85f",
   "metadata": {},
   "source": [
    "## 2.1 Extracting protein chains <a id=\"21\"></a>\n",
    "Here we extract only the protein chains containing a kinase domain from our database of downloaded PDB structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952b9660",
   "metadata": {},
   "source": [
    "We utilise the class `PDBChainExtractor()` to write to PDB files the coordinates of chains indicated by InterPro query output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.pdb_chain_extractor import PDBChainExtractor\n",
    "\n",
    "# Create an instance of the class\n",
    "chain_extractor = PDBChainExtractor()\n",
    "\n",
    "# Now call the method on the instance\n",
    "chain_extractor.extract_chains_parallel(pdb_data, 'Results/activation_segments/unaligned/', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7cfc7a",
   "metadata": {},
   "source": [
    "Let's make sure that the number of chains corresponds to at least the same amount of files downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab78944",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_directory = 'Results/activation_segments/unaligned/'\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f4a91",
   "metadata": {},
   "source": [
    "## 2.2 Filtering for activation loop  <a id=\"22\"></a>\n",
    "Here we exclude all kinase domains that do not have the characteristic conserved residue motifs DFG and APE that delimit the activation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c890d17",
   "metadata": {},
   "source": [
    "We utilise the method `copy_filtered_pdbs()` to extract amino acid sequences from the structures in our dataset and exclude those not containing DFG and APE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0034042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy filtered PDB files to a new directory\n",
    "source_dir = 'Results/activation_segments/unaligned/'\n",
    "target_dir = 'Results/activation_segments/motif_filtered/'\n",
    "valid_pdbs, invalid_pdbs = copy_filtered_pdbs(source_dir, target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9a273f",
   "metadata": {},
   "source": [
    "Let's check how many kinase domains we are left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_directory = 'Results/activation_segments/motif_filtered/'\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdcda83",
   "metadata": {},
   "source": [
    "## 2.3 Conformational classification  <a id=\"23\"></a>\n",
    "Here we investigate the conformational diversity of our kinase domains by applying the classification developed by the Dunbrack's group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf15ee8",
   "metadata": {},
   "source": [
    "The class `DunbrackWorkflow` enables performing the conformational classification using the `KinCore` software.\n",
    "\n",
    "**When KinCore fails**, structures are marked with `'failed'` status. This happens when:\n",
    "- KinCore cannot find the DFG or C-helix motifs\n",
    "- Structure has missing residues in critical regions\n",
    "- Non-standard kinase fold\n",
    "- Structure quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e531aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.DunbrackAssignment import DunbrackWorkflow\n",
    "\n",
    "# Initialize the workflow with your KinCore installation location\n",
    "workflow = DunbrackWorkflow(\n",
    "    input_dir='Results/activation_segments/motif_filtered/',\n",
    "    output_dir='Results/dunbrack_assignments',\n",
    "    kincore_dir='/home/marmatt/Documents/Kincore-standalone'  # Your actual KinCore installation\n",
    ")\n",
    "\n",
    "# Run the complete conformation assignment workflow\n",
    "results = workflow.run(\n",
    "    output_csv='kinase_conformation_assignments.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14545b",
   "metadata": {},
   "source": [
    "Let's now print some information about the KinCore analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282dc1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONFORMATION ASSIGNMENT RESULTS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "display(results.head(10))\n",
    "\n",
    "# Show conformation distribution\n",
    "print(\"\\nConformation Distribution:\")\n",
    "print(results['overall_conformation'].value_counts())\n",
    "\n",
    "# Show DFG motif distribution\n",
    "print(\"\\nDFG Motif Distribution:\")\n",
    "print(results['dfg_conformation'].value_counts())\n",
    "\n",
    "# Show C-helix distribution\n",
    "print(\"\\nC-helix Distribution:\")\n",
    "print(results['chelix_conformation'].value_counts())\n",
    "\n",
    "# Show ligand information\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIGAND INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLigand Distribution:\")\n",
    "ligand_counts = results['ligand'].value_counts()\n",
    "print(ligand_counts)\n",
    "\n",
    "print(f\"\\nTotal unique ligands: {len(ligand_counts)}\")\n",
    "print(f\"Structures with ligand: {(results['ligand'] != 'No_ligand').sum()}\")\n",
    "print(f\"Structures without ligand: {(results['ligand'] == 'No_ligand').sum()}\")\n",
    "\n",
    "# Show top 10 most common ligands (excluding No_ligand)\n",
    "print(\"\\nTop 10 most common ligands (excluding apo structures):\")\n",
    "top_ligands = results[results['ligand'] != 'No_ligand']['ligand'].value_counts().head(10)\n",
    "for ligand, count in top_ligands.items():\n",
    "    print(f\"  {ligand:.<20} {count:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e1b73b",
   "metadata": {},
   "source": [
    "Let's now visualise the metadata extracted from `KinCore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96022fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the multi-panel Dunbrack distribution plot (logic lives in the class now)\n",
    "_ = DunbrackWorkflow.plot_conformation_distribution(\n",
    "    assignments_csv=\"Results/dunbrack_assignments/kinase_conformation_assignments.csv\",\n",
    "    output_png=\"Results/dunbrack_assignments/conformation_distribution.png\",\n",
    "    show=True,\n",
    "    print_dunbrack_summary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e180be",
   "metadata": {},
   "source": [
    "## 2.4 Structural conservation  <a id=\"24\"></a>\n",
    "Here we will be investigating which residues of our reference BRAF kinase are structurally conserved across the collected dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaae2a6",
   "metadata": {},
   "source": [
    "We choose to assess structure conservation using a novel multiple structure alignment algorithm: FoldMason. It uses the structural alphabet from Foldseek to represent 3D structures as sequences, enabling fast comparison between large structure sets. The class `AlignmentFoldMason` is implemented for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7501505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.align_FoldMason import AlignmentFoldMason\n",
    "\n",
    "# Initialize\n",
    "aligner = AlignmentFoldMason(log_file=\"multiple_alignment_foldmason.log\")\n",
    "\n",
    "# Single multi-structure FoldMason run (optionally anchors with the template first)\n",
    "aligner.process_foldmason_alignment_multi(\n",
    "    pdb_path=\"Results/activation_segments/motif_filtered/\",\n",
    "    target_dir=\"Results/activation_segments/multi_aligned_foldmason/\",\n",
    "    template_pdb=\"6UAN_chainD.pdb\",  # omit if you don't want to include a template\n",
    "    out_name=\"msa\",                  # output prefix\n",
    "    report_mode=1                    # 0: no report, 1: HTML report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23837c15",
   "metadata": {},
   "source": [
    "Let's check that the number of structurally aligned files corresponds to the same number of files filtered for activation segment in the previous subsection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaab970",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_directory = 'Results/activation_segments/multi_aligned_foldmason/'\n",
    "pdb_count = count_pdb_files(pdb_directory, recursive=True)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ab771",
   "metadata": {},
   "source": [
    "We will be showing structure conservation with respect to the BRAF reference sequence. We have written the `analyse_alignment()` class to load the multi-structure alignment, calculate conservation at each BRAF residue position and select residues that fall within a certain conservation threshold (70%). We visualise this analysis as a histogram.\n",
    "\n",
    "**This class creates the `conservation` variable** that is used later in the feature selection workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.analyse_alignment_foldmason import analyse_alignment\n",
    "\n",
    "# Load multi-structure alignment\n",
    "analyser = analyse_alignment()\n",
    "multi_data = analyser.load_multi_alignment(\n",
    "    \"Results/activation_segments/multi_aligned_foldmason/msa_3di.fa\",\n",
    "    reference_name=\"6UAN_chainD\"  # Specify the template structure as reference\n",
    ")\n",
    "\n",
    "# Get residue names from reference structure (format: \"ALA-123\")\n",
    "reference_residues = braf_res()\n",
    "\n",
    "# Visualize conservation with residue labels on x-axis\n",
    "# Labels show: \"ALA449 (0)\" format (3-letter code + PDB number + 0-based index)\n",
    "conservation, highly_conserved = analyser.visualize_residue_conservation(\n",
    "    filtered_alignments=multi_data['structures'],\n",
    "    reference_residues=reference_residues,  # Pass residue names for x-axis labels\n",
    "    output_file=\"Results/multi_alignment_foldMason_conservation.png\",\n",
    "    show_plot=True\n",
    ")\n",
    "\n",
    "print(f\"Analyzed {len(multi_data['structures'])} structures (total: {multi_data['n_structures']})\")\n",
    "\n",
    "# Identify residues with conservation > 70%\n",
    "conservation_threshold = 0.70\n",
    "conserved_70_indices = np.where(conservation >= conservation_threshold)[0]\n",
    "\n",
    "print(f\"\\n=== Residues with ≥{conservation_threshold*100:.0f}% Conservation ===\")\n",
    "print(f\"Total conserved residues: {len(conserved_70_indices)}\")\n",
    "print(f\"\\nPositions and residues:\")\n",
    "for idx in conserved_70_indices:\n",
    "    if idx < len(reference_residues):\n",
    "        print(f\"  Position {idx:3d}: {reference_residues[idx]:>10s} - {conservation[idx]*100:.1f}% conserved\")\n",
    "    else:\n",
    "        print(f\"  Position {idx:3d}: (no reference) - {conservation[idx]*100:.1f}% conserved\")\n",
    "\n",
    "# Save to file\n",
    "conserved_df = pd.DataFrame({\n",
    "    'position': conserved_70_indices,\n",
    "    'residue': [reference_residues[i] if i < len(reference_residues) else 'N/A' for i in conserved_70_indices],\n",
    "    'conservation': [conservation[i] for i in conserved_70_indices]\n",
    "})\n",
    "conserved_df.to_csv('Results/conserved_residues_70percent.csv', index=False)\n",
    "print(f\"\\nSaved conserved residues to: Results/conserved_residues_70percent.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df090051",
   "metadata": {},
   "source": [
    "In order to assess the validity of our approach we can visualise how FoldMason aligns sequences by running the method `visualise_sequence_alignment()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6dd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.analyse_alignment_foldmason import visualise_sequence_alignment\n",
    "\n",
    "# Create visualizer instance\n",
    "visualizer = visualise_sequence_alignment()\n",
    "\n",
    "# Generate HTML for 3Di alignment\n",
    "di_stats = visualizer.generate_multi_alignment_html(\n",
    "    alignment_file=\"Results/activation_segments/multi_aligned_foldmason/msa_3di.fa\",\n",
    "    output_file=\"Results/multi_alignment_3di.html\",\n",
    "    reference_name=\"6UAN_chainD\"\n",
    ")\n",
    "\n",
    "print(f\"3Di alignment: {di_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07853425",
   "metadata": {},
   "source": [
    "## 2.5 Reconstructing small loop segments  <a id=\"25\"></a>\n",
    "Here we will be using homology modelling to reconstruct small missing residue regions within the activation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ceaccc",
   "metadata": {},
   "source": [
    "Many crystal structures exhibit missing residues in the activation loop since X-ray crystallography is not a useful technique to resolve disordered regions. We have written the class `ProteinReconstructor()` that extracts the full sequence from the original PDB file of each kinase domain, checks which structures require a reconstruction of less than 4 consecutive missing residues in the activation loop and utilises MODELLER to fill in missing residues with reasonable conformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a323b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.reconstruct import ProteinReconstructor\n",
    "\n",
    "# Configuration\n",
    "input_dir = \"Results/activation_segments/aligned_mda\"\n",
    "full_pdb_dir = \"Results/InterProPDBs\"\n",
    "output_dir = \"Results/activation_segments/reconstructedModeller\"\n",
    "max_gap_length = 4\n",
    "    \n",
    "# Create and run the reconstructor\n",
    "reconstructor = ProteinReconstructor(\n",
    "    input_dir=input_dir,\n",
    "    full_pdb_dir=full_pdb_dir,\n",
    "    output_dir=output_dir,\n",
    "    max_gap_length=max_gap_length\n",
    ")\n",
    "    \n",
    "reconstructor.run_modeller_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a61d5",
   "metadata": {},
   "source": [
    "We should now check how many reconstructed structures we are left with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dd0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_directory = 'Results/activation_segments/reconstructedModeller/'\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff12ed",
   "metadata": {},
   "source": [
    "## 2.6 Coarse-graining activation loops <a id=\"26\"></a>\n",
    "Here we present a method to coarse-grain activation loops and represent them with an equal number of coordinates independently from the length of the loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cbcc20",
   "metadata": {},
   "source": [
    "The class `CAStripper` provides means to save to a new folder only the coordinates of the Cα atoms of each activation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debcbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.ca_stripper import CAStripper\n",
    "\n",
    "stripper = CAStripper(motifs=['DFG', 'APE'])\n",
    "\n",
    "# Process a directory\n",
    "output_dir = stripper.strip_to_ca(\n",
    "    input_dir=\"Results/activation_segments/reconstructedModeller/\", \n",
    "    output_dir=\"Results/activation_segments/CA_segments/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81418b",
   "metadata": {},
   "source": [
    "Let's make sure that the number of structures being processed has not decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a474eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_directory = 'Results/activation_segments/CA_segments/'\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8b983",
   "metadata": {},
   "source": [
    "In the next two filtering steps we will be using the `OutlierStripper` class to retain a dataset of well-aligned and similarly-sized loop structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be5a1f",
   "metadata": {},
   "source": [
    "We are first going to apply Tukey's method to exclude activation loop structures characterised by a number of Cα atoms that lies outside the interquartile range of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e995ee08",
   "metadata": {},
   "source": [
    "At this point it would be useful to filter out loops whose extremities are not structurally aligned to the extremities of the reference BRAF structure. This is to minimise the impact of the lack of roto-translational invariance on the dimensionality reduction performed later. In order to accomplish this, we will be looking at the RMSD between the extremities of each structure and the ones of the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839754c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.ca_stripper import OutlierStripper\n",
    "\n",
    "# Load your reference structure\n",
    "ref_traj = md.load(\"6UAN_chainD.pdb\")\n",
    "ca_indices = ref_traj.topology.select(\"name CA\")\n",
    "\n",
    "# Initialize OutlierStripper with reference PDB for distance filtering\n",
    "distance_outlier_detector = OutlierStripper(\n",
    "    k_factor=1.5,\n",
    "    reference_pdb=\"6UAN_chainD.pdb\",\n",
    "    ref_first_resid=144,   # First residue ID\n",
    "    ref_last_resid=173     # Last residue ID\n",
    ")\n",
    "\n",
    "# Analyze with both CA count AND distance filtering\n",
    "final_results = distance_outlier_detector.analyze(\n",
    "    ca_segments_dir=\"Results/activation_segments/CA_segments/\",\n",
    "    create_plots=True,  # This creates histograms AND violin plots\n",
    "    distance_cutoff=5.0,\n",
    "    apply_distance_filter=True,\n",
    "    clean_dir_name=\"CA_segments_final_cleaned\"\n",
    ")\n",
    "\n",
    "print(f\"Final cleaned structures saved to: {final_results['clean_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe3aed",
   "metadata": {},
   "source": [
    "Let's again make sure the number of files retained after this filtering step is right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_directory = 'Results/activation_segments/CA_segments/CA_segments_final_cleaned'\n",
    "pdb_count = count_pdb_files(pdb_directory)\n",
    "\n",
    "print(f\"There are {pdb_count} PDB files in the directory '{pdb_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13521b5e",
   "metadata": {},
   "source": [
    "### 2.5.2 Cα interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17f312",
   "metadata": {},
   "source": [
    "We now focus on preparing the input to the dimensionality reduction algorithms chosen. The issue we have at present is that we are dealing with heterogeneity in the number of atoms of each input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c63e54",
   "metadata": {},
   "source": [
    "We utilise the class `Fitting()` to obtain a uniform representation of our dataset by fitting cubic splines to the carbon alphas of our structures and then sampling the path obtained an equal amount of times corresponding to the median of the histogram shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.fitting_class import Fitting\n",
    "\n",
    "# Process all PDB files in a directory\n",
    "input_directory = \"Results/activation_segments/CA_segments/CA_segments_final_cleaned/\" # PROBLEM it should be Results/activation_segments/CA_segments/CA_segments_final_cleaned/ \n",
    "output_directory = 'Results/activation_segments/fitted'\n",
    "\n",
    "# Initialise\n",
    "fitter = Fitting()\n",
    "\n",
    "# This will fit all structures and create comparison plots\n",
    "fitter.process_directory(\n",
    "    input_dir=input_directory,\n",
    "    output_dir=output_directory,\n",
    "    create_plots=True,  # Set to False if you don't want plots\n",
    "    plot_dir='Results/activation_segments/plots'  # Optional: specify plot directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670258d9",
   "metadata": {},
   "source": [
    "# 3. Dimensionality reduction  <a id=\"3\"></a>\n",
    "In this section we will apply dimensionality reduction to our coarse-grained representation of selected activation loops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0251bd",
   "metadata": {},
   "source": [
    "## 3.1 Principal component analysis (PCA)  <a id=\"31\"></a>\n",
    "We are now going to perform Principal Component Analysis (PCA) in order to reduce the dimensionality of our coarse-grained dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c6e79",
   "metadata": {},
   "source": [
    "We have written the class `PCAWorkflow()` in order to apply PCA to our activation loop dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffabeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.pca_analysis import PCAWorkflow\n",
    "\n",
    "workflow = PCAWorkflow(n_components=8, n_clusters=2)\n",
    "results = workflow.run_full_analysis(\n",
    "    structures_path=\"Results/activation_segments/fitted/\",\n",
    "    output_prefix=\"my_analysis\"\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"Structures: {len(results['structure_names'])}\")\n",
    "print(f\"PC1: {results['explained_variance'][0]:.1f}% variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9113784",
   "metadata": {},
   "source": [
    "We now use the class `ClusterAnalyzer()` in order to visualise how our dataset projects along the first two principal components. We cluster in this reduced space and obtain two labels for two clusters of projected conformations. We also visualise how active and inactive labels from KinCore project in PC space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.pca_analysis import ClusterAnalyzer\n",
    "\n",
    "cluster_analyzer = ClusterAnalyzer(n_clusters=2)\n",
    "\n",
    "_ = cluster_analyzer.plot_pca_cluster_and_activation(\n",
    "    results,\n",
    "    kincore_file=\"Results/dunbrack_assignments/kinase_conformation_assignments.csv\",\n",
    "    cluster_plot_path=\"pca_clustering_labels.png\",\n",
    "    activation_plot_path=\"pca_activation_states.png\",\n",
    "    show=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896c46f",
   "metadata": {},
   "source": [
    "## 3.2 Convolutional autoencoder  <a id=\"32\"></a>\n",
    "We are now going to train a small Convolutional Autoencoder (CNN) in order to reduce the dimensionality of our coarse-grained dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827af99",
   "metadata": {},
   "source": [
    "We will be exploiting the framework named `molearn` for training a CNN on activation loop coarse-grained conformations. To facilitate running all the steps required by this package for training, we have written the class `AutoencoderWorkflow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.autoencoder_workflow import AutoencoderWorkflow\n",
    "\n",
    "# Create workflow\n",
    "myworkflow = AutoencoderWorkflow(\n",
    "    folder_name='Results/activation_segments/fitted',\n",
    "    output_base_dir='Results/run_trial_BRAFActivationLoop_postalign_checkpoint0',\n",
    "    manual_seed=25,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45bff6",
   "metadata": {},
   "source": [
    "Let's prepare the data for input into training and let's train our model for 32 consecutive epochs with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "myworkflow.prepare_data(atom_selection=['CA'])\n",
    "myworkflow.train(max_epochs=32, patience=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b67ba",
   "metadata": {},
   "source": [
    "After training our Autoencoder we are going to load the trained model so that we can perform some analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "myworkflow.load_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ff06f",
   "metadata": {},
   "source": [
    "Let's now initialise the analysis class and decode structures in order to be able to quantify the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a24376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run analysis\n",
    "myworkflow.setup_analysis()\n",
    "myworkflow.extract_dataset()\n",
    "myworkflow.decode_structures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1022d63",
   "metadata": {},
   "source": [
    "Since the data gets shuffled before input into our model we are going to keep track of the shuffled indices to facilitate our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "myworkflow.rename_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51aa29a",
   "metadata": {},
   "source": [
    "We can now save the learnt latent representation for future visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7982b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "myworkflow.calculate_errors()\n",
    "myworkflow.scan_error_landscape()\n",
    "myworkflow.extract_encoded_coordinates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0d6d1",
   "metadata": {},
   "source": [
    "We can also load the PCA labels obtained in the previous section in order to visualise how they project in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCA cluster labels \n",
    "pca_labels_file = 'cluster_labels_my_analysis_hierarchical.txt'\n",
    "myworkflow.load_external_labels(pca_labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6ab07",
   "metadata": {},
   "source": [
    "Finally let's visualise how both training and validation data project in latent space and what is the distribution of PCA and activation labels over the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf75af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 1: Latent space colored by PCA cluster labels ---\n",
    "myworkflow.plot_latent_space(\n",
    "    title=\"Latent Space Projection (PCA Cluster Labels)\",\n",
    "    output_file='latent_space_pca_labels.png'\n",
    ")\n",
    "\n",
    "# --- Plot 2: Latent space colored by KinCore activation state labels ---\n",
    "# Load activation state labels from KinCore classification\n",
    "myworkflow.load_kincore_labels(\n",
    "    kincore_file='Results/dunbrack_assignments/kinase_conformation_assignments.csv'\n",
    ")\n",
    "\n",
    "# Plot with activation labels (pass them explicitly)\n",
    "# Use red for inactive (0) and green for active (1)\n",
    "myworkflow.plot_latent_space(\n",
    "    labels=(myworkflow.activation_labels_train, myworkflow.activation_labels_valid),\n",
    "    title=\"Latent Space Projection (Activation States)\",\n",
    "    output_file='latent_space_activation_states.png',\n",
    "    activation_colors=['red', 'green']  # [inactive, active]\n",
    ")\n",
    "\n",
    "# Organize structures by PCA cluster labels\n",
    "myworkflow.organize_by_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db435a1",
   "metadata": {},
   "source": [
    "Let's now investigate whether there is a correlation between the labels assigned by Dunbrack and the ones obtained through clustering in PC space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.pca_analysis import ClusterAnalyzer\n",
    "\n",
    "cluster_analyzer = ClusterAnalyzer(n_clusters=2)\n",
    "\n",
    "out = cluster_analyzer.integrate_dunbrack_with_pca_clusters(\n",
    "    pca_labels_file=\"cluster_labels_my_analysis_hierarchical.txt\",\n",
    "    dunbrack_assignments_csv=\"Results/dunbrack_assignments/kinase_conformation_assignments.csv\",\n",
    "    prefix_len=6,\n",
    "    merged_output_csv=\"Results/dunbrack_assignments/pca_dunbrack_merged.csv\",\n",
    "    print_tables=True,\n",
    "    print_percentages=True,\n",
    ")\n",
    "\n",
    "# keep the merged dataframe available for downstream cells\n",
    "merged = out[\"merged\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_analyzer = ClusterAnalyzer(n_clusters=2)\n",
    "\n",
    "out = cluster_analyzer.analyze_cluster_vs_activity_status(\n",
    "    merged_csv=\"Results/dunbrack_assignments/pca_dunbrack_merged.csv\",\n",
    "    print_tables=True,\n",
    "    print_percentages=True,\n",
    "    print_enrichment=True,\n",
    "    enrichment_threshold_pct=10.0,\n",
    ")\n",
    "\n",
    "# keep commonly used objects in the notebook namespace\n",
    "merged = out[\"merged\"]\n",
    "merged_clean = out[\"merged_clean\"]\n",
    "activity_crosstab = out[\"activity_crosstab\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b10175",
   "metadata": {},
   "source": [
    "It can be useful to visualise if there is a correlation with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fb8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_analyzer = ClusterAnalyzer(n_clusters=2)\n",
    "\n",
    "out = cluster_analyzer.plot_cluster_vs_activation_state_heatmap(\n",
    "    merged_csv=\"Results/dunbrack_assignments/pca_dunbrack_merged.csv\",\n",
    "    output_png=\"Results/dunbrack_assignments/pca_activation_correlation_plot.png\",\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "# keep commonly used objects in the notebook namespace\n",
    "merged = out[\"merged\"]\n",
    "merged_clean = out[\"merged_clean\"]\n",
    "activation_crosstab = out[\"activation_crosstab\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81549bf0",
   "metadata": {},
   "source": [
    "# 4. Feature definition  <a id=\"4\"></a>\n",
    "In this section we will featurise the kinase domains in our dataset and select only features of statistical relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab71727",
   "metadata": {},
   "source": [
    "## 4.1 Feature matrix  <a id=\"41\"></a>\n",
    "Here we define our featurisation approach. We construct a feature matrix for each structure in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461dc34a",
   "metadata": {},
   "source": [
    "Let's first import and initialise the class `FeatureSelection()` that we use to perform all the necessary steps to featurise kinase domains outside the activation loop region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd04a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.feature_selection import FeatureSelection\n",
    "\n",
    "# Initialize the feature selection object\n",
    "fs = FeatureSelection(\n",
    "    dfg_index=145,  # Position of DFG motif\n",
    "    ape_index=174,  # Position of APE motif\n",
    "    conservation_threshold=0.70  # 70% conservation threshold\n",
    ")\n",
    "\n",
    "print(\"FeatureSelection initialized\")\n",
    "print(f\"Activation loop region: {fs.dfg_index} to {fs.ape_index}\")\n",
    "print(f\"Conservation threshold: {fs.conservation_threshold * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7991f09",
   "metadata": {},
   "source": [
    "The first step uses the conservation data to identify residues that are:\n",
    "- Conserved in ≥70% of structures\n",
    "- Located outside the activation loop region (DFG to APE motif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb2127-3d7a-4463-a474-e718643796ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>PRE-REQUISITE:</b> You must first run the conservation analysis cell which calculates the <code>conservation</code> variable by analyzing the multi-structure alignment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if conservation variable exists\n",
    "try:\n",
    "    conservation\n",
    "except NameError:\n",
    "    raise NameError(\n",
    "        \"The 'conservation' variable is not defined. \"\n",
    "        \"Please run Cell 48 first to calculate conservation from the multi-structure alignment.\\n\"\n",
    "        \"Cell 48 runs: analyser.visualize_residue_conservation()\"\n",
    "    )\n",
    "\n",
    "# Load reference residue names\n",
    "reference_residues = braf_res()\n",
    "\n",
    "# Identify conserved residues (uses 70% threshold set in FeatureSelection initialization)\n",
    "# This will find residues that are:\n",
    "# - Conserved in ≥70% of structures\n",
    "# - Located OUTSIDE the activation loop (not between DFG at 145 and APE at 174)\n",
    "fs.identify_conserved_residues(\n",
    "    conservation=conservation,\n",
    "    reference_residues=reference_residues\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(fs.fully_conserved)} conserved residues (≥70% conservation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24342d5d",
   "metadata": {},
   "source": [
    "Before calculating distance features, we need to organize structures into cluster-specific folders based on the PCA clustering results in order to be able to easily retain the label of each structure in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d83785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PCA cluster labels (use hierarchical clustering results)\n",
    "pca_labels_file = 'cluster_labels_my_analysis_hierarchical.txt'\n",
    "df_labels = pd.read_csv(pca_labels_file, skiprows=1, header=None, \n",
    "                        names=['ClusterLabel', 'PDBCode', 'FullName'])\n",
    "\n",
    "# Create output directories for each cluster (clearing old structures)\n",
    "cluster0_dir = \"Results/activation_segments/structuresToFeaturiseCluster0/\"\n",
    "cluster1_dir = \"Results/activation_segments/structuresToFeaturiseCluster1/\"\n",
    "clear_and_make(cluster0_dir)\n",
    "clear_and_make(cluster1_dir)\n",
    "\n",
    "# Source directory with aligned structures\n",
    "source_dir = \"Results/activation_segments/unaligned/\"\n",
    "\n",
    "# Organize structures by cluster\n",
    "copied_cluster0 = []\n",
    "copied_cluster1 = []\n",
    "missing_files = []\n",
    "\n",
    "for _, row in df_labels.iterrows():\n",
    "    cluster_label = int(row['ClusterLabel'])\n",
    "    structure_name = row['FullName']\n",
    "    \n",
    "    # Ensure .pdb extension\n",
    "    if not structure_name.endswith('.pdb'):\n",
    "        structure_name = structure_name + '.pdb'\n",
    "    \n",
    "    # Extract first 6 characters for matching (PDB code)\n",
    "    pdb_prefix = structure_name[:6]\n",
    "    \n",
    "    # Find matching file in source directory using prefix\n",
    "    # Look for any file that starts with the 6-character prefix\n",
    "    matching_files = [f for f in os.listdir(source_dir) \n",
    "                     if f.startswith(pdb_prefix) and f.endswith('.pdb')]\n",
    "    \n",
    "    if not matching_files:\n",
    "        missing_files.append(structure_name)\n",
    "        continue\n",
    "    \n",
    "    # Use the first matching file\n",
    "    source_file = os.path.join(source_dir, matching_files[0])\n",
    "    \n",
    "    # Copy to appropriate cluster folder (keep original name from PCA labels)\n",
    "    if cluster_label == 0:\n",
    "        dest_file = os.path.join(cluster0_dir, matching_files[0])\n",
    "        shutil.copy2(source_file, dest_file)\n",
    "        copied_cluster0.append(matching_files[0])\n",
    "    elif cluster_label == 1:\n",
    "        dest_file = os.path.join(cluster1_dir, matching_files[0])\n",
    "        shutil.copy2(source_file, dest_file)\n",
    "        copied_cluster1.append(matching_files[0])\n",
    "\n",
    "print(f\"=== PCA Cluster Organization ===\")\n",
    "print(f\"Matching files using first 6 characters of structure name\")\n",
    "print(f\"Source directory: {source_dir}\")\n",
    "print(f\"\\nCluster 0: {len(copied_cluster0)} structures → {cluster0_dir}\")\n",
    "print(f\"Cluster 1: {len(copied_cluster1)} structures → {cluster1_dir}\")\n",
    "print(f\"Total structures organized: {len(copied_cluster0) + len(copied_cluster1)}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n⚠️  Warning: {len(missing_files)} files not found in {source_dir}\")\n",
    "    print(\"First 5 missing files (showing first 6 chars used for matching):\")\n",
    "    for f in missing_files[:5]:\n",
    "        print(f\"  - {f[:6]}* (from {f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e93574",
   "metadata": {},
   "source": [
    "We can now calculate pairwise distances between conserved residues for structures in each cluster which will constitute our feature dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060b181a",
   "metadata": {},
   "source": [
    "We calculate distances first for the structures in cluster 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5962409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your aligned structures and alignment function\n",
    "aligned_structures = multi_data['structures']  # Your list of alignment objects\n",
    "\n",
    "# Choose which cluster to analyze\n",
    "# Options: cluster0_dir or cluster1_dir (defined in previous cell)\n",
    "# Cluster 0 is typically the inactive state, Cluster 1 is active state (or vice versa)\n",
    "pdb_directory = cluster0_dir  # Change to cluster1_dir to analyze the other cluster\n",
    "\n",
    "print(f\"Analyzing structures from: {pdb_directory}\")\n",
    "print(f\"Number of structures: {len(os.listdir(pdb_directory))}\")\n",
    "\n",
    "# Calculate distances between conserved residues\n",
    "distance_df_cluster0 = fs.calculate_intra_structure_distances(\n",
    "    aligned_structures=aligned_structures,\n",
    "    pdb_directory=pdb_directory,\n",
    "    alignment_function=make_seg  # Your function to create alignment segment\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Distance Calculation Results (Cluster 0) ===\")\n",
    "print(f\"Calculated {len(distance_df_cluster0)} distance measurements\")\n",
    "print(f\"Across {len(fs.structures)} structures\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "distance_df_cluster0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb74969",
   "metadata": {},
   "source": [
    "Then we calculate distances for structures in cluster 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f27a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances for Cluster 1\n",
    "pdb_directory_cluster1 = cluster1_dir\n",
    "\n",
    "print(f\"Analyzing structures from: {pdb_directory_cluster1}\")\n",
    "print(f\"Number of structures: {len(os.listdir(pdb_directory_cluster1))}\")\n",
    "\n",
    "# Calculate distances between conserved residues for Cluster 1\n",
    "distance_df_cluster1 = fs.calculate_intra_structure_distances(\n",
    "    aligned_structures=aligned_structures,\n",
    "    pdb_directory=pdb_directory_cluster1,\n",
    "    alignment_function=make_seg\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Distance Calculation Results (Cluster 1) ===\")\n",
    "print(f\"Calculated {len(distance_df_cluster1)} distance measurements\")\n",
    "print(f\"Across {len(fs.structures)} structures\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "distance_df_cluster1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713a2ce",
   "metadata": {},
   "source": [
    "Now we can create a dataset of all the features extracted from our kinase domains which will be later used in classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine distance measurements from both clusters\n",
    "combined_distance_df = pd.concat([distance_df_cluster0, distance_df_cluster1], \n",
    "                                  ignore_index=True)\n",
    "\n",
    "# Update fs.intra_structure_df with combined data\n",
    "fs.intra_structure_df = combined_distance_df\n",
    "\n",
    "print(f\"=== Combined Distance Data ===\")\n",
    "print(f\"Cluster 0: {len(distance_df_cluster0)} measurements from {distance_df_cluster0['structure'].nunique()} structures\")\n",
    "print(f\"Cluster 1: {len(distance_df_cluster1)} measurements from {distance_df_cluster1['structure'].nunique()} structures\")\n",
    "print(f\"Combined: {len(combined_distance_df)} total measurements from {combined_distance_df['structure'].nunique()} structures\")\n",
    "print(f\"\\nSample of combined data:\")\n",
    "print(combined_distance_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabd416",
   "metadata": {},
   "source": [
    "Let's add a column to our feature dataset in order to be able to track what labels are associated with what structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2376dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cluster directories are defined (from Cell 105)\n",
    "try:\n",
    "    cluster0_dir\n",
    "    cluster1_dir\n",
    "except NameError:\n",
    "    # If not defined, set them to the expected paths\n",
    "    print(\"⚠️  WARNING: cluster0_dir and cluster1_dir not found in environment.\")\n",
    "    print(\"Please run Cell 105 first to organize structures by PCA clusters.\")\n",
    "    print(\"Using default paths as fallback...\\n\")\n",
    "    \n",
    "    cluster0_dir = \"Results/activation_segments/structuresToFeaturiseCluster0/\"\n",
    "    cluster1_dir = \"Results/activation_segments/structuresToFeaturiseCluster1/\"\n",
    "\n",
    "# Use the cluster directories created from PCA analysis\n",
    "cluster_dirs = {\n",
    "    0: cluster0_dir,  # \"Results/activation_segments/structuresToFeaturiseCluster0/\"\n",
    "    1: cluster1_dir   # \"Results/activation_segments/structuresToFeaturiseCluster1/\"\n",
    "}\n",
    "\n",
    "print(\"Using PCA-based cluster directories:\")\n",
    "print(f\"  Cluster 0: {cluster_dirs[0]}\")\n",
    "print(f\"  Cluster 1: {cluster_dirs[1]}\")\n",
    "\n",
    "# Assign labels based on cluster membership\n",
    "fs.assign_labels_from_clusters(cluster_dirs)\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\n=== Label Distribution in Dataset ===\")\n",
    "if fs.intra_structure_df is not None and 'label' in fs.intra_structure_df.columns:\n",
    "    # IMPORTANT: Each structure has MANY rows (one per residue pair distance)\n",
    "    # So we need to count both rows AND unique structures\n",
    "    \n",
    "    print(\"📊 Unique structures per label:\")\n",
    "    for label in sorted(fs.intra_structure_df['label'].unique()):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        n_structures = fs.intra_structure_df[fs.intra_structure_df['label'] == label]['structure'].nunique()\n",
    "        n_measurements = len(fs.intra_structure_df[fs.intra_structure_df['label'] == label])\n",
    "        print(f\"  Label {label}: {n_structures} structures ({n_measurements} distance measurements)\")\n",
    "    \n",
    "    total_unique = fs.intra_structure_df[fs.intra_structure_df['label'] != -1]['structure'].nunique()\n",
    "    total_measurements = len(fs.intra_structure_df[fs.intra_structure_df['label'] != -1])\n",
    "    print(f\"\\n✅ Total: {total_unique} labeled structures, {total_measurements} total distance measurements\")\n",
    "    \n",
    "    if (fs.intra_structure_df['label'] == -1).any():\n",
    "        unlabeled = fs.intra_structure_df[fs.intra_structure_df['label'] == -1]['structure'].nunique()\n",
    "        print(f\"⚠️  Warning: {unlabeled} structures without labels\")\n",
    "else:\n",
    "    print(\"No labels assigned yet. Run distance calculation and combination first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96f644",
   "metadata": {},
   "source": [
    "We can now organise all distance measurements into a structured feature matrix for each structure. Since not all residues between which we compute distances are conserved we use median imputation to ensure all feature matrices include the same number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad59299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature matrix with median imputation\n",
    "feature_matrix, imputation_mask = fs.build_feature_matrix(\n",
    "    use_median_imputation=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"Number of structures: {len(fs.structure_names)}\")\n",
    "print(f\"Number of features: {len(fs.unique_pairs)}\")\n",
    "print(f\"\\nSample feature names: {[f'{p[0]}-{p[1]}' for p in fs.unique_pairs[:5]]}\")\n",
    "\n",
    "# Save all results for later reloading\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving feature matrix and related data...\")\n",
    "print(\"=\"*60)\n",
    "fs.save_results(output_prefix=\"\")\n",
    "print(\"✅ All data saved! Can be reloaded without recomputing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c24801",
   "metadata": {},
   "source": [
    "Let's visualise some feature matrices as heat maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ffc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example heatmaps\n",
    "fs.plot_distance_heatmaps(\n",
    "    n_examples=4,\n",
    "    save_dir=None  # Set to a directory path to save all heatmaps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcdc2bf",
   "metadata": {},
   "source": [
    "# 5. Feature selection  <a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3fe66a-3dd3-499e-837b-1ed4684fdde3",
   "metadata": {},
   "source": [
    "In this section we will study how each feature is distributed across our kinase dataset and we will filter out features that are not statistically relevant in order to facilitate the classification step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d1d96",
   "metadata": {},
   "source": [
    "If you've already computed feature matrices and saved them, you can skip the previous cells and reload the data here using the class `FeatureSelection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflow.feature_selection import FeatureSelection\n",
    "\n",
    "# Create new FeatureSelection object\n",
    "fs = FeatureSelection(dfg_index=145, ape_index=174, conservation_threshold=0.97)\n",
    "\n",
    "# Load reference data\n",
    "fs.load_results('reference_data.pkl')\n",
    "\n",
    "# Load feature matrix\n",
    "feature_df = pd.read_csv('feature_matrix.csv', index_col=0)\n",
    "fs.feature_matrix = feature_df.values\n",
    "fs.structure_names = list(feature_df.index)\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv('labels.csv')\n",
    "fs.labels = labels_df['label'].values\n",
    "\n",
    "# Load distance dataframe\n",
    "fs.intra_structure_df = pd.read_csv('intra_structure_distances.csv')\n",
    "\n",
    "# Calculate statistics\n",
    "final_shape = fs.feature_matrix.shape\n",
    "final_total = fs.feature_matrix.size\n",
    "final_valid = np.sum(~np.isnan(fs.feature_matrix))\n",
    "\n",
    "print(f\"\\n✅ Reloaded successfully!\")\n",
    "print(f\"\\n📊 Feature matrix:\")\n",
    "print(f\"   Matrix shape: {final_shape[0]:,} structures × {final_shape[1]:,} residue pairs\")\n",
    "print(f\"   Total entries: {final_total:,}\")\n",
    "print(f\"   Valid measurements: {final_valid:,}\")\n",
    "print(f\"   NaN values: {final_total - final_valid:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b30b27",
   "metadata": {},
   "source": [
    "We first look for abnormally large features that might indicate structural issues and exclude them from the feature matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prerequisites\n",
    "try:\n",
    "    fs\n",
    "    if fs.feature_matrix is None:\n",
    "        raise ValueError(\"Feature matrix not built yet. Run Cell 113 first.\")\n",
    "except NameError:\n",
    "    raise NameError(\"FeatureSelection object 'fs' not defined. Run Cell 102 first.\")\n",
    "\n",
    "# One-liner replacement for the long outlier/NaN-cleaning snippet.\n",
    "# - sets >50Å distances to NaN\n",
    "# - drops all-NaN features/structures\n",
    "# - saves outlier table to CSV\n",
    "results = fs.filter_outlier_distances_and_drop_nan(\n",
    "    threshold=50.0,\n",
    "    set_to_nan=True,\n",
    "    max_nan_fraction=1.0,\n",
    "    outliers_csv_path=\"outlier_distances.csv\",\n",
    "    print_top_n=10,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feaa7ab",
   "metadata": {},
   "source": [
    "We now filter out all features that are defined between consecutive residues, these are most likely highly-correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out features with consecutive residue indices\n",
    "# These features (e.g., 130-131) are not informative since consecutive residues\n",
    "# are always close together in the protein structure\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FILTERING CONSECUTIVE RESIDUE FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count features before filtering\n",
    "features_before = len(fs.unique_pairs)\n",
    "print(f\"\\nFeatures before filtering: {features_before}\")\n",
    "\n",
    "# Find and remove consecutive residue pairs\n",
    "consecutive_features = fs.filter_consecutive_residues(remove=True)\n",
    "\n",
    "# Show final count\n",
    "features_after = len(fs.unique_pairs)\n",
    "print(f\"\\n📊 Final feature count: {features_after}\")\n",
    "print(f\"   Features removed: {features_before - features_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51fa090",
   "metadata": {},
   "source": [
    "We then specifically look for highly-correlated features, construct feature classess of correlated features and pick a representative feature from each based on the largest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1950d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation-based feature selection\n",
    "# Identify groups of highly correlated features and keep only the feature\n",
    "# with the highest standard deviation from each group\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION-BASED FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Speed tips:\n",
    "# - If you have no NaN values, correlation will be much faster (uses numpy's corrcoef)\n",
    "# - use_parallel=True enables parallel processing (2-8x faster with NaN values)\n",
    "# - Set plot_histogram=False to skip plotting\n",
    "# - Increase correlation_threshold (e.g., 0.95) to find fewer groups\n",
    "\n",
    "print(f\"Current feature matrix shape: {fs.feature_matrix.shape}\")\n",
    "print(f\"Has NaN values: {np.any(np.isnan(fs.feature_matrix))}\")\n",
    "\n",
    "# Perform correlation analysis\n",
    "# If parallel processing has issues, set use_parallel=False to use the safe sequential method\n",
    "selected_features, analysis_info = fs.filter_correlated_features(\n",
    "    correlation_threshold=0.90,  # Higher threshold = fewer correlated groups = faster\n",
    "    plot_histogram=True,          # Set to False to skip plotting\n",
    "    plot_network=False,           # Set to True to see the correlation network (slow for many features)\n",
    "    use_parallel=True,            # Set to False if you encounter issues with parallel processing\n",
    "    n_jobs=-1                     # Use all CPU cores (-1), or specify number (e.g., 4)\n",
    ")\n",
    "\n",
    "# Apply the selection to the feature matrix\n",
    "fs.apply_feature_selection(selected_features)\n",
    "\n",
    "print(\"\\n✅ Correlation-based feature selection complete!\")\n",
    "\n",
    "# Save intermediate results (after correlation selection)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving correlation-filtered feature matrix...\")\n",
    "print(\"=\"*60)\n",
    "fs.save_results(output_prefix=\"corr_filtered_\")\n",
    "print(\"\\n✅ Saved correlation-filtered results!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eeed32",
   "metadata": {},
   "source": [
    "If you've already run correlation-based feature selection and want to skip directly to same-mean and variance filtering, you can run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb351e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = FeatureSelection(dfg_index=145, ape_index=174, conservation_threshold=0.97)\n",
    "fs.load_results('corr_filtered_reference_data.pkl')\n",
    "\n",
    "feature_df = pd.read_csv('corr_filtered_feature_matrix.csv', index_col=0)\n",
    "fs.feature_matrix = feature_df.values\n",
    "fs.structure_names = list(feature_df.index)\n",
    "\n",
    "labels_df = pd.read_csv('corr_filtered_labels.csv')\n",
    "fs.labels = labels_df['label'].values\n",
    "\n",
    "fs.intra_structure_df = pd.read_csv('corr_filtered_intra_structure_distances.csv')\n",
    "\n",
    "print(f\"✅ Reloaded correlation-filtered data!\")\n",
    "print(f\"   Matrix shape: {fs.feature_matrix.shape}\")\n",
    "print(f\"   Number of features: {len(fs.unique_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d0b92",
   "metadata": {},
   "source": [
    "We now further reduce the feature space by filtering out features that have the same mean (within 0.01 Å) and pick the feature with highest variance from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc1723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features with the same mean\n",
    "# Keeps only the feature with highest standard deviation from each mean group\n",
    "\n",
    "same_mean_info = fs.filter_same_mean_features(\n",
    "    mean_tolerance=0.01,  # Features within 0.01 Å mean are considered \"same\"\n",
    "    remove=True\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Same-mean filtering complete!\")\n",
    "print(f\"   Mean groups found: {same_mean_info['n_groups']}\")\n",
    "print(f\"   Features removed: {same_mean_info['n_removed']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96287a4",
   "metadata": {},
   "source": [
    "We further reduce the feature space by filtering out features that have low variance (< 0.1 Å)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3582c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features with low variance\n",
    "# Removes features that don't vary much across structures\n",
    "\n",
    "selected_variance_indices = fs.filter_low_variance_features(\n",
    "    variance_threshold=0.1,  # Remove features with variance < 0.1\n",
    "    remove=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Low variance filtering complete!\")\n",
    "print(f\"   Final feature count: {len(fs.unique_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2fbbd",
   "metadata": {},
   "source": [
    "Finally, we exploit the ANOVA SUM method to select a sub-set of statistically-relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78741bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features using ANOVA F-value\n",
    "# Keeps top N features that best distinguish between classes (active vs inactive)\n",
    "\n",
    "selected_anova_indices = fs.filter_anova_features(\n",
    "    n_features=300,      # Keep top 300 features\n",
    "    plot_scores=False,   # Set to True to see F-value distribution\n",
    "    remove=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ ANOVA F-value filtering complete!\")\n",
    "print(f\"   Final feature count: {len(fs.unique_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0dbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute any remaining NaN values before saving and classification\n",
    "fs.impute_remaining_nan(strategy='median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99de28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fully filtered feature matrix and related data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING FILTERED FEATURE MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFinal feature matrix shape: {fs.feature_matrix.shape}\")\n",
    "print(f\"  Structures: {fs.feature_matrix.shape[0]}\")\n",
    "print(f\"  Features: {fs.feature_matrix.shape[1]}\")\n",
    "\n",
    "fs.save_results(output_prefix=\"filtered_\")\n",
    "print(\"\\n✅ Saved filtered results! Can be reloaded for downstream analysis.\")\n",
    "\n",
    "# Print filtering summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILTERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Applied filters in order:\")\n",
    "print(\"  1. ✓ Outlier distances (>50Å)\")\n",
    "print(\"  2. ✓ All-NaN features/structures\")\n",
    "print(\"  3. ✓ Consecutive residue pairs\")\n",
    "print(\"  4. ✓ Correlation-based selection (r > 0.90)\")\n",
    "print(\"  5. ✓ Same-mean features (tolerance 0.01Å)\")\n",
    "print(\"  6. ✓ Low variance features (threshold 0.1)\")\n",
    "print(\"  7. ✓ ANOVA F-value selection (top 300)\")\n",
    "print(f\"\\nFinal: {fs.feature_matrix.shape[0]} structures × {fs.feature_matrix.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b054c",
   "metadata": {},
   "source": [
    "# 6. Feature classification  <a id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf011ba-e51b-49b2-92a6-ed911473b28b",
   "metadata": {},
   "source": [
    "In this section we will train and analyse Random Forest (RF) classifier to investigate what features are most significant in predicting predominant activation loop conformational changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a37f8",
   "metadata": {},
   "source": [
    "The class `FeatureClassification` facilitates running all the steps required to train a RF classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c88b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Analysis using FeatureClassification class\n",
    "from workflow.feature_classification import FeatureClassification\n",
    "\n",
    "# Use the filtered feature matrix and labels from feature selection (full dataset, no balancing)\n",
    "classifier = FeatureClassification(\n",
    "    feature_matrix=fs.feature_matrix,\n",
    "    labels=fs.labels,\n",
    "    unique_pairs=fs.unique_pairs,\n",
    "    fully_conserved=fs.fully_conserved,\n",
    "    structure_names=fs.structure_names\n",
    ")\n",
    "\n",
    "print(f\"✅ FeatureClassification initialized\")\n",
    "print(f\"   Features: {len(classifier.unique_pairs)}\")\n",
    "print(f\"   Structures: {len(classifier.labels)}\")\n",
    "print(f\"   Classes: {np.unique(classifier.labels)}\")\n",
    "print(f\"   Class distribution: {dict(zip(*np.unique(classifier.labels, return_counts=True)))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ad3ec",
   "metadata": {},
   "source": [
    "Let's first split the data into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0556a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split data into train/test sets\n",
    "classifier.split_data(train_size=0.9, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30066f",
   "metadata": {},
   "source": [
    "We can now train the model with our input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61419085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Train Random Forest model\n",
    "classifier.train_model(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173c0d74",
   "metadata": {},
   "source": [
    "Let's evaluate model performance and visualise it with a confusion matrix to make sure our classifier is able to deal with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate model performance\n",
    "metrics = classifier.evaluate_model()\n",
    "\n",
    "# Step 4: Plot confusion matrix\n",
    "cm = classifier.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b69b98",
   "metadata": {},
   "source": [
    "Let's now visualise and investigate what are the most significant features both looking at Mean Decrease in Impurity (MDI) and SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compute feature importances (MDI)\n",
    "importances, importances_std, importances_sem = classifier.compute_feature_importances()\n",
    "\n",
    "# Step 6: Print top features\n",
    "top_indices = classifier.print_top_features(n_top=20)\n",
    "\n",
    "# Step 7: Plot feature ranking\n",
    "classifier.plot_feature_ranking(n_top=20)\n",
    "\n",
    "# Step 8: Compute permutation importances\n",
    "perm_result = classifier.compute_permutation_importances(n_repeats=10, n_jobs=4)\n",
    "\n",
    "# Step 9: Compute SHAP values (can be slow)\n",
    "shap_values = classifier.compute_shap_values()\n",
    "classifier.plot_shap_summary(class_idx=0, max_display=20)  # Class 0\n",
    "classifier.plot_shap_summary(class_idx=1, max_display=20)  # Class 1\n",
    "classifier.plot_feature_distributions(n_top=20, class_idx=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ CLASSIFICATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05856717-f747-4859-9c34-6948fc8af337",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
